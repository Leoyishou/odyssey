---
draw:
title: Packing
date created: 2025-03-23
date modified: 2025-03-24
---
**Packing**（打包）通俗来说，就是把短句子们一个接一个地拼成更长的句子，充分利用模型的训练空间，让训练过程更高效。

我举个简单例子来形象说明一下：

---

## 📌 **举个生活中的例子（直观易懂）**

假设你在搬家，有很多不同大小的纸箱子，你租了一辆货车：

- 如果每个纸箱子（句子）**单独占用一辆货车**（模型的一次计算资源）：
    
    - **问题**：小箱子明显浪费了货车空间。
    - 例如：
        - 一辆货车本来能装满很多东西，但现在只放了一个小箱子，浪费空间了。
- 如果你把几个小箱子（短句子）**合并打包**起来：
    
    - 把多个小箱子放到一辆货车里，让货车空间被合理利用。
    - **效果**：每辆货车都装得满满的，效率变高了。

这就是**packing**的核心思想。

---

## 📌 **再回到你的训练场景**

你的模型（如Qwen）每次训练能处理的文本长度是固定的（例如你设置了`max_seq_length=2048`）：

- 如果你直接把数据集中**每个较短的样本单独输入模型**，会有浪费：
    
    - 例如一个句子只有500个token，剩余的1500个位置浪费了。
- 使用**packing**方法：
    
    - 将多个短的句子（如500+800+700等）拼接在一起，填满2048的容量。
    - 这样每次训练步骤，都能最大限度地利用模型计算能力，避免浪费资源。

---

## 📌 **为什么修改序列长度需要重新packing？**

就像租货车时，每辆车的空间大小突然发生变化：

- 原来租了大货车（序列长度4096），现在换成了小货车（序列长度2048），你必须重新安排一下箱子怎么放，才能保证充分利用空间。

因此，每次修改序列长度时，模型都需要重新执行一次packing过程。

---

## 📌 **Packing带来的好处**

- **显著提高训练效率**：每个训练步骤都充分利用算力。
- **减少训练耗时**：因为浪费的计算量大大减少了。
- **节约成本**：尤其在使用昂贵的GPU资源时，效率更高意味着成本更低。

---

## 📌 **通俗一句话解释**

> Packing 就是“合理拼凑句子，填满模型容量，避免浪费算力”，和搬家时合理装箱、填满货车的道理一样。

希望这样解释能让你更直观、更通俗地理解Packing的含义～

这个过程耗时长的原因主要来自以下几个方面：

---

## 📌 **1. 什么是 Packing（数据打包）？**

所谓 Packing，是指把 **tokenization后的样本** 以最优化的方式组织起来，以便有效利用GPU显存资源，尤其适用于训练长序列模型（如你的模型 `max_seq_length=16384` 时尤其明显）。这种方法通常会：

- **拼接多个较短的样本**，使每个输入尽量接近模型的最大输入长度，从而提高显存使用效率。
- 减少模型 padding（填充）的开销，使得显存得到充分利用。

---

## 📌 **2. Packing 操作为何耗时如此长？**

Packing 本质上是一个非常复杂的数据预处理操作，其耗时的原因主要是：

|原因|详细说明|影响程度|
|---|---|---|
|数据规模巨大|本次数据量共 93,733 条，数据量较大|⭐⭐⭐⭐|
|序列长度极长|模型最大序列长度达到16384 Token，导致单个batch的样本计算量显著增加|⭐⭐⭐⭐⭐|
|CPU计算密集型操作|Packing 过程通常需要大量的CPU端拼接、排序和整理操作，计算密集型任务导致整体过程变慢|⭐⭐⭐⭐|
|硬盘 I/O 瓶颈|打包过程涉及频繁读取tokenized的数据片段，并重新写入打包后的数据，受限于磁盘I/O性能|⭐⭐⭐|

---

## 📌 **3. 如何优化或加速？**

你可以尝试的几个策略：

- **缓存结果（Cache）**
    
    - 首次运行后将处理好的Packing数据缓存，下次直接使用。
    - 比如HuggingFace的 datasets 库支持自动缓存，下次运行同样命令时会自动加载缓存。
- **更高性能存储**
    
    - 选择更快的磁盘（SSD或更快的云存储）进行数据读写操作。
- **减少序列长度（若允许）**
    
    - 降低序列长度可以极大降低耗时，但需要权衡任务效果。
- **多线程/多进程**
    
    - HuggingFace datasets库支持多线程/多进程，适当增加并行处理线程。
- **提前预处理数据**
    
    - 在正式训练前提前进行数据预处理，保存成预处理完成后的数据文件，再用于训练过程。

---

## 📌 **4. 当前情况分析**

根据你的情况：

- 第一次Packing耗时约37分钟左右。
- 第二次执行时，发现数据预处理（如tokenization）已经缓存，仅重新进行了Packing。这说明Packing本身未缓存，而tokenization等前序操作已缓存。

若此训练过程频繁进行，建议明确保存 Packing 后的数据，以减少后续的反复操作。

---

## 📌 **结论**

Packing 是长序列训练中不可避免的耗时步骤，但可以通过缓存或优化数据处理流程来缓解。

当前情况正常，无需担心，耐心等待完成即可。
