---
date created: 2025-03-25
date modified: 2025-07-10
uid: 0fe9997b-2556-4daf-8edc-0d78f56bc04e
---
## 📌 一、评估（Evaluation）模型性能

在Open-R1项目中，使用了`lighteval`工具来进行模型评估，你可以直接使用项目提供的脚本。

**例如，以AIME2024数据集为例：**

在终端执行（以下是适用于单GPU的简单示例）：

```bash
MODEL=你的模型目录路径（例如：data/Qwen2.5-1.5B-Open-R1-Distill）
MODEL_ARGS="pretrained=$MODEL,dtype=bfloat16,max_model_length=32768,gpu_memory_utilization=0.8,generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}"
OUTPUT_DIR=./data/evals/your_model_eval_results

TASK=aime24
lighteval vllm $MODEL_ARGS "custom|$TASK|0|0" \
    --custom-tasks src/open_r1/evaluate.py \
    --use-chat-template \
    --output-dir $OUTPUT_DIR
```

执行完成后会在`$OUTPUT_DIR`中生成评估结果，可直接查看。

类似的方式，你也可以评估：

- `math_500`：数学推理任务
- `gpqa:diamond`：常识推理任务
- `lcb:codegeneration`：代码生成任务

只需将上面的`TASK`变量改成你想评估的任务名称即可。

---

## 📌 二、人工评估模型推理能力

你也可以直接调用训练好的模型，人工验证一些问题：

**使用 Hugging Face transformers 库进行推理：**

### CPU 推理

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_path = "data/Qwen2.5-1.5B-Open-R1-Distill"

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# CPU模式下禁用所有GPU特殊优化，直接用float32稳定运行
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map='cpu',
    trust_remote_code=True,
    torch_dtype=torch.float32,
    use_sliding_window=False
)

prompt = "Solve the following math problem step-by-step: What is 13 multiplied by 15?"

inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)


```

### GPU 推理

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_path = "data/Qwen2.5-1.5B-Open-R1-Distill"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map='auto',
    trust_remote_code=True,
    torch_dtype=torch.float16,
    attn_implementation='flash_attention_2',
    use_sliding_window=False
)

prompt = "Solve the following math problem step-by-step: What is 13 multiplied by 15?"

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)

```

通过这种方式，你可以快速感受一下模型的实际推理表现，判断其推理能力是否符合你的预期。

---

## 📌 三、模型推理效果定量分析（指标对比）

你可以同时评估训练前后的模型指标对比，例如Loss、Accuracy、Pass Rate（通过率）等：

|方法|指标|推荐的benchmark数据集|
|---|---|---|
|数学推理|Pass Rate（通过率）|`math_500`, `aime24`|
|代码生成|Pass Rate|`lcb:codegeneration`（LiveCodeBench）|
|常识推理|Accuracy|`gpqa:diamond`|

在模型训练前后分别评估这些benchmark，然后比较两个结果的差距，可以清晰了解模型的提升。

---

## 📌 四、模型部署后实际场景测试

- 若你的模型有特定的下游任务（例如对话机器人、数学求解等），可将模型集成到实际场景中，进行测试，以评估真实的应用性能。

---

## 🌟 推荐的评估顺序

1. **快速人工推理**：简单验证模型是否正常运行，生成的文本是否合理；
2. **自动化benchmark评估**：客观地衡量模型的能力；
3. **结果对比分析**：与之前的版本或baseline对比，确认模型性能提升。

---

综上所述，根据你的需求，我建议先用第一个方法（benchmark评估）进行客观衡量，再进行第二个（人工推理）进行感性验证，这样你能对模型的推理能力有一个全面的理解。
