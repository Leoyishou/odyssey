---
draw:
title: 支持向量机
date created: 2025-02-15
date modified: 2025-02-22
---

支持向量机（SVM）是一种非常直观且强大的分类（以及回归）方法，其核心思想是**寻找一个最优的分割超平面**，用以区分不同类别的数据。下面我将从多个角度解释 SVM 的基本原理及如何理解它。

- **与逻辑回归/感知机的区别**：
  逻辑回归和单层感知机直接学习一个分类边界，但它们的目标不在于最大化间隔，而 SVM 则明确追求最大化间隔，从而提高泛化能力。
- **直观比喻**：
  想象在一张纸上用一条直线分开两组点。SVM 寻找那条能让最近的点离直线最远的线，就像你想在两个相互靠近的物体之间放置一块最大的泡沫垫一样，这块"垫子"越厚，你对它们的区分就越自信。

## 1. 基本思想

### 1.1 分割超平面

- **目标**：在特征空间中找到一条（或一个超平面）能够将不同类别的样本分开的决策边界。
- **线性可分**：对于线性可分的数据，SVM 寻找的超平面不仅能够分开所有样本，还要使得距离各类样本最近的点（支持向量）尽可能远，即**最大化分类间隔（Margin）**。

### 1.2 最大间隔

- **间隔（Margin）**：指的是超平面到距离它最近的正负样本的距离。SVM 的目标是**最大化这个距离**，从而提高模型的泛化能力。
- **支持向量**：处于间隔边界上的那些样本点，它们直接决定了超平面的位置。其他远离超平面的点对最终的决策边界没有直接影响。

---

## 2. 数学表达

对于**线性可分**的情况，假设样本 $(x_i, y_i)$ 中 $y_i \in \{-1, +1\}$，SVM 寻找的超平面可以表示为：

$w \cdot x + b = 0,$

要求所有样本满足：

$y_i (w \cdot x_i + b) \ge 1, \quad \text{for all } i.$

目标是**最大化间隔**，等价于**最小化** $\frac{1}{2}\|w\|^2$（因为间隔与 $1/\|w\|$ 成反比）：

$\min_{w, b} \; \frac{1}{2} \|w\|^2 \quad \text{subject to} \quad y_i (w \cdot x_i + b) \ge 1.$

---

## 3. 处理非线性和噪声数据

### 3.1 软间隔 SVM

- 当数据不可完美分割时，引入**松弛变量** $\xi_i$ 来允许某些样本违反间隔要求，同时在目标函数中加入惩罚项。
- 优化目标变为：

$\min_{w, b, \xi} \; \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{m} \xi_i,$

其中 $C$ 是一个正则化参数，用来平衡间隔最大化和分类错误的代价。

### 3.2 核技巧（Kernel Trick）

- 对于非线性可分的数据，可以**映射**到高维空间，在高维空间中找到一个线性可分的超平面。
- 核函数（如 RBF、polynomial 核等）允许我们在不显式计算高维映射的情况下，直接计算出映射后的内积，从而实现非线性决策边界。

---

## 5. 总结

- **核心思想**：SVM 通过寻找具有最大间隔的分割超平面，来实现更好的分类效果和泛化能力。
- **数学优化**：对于线性问题，通过最小化 $\frac{1}{2}\|w\|^2$ 并满足约束 $y_i (w \cdot x_i + b) \ge 1$；对于非线性或噪声数据，通过引入软间隔和核技巧进行扩展。
- **支持向量**：仅依赖于那些处于间隔边界上的样本，其他样本不直接影响最终的决策边界。

通过这种方式，你可以将 SVM 理解为一种既注重分类准确性，又注重对未来数据具有鲁棒性的分类方法。
