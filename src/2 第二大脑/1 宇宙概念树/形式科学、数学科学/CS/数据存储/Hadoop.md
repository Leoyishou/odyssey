---
date created: 2025-02-24
date modified: 2025-07-10
uid: 13f09d4b-ecca-4930-8ea5-c29c9e33eaea
---

想象你有一堆超大的文件，比如几百个 TB 的日志数据，放在一台电脑上存不下了怎么办？Hadoop 就登场了。它是个开源工具，专门帮你把这些数据拆开放到很多台电脑上（分布式存储），还能帮你把计算任务也分散到这些电脑上一起跑。它有两个核心零件：

- HDFS：相当于一个超级大硬盘，把数据分散存到很多机器上，还能保证数据不会丢。
    
- MapReduce：这是一种干活的方式。假设你要统计所有日志里某个词出现的次数，MapReduce 会先把任务拆成小块（Map 阶段），每台机器统计自己那部分数据，然后再把结果汇总起来（Reduce 阶段）。有点像你把一个大任务分给很多人去做，最后把结果收回来。
    

简单说，Hadoop 是"仓库+工人"的组合，MapReduce 是具体干活的"方法"。
