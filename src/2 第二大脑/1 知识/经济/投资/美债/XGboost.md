---
draw:
title: XGboost
date created: 2025-01-24
date modified: 2025-02-14
---

#第一性原理 Extreme Gradient Boosting（每一步都计算损失函数的梯度，指导下一棵树往哪个方向去纠正错误）
[[决策树]]就是类似于用纹不纹身来判断一个人是不是好女孩，价值观太单一了，用 xgboost 搞多棵树，逐步补充自己判断失误的地方  
单棵[[决策树]]就像是用一个过于简单的标准来判断，比如"纹身=不好"这种非黑即白的思维。这样显然是有偏见且不准确的：

- 可能有纹身的是艺术家
- 没纹身的也可能是不好的人
- 纹身的大小、位置、内容都没考虑

而 XGBoost 的思路就是:

1. 第一棵树用"纹身"特征分类，发现预测错了一批艺术家
2. 第二棵树就专门学习"职业"特征，来纠正对艺术家的误判
3. 第三棵树可能会看"教育背景"
4. 第四棵树再看"兴趣爱好"  
...

通过这种方式，模型逐步:

- 纠正偏见
- 考虑更多维度
- 形成更全面的判断

就像人的成长一样，从最初的简单判断，通过不断学习和纠正，逐渐形成更成熟、更包容的认知。

这也说明了为什么在机器学习中，单一的简单模型往往不如集成模型表现好 - 因为现实世界太复杂了，需要从多个角度去理解和判断。

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2025%2F01%2F24%2F16-46-21-dac5f273d7c5066d91f3a109486250bc-202501241646992-812b50.png)
