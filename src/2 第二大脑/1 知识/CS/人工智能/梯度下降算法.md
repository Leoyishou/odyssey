---
draw:
tags: []
title: 梯度下降算法
date created: 2024-09-20
date modified: 2025-01-29
---

好的，我将把之前的回答中 LaTeX 公式修改为更适合 Obsidian 的 Markdown 数学公式形式，并进行一些排版上的优化，使其在 Obsidian 中显示更清晰。

下面是修改后的回答：

下面我来详细解释一下为什么求偏导数是梯度下降优化的关键：

**1. 梯度下降的核心思想**

梯度下降是一种迭代优化算法，用于寻找函数的最小值。其核心思想是：沿着函数梯度（对于多变量函数，就是偏导数组成的向量）的反方向移动，函数值下降最快。就像一个下山的人，为了尽快到达山底，他会沿着坡度最陡的方向下山。

**2. 偏导数的意义**

- **方向：** 对于多变量函数，偏导数表示函数沿着某个坐标轴方向的变化率。梯度是由所有偏导数组成的向量，它指向函数值增长最快的方向。因此，梯度的反方向就是函数值下降最快的方向。
- **步长：** 偏导数的大小反映了函数在该方向上的变化速度。偏导数越大，函数在该方向上的变化越快，梯度下降时应该走的步长也应该相应地大一些；反之，偏导数越小，函数在该方向上的变化越慢，梯度下降时应该走的步长也应该相应地小一些。

**3. 梯度下降的步骤**

梯度下降的步骤如下：

1. **选择初始点：** 在函数曲面上随机选择一个起始点。
    
2. **计算梯度：** 计算函数在该点的梯度（所有偏导数）。
    
3. **更新参数：** 沿着梯度的反方向移动一定的步长，更新参数。更新公式为：

$$
    \theta_{\text{new}} = \theta_{\text{old}} - \eta \cdot \nabla J(\theta)
$$

其中：

- θ：表示需要优化的参数（例如 Word2Vec 中的词向量）。
- η：表示学习率，控制每次更新的步长。
- ∇J(θ)：表示损失函数 J 关于参数 θ 的梯度。

1. **重复步骤 2 和 3：** 不断重复计算梯度和更新参数，直到函数收敛到最小值或达到预定的迭代次数。
    

**4. 在 Word2Vec 中的应用**

在 Word2Vec 中，我们需要优化的参数是词向量。损失函数衡量了模型预测的上下文词的概率与真实上下文词的差距。通过计算损失函数关于词向量的偏导数，我们可以得到梯度，然后使用梯度下降算法来更新词向量，使得损失函数不断减小，模型的预测能力不断提高。

例如，在之前的推导中，我们得到了 `J<sub>naive-softmax</sub>` 关于 `v<sub>c</sub>` 的偏导数：

$$
\frac{\partial J_{\text{naive-softmax}}(v_c, o, U)}{\partial v_c} = (\hat{y} - y)^T U
$$

这个偏导数就告诉我们应该如何更新中心词向量 vc​：

$$
v_c^{\text{new}} = v_c^{\text{old}} - \eta \cdot (\hat{y} - y)^T U
$$

**总结**

求偏导数是梯度下降优化的核心步骤，它提供了梯度下降算法所需的**方向**和**步长**信息。通过不断地沿着梯度的反方向更新参数，我们可以使函数收敛到最小值，从而达到优化的目的。在 Word2Vec 中，求偏导数是为了更新词向量，使得模型能够更好地预测上下文词。所以，你说的"求偏导数的目的是完成梯度下降的优化"是完全正确的。

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F20%2F11-42-21-5ec4c7c1cddba14c6e75ebc678ff7a9d-202409201142560-9bbf3b.png)

1. 想做回归，找了一个两个参数的直线
2. 目标是所有点与直线的 y 轴距离和最短，其实就是真实值和预测值的偏差越小，也就是**损失函数**
3. 求损失函数的时候的最小值时就需要用梯度下降算法  
![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F20%2F11-44-37-ccc37a8462a69c9bec124c80f03b6722-202409201144743-9db084.png)

用"下山"来比喻梯度下降，其实是一个很形象的类比：**我们希望通过不断"沿着山坡往下走"来找到最低点**（损失函数的极小值）。在这个类比里：

- **"山的高度"** 对应于 **损失函数值 (loss)**。
- **"山坡的陡峭程度和方向"** 对应于 **梯度 (gradient)**。

梯度下降则是根据梯度的方向和大小来决定"往哪个方向走，以及走多大步"，从而一步步下降到谷底（或尽量接近它）。

然而，有时候我们会遇到特殊地形，让"往下走"这件事变得困难：

- **梯度消失 (vanishing gradient)**：相当于山坡突然变得非常平坦，几乎感觉不到坡度，走一步基本没前进。
- **梯度爆炸 (exploding gradient)**：相当于山坡异常陡峭，走一步就跌下悬崖式的大坡，跳得过猛，"走过了头"。

下面进一步解释这两种情况在"下山"类比中的含义。

---

## 1. 梯度消失 (Vanishing Gradient)——"平坦高原"

在"山"的类比中，**梯度消失**就像走到了一个"非常平坦的高原或盆地"：

1. **坡度几乎为零**：由于地形平坦，不管往哪个方向看，感觉坡度都非常小，看上去像是"没有什么路要走"。
2. **步子微乎其微**：因为感知到的斜率很小，按照梯度下降的策略，你每一步都只能"走"非常小的一步，导致走了很久还是差不多在原地打转。
3. **学习停滞**：从神经网络训练角度看，这意味着**参数几乎不更新**，训练速度变得极其缓慢，甚至像卡住了一样。

现实中的原因通常是：

- 在深层网络，链式求导会让梯度被反复相乘，数值非常小就会逐层衰减；
- 一些激活函数（如 sigmoid）在较远区间时导数很小，也会导致梯度衰减。

### 类比重点

- **想象自己在一片"平原"或"高原"**：环顾四周，地面看起来几乎水平，很难辨别哪边是更低的方向。
- **走得特别慢**：每一步都很小，因为地势太平；小到感受不到明显进展。

---

## 2. 梯度爆炸 (Exploding Gradient)——"陡峭悬崖"

和梯度消失相反，**梯度爆炸**则像走到了一处**异常陡峭的悬崖**：

1. **坡度极大**：地形突然变得特别陡，任何微小的移动都可能带来巨大高度变化。
2. **一步走过头**：根据梯度下降策略，因为斜率很大，算法可能一下子"走出"了非常大的步幅，结果不是稳步往山脚接近，而是直接冲过头掉到其他地方——甚至"越过山头"又跌到另一面。
3. **不稳定震荡**：在神经网络中，如果梯度非常大，更新量一下子很大，导致权重参数在每次更新后变得失控式跳动，有时还会让损失激增，训练过程不收敛。

现实中的原因通常是：

- 反向传播时，梯度被反复相乘后数值可能非常大；
- 输入数据异常或权重初始化不当；
- 学习率过高，也会让"下山"的步子过于粗暴，容易蹦蹦跳跳不稳定。

### 类比重点

- **想象在一段极其陡峭的峭壁**：稍不留神你就会"滚"得很远，甚至直接冲下山崖。
- **脚步不稳**：每次迈步都可能因为太"陡"而滑得过猛，完全失控。

---

## 3. 如何在"下山"过程中避免这两个极端？

1. **梯度消失**：
    
    - 换一些激活函数，比如 ReLU、Leaky ReLU；
    - 使用批归一化 (Batch Normalization)；
    - 用更适合深层网络的结构（LSTM、GRU、ResNet 等）；
    - 合理初始化权重。
2. **梯度爆炸**：
    
    - 梯度裁剪 (Gradient Clipping)：当检测到梯度过大时，把它截断到一个合理范围；
    - 调整学习率：让步子不要过大，或者使用自适应优化算法（如 Adam）；
    - 检查数据是否有异常值，检查网络结构是否合适。

---

## 小结

- **梯度下降**：就像**顺着山坡往下走**。
- **梯度消失**：山坡变平，几乎感觉不到坡度（看似踩到"平地"），走得极慢。
- **梯度爆炸**：山坡过陡，一步就冲下去或乱蹦，导致不稳定。

通过这个类比，我们可以更直观地理解为什么神经网络训练会遇到这两类问题，以及它们对训练带来的挑战。最终的解决思路，就是在神经网络结构、激活函数、优化方法等方面做改进，让"下山"之路既不要太平，也不要太陡，才能顺利走到山谷最低点。
