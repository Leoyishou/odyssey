---
draw:
title: 张量
date created: 2025-02-14
date modified: 2025-03-27
---

张量可以是标量（0维）、向量（1维）、矩阵（2维）或更高维数组。

张量拥有形状（shape）和数据类型（dtype）等属性，

以下是一个根据常见使用模式估算频率的 Markdown 表格，**从高频到相对低频**排列：

|                                                                    |                                                |                                                                     |
| ------------------------------------------------------------------ | ---------------------------------------------- | ------------------------------------------------------------------- |
| **操作 (Operation)**                                                 | **描述 (Description)**                           | **常见用途/示例 (Common Use/Example)**                                    |
| `torch.tensor(data, dtype=None, device=None, requires_grad=False)` | 创建                                             |                                                                     |
|                                                                    | 原地操作                                           |                                                                     |
| **算术运算** (`+`, `-`, `*`, `/`)                                      | 逐元素的加、减、乘、除。| 几乎所有计算，如加偏置、特征缩放、计算误差等。`output = input * weight + bias`            |
| `torch.matmul` / `@`                                               | 矩阵乘法。| 实现线性层（全连接层）、注意力机制等。`y = x @ W.T + b`                               |
| `view()` / `reshape()`                                             | 改变张量的形状，而不改变其数据。`view` 要求连续内存，`reshape` 更灵活。| 在卷积层和全连接层之间转换、展平图像、调整批次维度。`x = x.view(batch_size, -1)`             |
| `.to()` / `.cuda()` / `.cpu()`                                     | 将张量移动到不同的设备（CPU/GPU）或改变其数据类型 (`dtype`)。| 模型训练前将数据和模型移到 GPU，推理时移回 CPU。`tensor = tensor.to('cuda')`           |
| **索引与切片** (`[:]`, `[...]`)                                         | 使用类似 NumPy 的语法访问张量的部分数据。| 获取数据集中的批次、选择特定特征、操作序列数据。`batch = data[start:end]`                  |
| `torch.mean()`                                                     | 计算张量沿指定维度的平均值。| 计算损失函数的平均值、特征归一化。`loss = criterion(outputs, labels).mean()`        |
| `torch.sum()`                                                      | 计算张量沿指定维度的和。| 计算总损失、累积梯度、统计数量。`total_loss = loss.sum()`                          |
| `torch.relu` / `F.relu`                                            | ReLU (Rectified Linear Unit) 激活函数，`max(0, x)`。| 神经网络中非常常用的非线性激活层。`x = torch.relu(linear_output)`                   |
| `torch.cat()`                                                      | 沿给定维度拼接一系列张量。| 合并不同来源的特征、在序列模型中组合输出。`features = torch.cat((feat1, feat2), dim=1)` |
| `torch.unsqueeze()`                                                | 在指定位置增加一个维度（大小为 1）。| 适配期望特定维度数的操作，如添加批次维度或通道维度。`img = img.unsqueeze(0)`                 |
| `torch.squeeze()`                                                  | 移除张量中所有大小为 1 的维度，或移除指定位置的大小为 1 的维度。| 去除不必要的单维度，简化张量形状。`scalar = tensor.squeeze()`                       |
| `torch.randn()` / `torch.zeros()` / `torch.ones()`                 | 创建具有特定形状、填充随机数（正态分布）、0 或 1 的张量。| 初始化模型权重、创建掩码、生成初始状态。`weights = torch.randn(in_f, out_f)`           |
| `torch.softmax` / `F.softmax`                                      | 将张量转换为概率分布。| 分类问题的输出层，使输出和为 1。`probs = F.softmax(logits, dim=-1)`               |
| `torch.sigmoid` / `F.sigmoid`                                      | Sigmoid 激活函数，将值压缩到 (0, 1) 之间。| 二分类问题输出、门控机制（LSTM/GRU）。`prob = torch.sigmoid(logit)`               |
| `torch.max()` / `torch.min()`                                      | 返回张量沿指定维度的最大/最小值及其索引 (`argmax`/`argmin`)。| 获取预测类别、实现最大池化、寻找关键特征。`preds = outputs.argmax(dim=1)`               |
| `torch.log` / `torch.exp`                                          | 逐元素的自然对数 / 指数运算。| 计算对数似然损失、应用指数加权、某些激活函数或归一化。`log_probs = F.log_softmax(logits)`     |
| `torch.transpose()` / `.t()`                                       | 交换张量的两个维度。`.t()` 仅适用于 2D 张量（矩阵转置）。| 矩阵转置、调整数据格式以匹配某些库的要求。`x = x.transpose(0, 1)`                       |
| `torch.nn.functional` (as `F`)                                     | 包含许多无状态的操作，如激活函数、池化、损失函数等（常在 `forward` 方法中使用）。| `F.relu`, `F.max_pool2d`, `F.cross_entropy`, `F.dropout`            |
| `torch.pow()` / `**`                                               | 逐元素的幂运算。| 实现多项式特征、计算均方误差（配合 `mean`）。`squared_error = (preds - targets) ** 2` |
| `torch.stack()`                                                    | 沿一个 _新_ 的维度拼接一系列张量（要求所有张量形状相同）。| 将一个列表中的多个张量合并成一个更高维的张量。`batch = torch.stack(list_of_tensors)`      |

**重要说明:**

1. **频率的主观性:** 这个排序是基于一般观察，具体项目的实际使用频率会有所不同。例如，处理序列数据的 NLP 项目可能会更频繁地使用 `torch.cat`, `torch.stack`, `torch.transpose`。
2. **基础性 vs. 高频:** 像 **索引/切片** 和 **`.to()`** 这样的操作虽然极其基础和必要，但可能在代码行数上不如算术运算或 `view`/`reshape` 出现得多。
3. **多种调用方式:** 很多操作既有 `torch.function()` 的形式，也有张量方法的 `.method()` 形式（如 `torch.add(a, b)` vs `a.add(b)`），还有运算符（`a + b`）。这里主要列出概念，具体写法可能多样。
4. **`torch.nn.functional`:** `F` 模块包含了大量常用操作，尤其是激活函数和池化，它们的使用频率也非常高。
5. **创建张量:** `torch.tensor()`, `torch.from_numpy()` 等创建操作也是基础，但可能在数据加载阶段使用更集中。

这个表格应该能为你提供一个关于 PyTorch 中哪些张量操作最常用、最值得优先掌握的良好概览。建议查阅 PyTorch 官方文档以获取最全面和准确的信息。
