Directory Tree:
==============

└── 1 一切皆项目
    ├── .DS_Store
    ├── .DS_Store.md
    ├── CS
    │   ├── + 数学之美
    │   │   ├── 史记和圣经.md
    │   │   ├── 向量数据库和大模型是否能帮助第二大脑库的抽象语义搜索.md
    │   │   ├── 图灵测试.md
    │   │   ├── 文字.md
    │   │   ├── 罗塞塔石碑.md
    │   │   ├── 翻译.md
    │   │   ├── 自然语言处理.md
    │   │   └── 阿拉伯数字.md
    │   ├── ++Vue学习
    │   │   └── vue学习$.md
    │   ├── Q1：大模型微调
    │   │   └── Untitled.md
    │   ├── Q1：算法入门
    │   │   ├── Q3：kaggle 比赛
    │   │   │   ├── Coursera Deep Learning.md
    │   │   │   ├── pandas.md
    │   │   │   ├── 感知机.md
    │   │   │   ├── 训练赛.md
    │   │   │   └── 逻辑回归.md
    │   │   └── 算法入门.md
    │   └── Q2：做CS的经典lab
    │       ├── CMU 15-445 Database Systems
    │       │   └── CMU 15-445  Database Systems.md
    │       ├── CS144 Computer Network
    │       │   ├── CPP
    │       │   │   ├── algorithm.md
    │       │   │   ├── cmake.md
    │       │   │   ├── size_t.md
    │       │   │   ├── 命名空间.md
    │       │   │   ├── 数据类型.md
    │       │   │   ├── 构造函数.md
    │       │   │   └── 流操作符.md
    │       │   ├── CS144 Computer Network.md
    │       │   ├── EOF.md
    │       │   ├── Lab0 ByteStream.md
    │       │   ├── Lab2 TCP Receiver.md
    │       │   ├── Lab3 TCP Sender.md
    │       │   ├── Lab4 TCP Connection.md
    │       │   ├── Lab5 Network Interface.md
    │       │   ├── Lab6 Router.md
    │       │   ├── Untitled.md
    │       │   ├── r n.md
    │       │   └── 数据流重组 Lab1 Stream Reassembler.md
    │       ├── CS224n Natural Language Processing
    │       │   ├── Assignment 1 Exploring Word Vectors.md
    │       │   ├── Assignment 2 word2vec.md
    │       │   ├── Assignment 3 Dependency Parsing.md
    │       │   ├── Assignment 3.2 Neural Transition-Based Dependency Parsing.md
    │       │   ├── Assignment 3.md
    │       │   ├── Assignment 4 RNNs.md
    │       │   ├── Assignment 4.md
    │       │   ├── Assignment 5 CNN.md
    │       │   ├── Assignment 5.md
    │       │   ├── CS224n Natural Language Processing.md
    │       │   ├── GloVe.md
    │       │   ├── Neural Machine Translation.md
    │       │   ├── Sigmoid函数.md
    │       │   ├── Untitled.md
    │       │   ├── Word2vec.md
    │       │   ├── softmax.md
    │       │   ├── 交叉熵.md
    │       │   ├── 依存句法分析.md
    │       │   ├── 偏导数.md
    │       │   ├── 共现矩阵.md
    │       │   ├── 函数图像.md
    │       │   ├── 复合函数求导.md
    │       │   ├── 差分.md
    │       │   ├── 强独立性假设.md
    │       │   ├── 损失函数.md
    │       │   ├── 最大似然估计.md
    │       │   ├── 条件概率.md
    │       │   ├── 注意力机制.md
    │       │   ├── 激活函数.md
    │       │   ├── 编码器、解码器.md
    │       │   ├── 自注意力.md
    │       │   ├── 词向量 embedding.md
    │       │   ├── 词向量.md
    │       │   ├── 语法分析、依存句法分析.md
    │       │   ├── 负对数.md
    │       │   ├── 链式法则.md
    │       │   └── 问答系统.md
    │       ├── CS229
    │       │   └── CS229 Machine Learning.md
    │       ├── MIT 6.828
    │       │   ├── C
    │       │   │   ├── C语言.md
    │       │   │   ├── goto.md
    │       │   │   ├── 头文件.md
    │       │   │   ├── 导包机制.md
    │       │   │   └── 格式说明符.md
    │       │   ├── MIT 6.828 Operating System Engineering.md
    │       │   ├── Makefile.md
    │       │   ├── QEMU.md
    │       │   ├── RISC-V.md
    │       │   ├── lab
    │       │   │   ├── Lab1 Unix utilities.md
    │       │   │   ├── Lab2 System calls.md
    │       │   │   ├── Lab3 Page tables.md
    │       │   │   ├── Lab4 traps.md
    │       │   │   ├── 信令风暴.md
    │       │   │   └── 常用命令.md
    │       │   └── xv6
    │       │       ├── fork.md
    │       │       ├── kernel.md
    │       │       ├── pipe.md
    │       │       ├── system call.md
    │       │       ├── xv6.md
    │       │       ├── 中断陷阱.md
    │       │       ├── 懒分配.md
    │       │       ├── 懒拷贝.md
    │       │       ├── 文件系统.md
    │       │       ├── 时钟.md
    │       │       ├── 用户态、内核态.md
    │       │       ├── 网络.md
    │       │       ├── 陷阱帧.md
    │       │       └── 页表.md
    │       ├── MIT6.824
    │       │   ├── Lab 1 MapReduce.md
    │       │   ├── Lab 3 Key Value server.md
    │       │   ├── Lab2 raft.md
    │       │   ├── MIT 6.824 Distributed System.md
    │       │   ├── MapReduce.md
    │       │   ├── Raft.md
    │       │   └── 共识算法.md
    │       ├── UCB CS186 Introduction to Database System
    │       │   └── UCB CS186 Introduction to Database System.md
    │       ├── lab 驱动的实践教学.md
    │       ├── 团队协作.md
    │       ├── 如何开始学习一个 Lab，以 MIT6.824 Distributed System为例 $.md
    │       ├── 解耦学习.md
    │       └── 谷歌三剑客.md
    ├── Q1：摄影计划
    ├── Q2：如何达到 50%的三分命中率
    │   ├── $ 如何达到 50%的三分命中率.md
    │   ├── 一分钟自投自抢.md
    │   ├── 寻找达人反馈.md
    │   ├── 投篮.md
    │   └── 辅助手.md
    ├── Q2：海燕计划
    │   ├── NFTurbo.md
    │   ├── 海燕计划$.md
    │   └── 面试.md
    ├── Q2：自媒体一万粉丝
    │   └── 爬虫计划.md
    ├── Q3：Viva
    │   ├── python
    │   │   ├── SQLAlchemy.md
    │   │   ├── pycache.md
    │   │   └── 包.md
    │   ├── viva
    │   │   ├── .DS_Store
    │   │   ├── 0. 我的奥德赛.md
    │   │   ├── CICD for viva.md
    │   │   ├── Session Auth.md
    │   │   ├── Untitled.md
    │   │   ├── 产品
    │   │   │   ├── Untitled.md
    │   │   │   ├── Viva.md
    │   │   │   ├── 向业内人士征求意见.md
    │   │   │   ├── 哈希主义.md
    │   │   │   └── 需求池.md
    │   │   ├── 前端
    │   │   │   ├── viva UI UE.md
    │   │   │   └── viva 前端.md
    │   │   ├── 口语课复盘 demo.md
    │   │   ├── 后端
    │   │   │   ├── Anki 模块.md
    │   │   │   ├── viva 后端.md
    │   │   │   ├── viva 数据库.md
    │   │   │   ├── 后端方案.md
    │   │   │   ├── 接口文档.md
    │   │   │   ├── 数据库.md
    │   │   │   ├── 日志.md
    │   │   │   └── 社交认证.md
    │   │   ├── 用户系统.md
    │   │   └── 算法
    │   │       ├── 分词.md
    │   │       └── 微调.md
    │   ├── 进度可视化
    │   │   └── 英语学习可视化$.md
    │   └── 项目管理.md
    ├── Q3：丽景私募
    │   ├── 丽景私募 一期.md
    │   ├── 分词计划
    │   ├── 如何培养仓位的思维？.md
    │   └── 第一期
    │       ├── 0 入金.md
    │       ├── 1 交易记录.md
    │       ├── 周报
    │       │   ├── TradingView.md
    │       │   ├── 美股投资周报 第一周.md
    │       │   └── 美股投资周报 第二周.md
    │       └── 学习点.md
    ├── Q3：如何投资
    │   ├── 观察仓.md
    │   └── 项目总结.md
    ├── Q3：领航者 陪跑计划
    │   └── 领航者 陪跑计划.md
    ├── Q4：DEV insight
    │   ├── Java 基础题$.md
    │   └── Q4：DEV insight.md
    ├── Q4：一百首歌
    │   └── 歌单.md
    ├── Q4：博客计划
    ├── Q4：听力自动化
    ├── Q4：自建 publish
    │   └── Publish 插件.md
    ├── qt Agent
    │   ├── API 特性
    │   │   ├── Assistant API.md
    │   │   ├── Batch API.md
    │   │   ├── Function Call.md
    │   │   ├── GPT Actions.md
    │   │   ├── Prompt Caching.md
    │   │   ├── 模型性能评估.md
    │   │   ├── 模型蒸馏.md
    │   │   └── 预测输出.md
    │   ├── 开发框架
    │   │   ├── LlamaIndex.md
    │   │   └── langchain.md
    │   └── 意图识别.md
    └── 英语
        ├── ++如何畅游英语 youtube
        │   ├── 如何畅游英语油管$.md
        │   └── 词汇过滤.md
        ├── 2024 Q3：切入reddit
        │   ├── 从 workout 切入 quora 和 reddit $.md
        │   └── 结项报告.md
        ├── Q3：托福考到 100
        │   ├── ++成为兼职托福老师
        │   │   └── 如何成为兼职托福老师.md
        │   ├── bionic reading.md
        │   ├── 托福和小红书结合的计划$.md
        │   ├── 托福考到100$.md
        │   ├── 旅行.md
        │   └── 职业规划.md
        └── 口语 review
            ├── Untitled.md
            └── 口语复盘.md

File Contents:
==============


[.DS_Store.md]
==============




[CS/+ 数学之美/史记和圣经.md]
====================

---
draw:
title: 史记和圣经
tags: []
date created: 2024-06-15
date modified: 2024-11-12
---

excerpt

<!-- more -->

当司马迁用近 53 万字记载了中国上千年历史的同时，远在中东的犹太人也用类似的篇幅记载了自创世纪以来，主要是摩西以来他们祖先的历史，这就是《圣经》中的《旧约》部分。《圣经》简洁的文风和中国的《史记》颇有相似之处。但是和《史记》这本由唯一作者写成的史书不同，《圣经》的写作持续了很多世纪，后世的人在做补充时，看到的是几百年前甚至上千年前原作的抄本。抄写的错误便在所难免。据说今天也只有牛津大学保留了一本没有任何错误的古本。虽然做事认真的犹太人要求在抄写《圣经》时，要虔诚并且打起十二分精神，尤其是每写到 " 上帝 "（God 和 Lord）这个词时要去洗手祈祷，不过抄写错误还是难以避免。于是犹太人发明了一种类似于我们今天计算机和通信中校验码的方法。他们把每一个希伯来字母对应于一个数字，这样每行文字加起来便得到一个特殊的数字，这个数字便成为了这一行的校验码（见图 1.8）。同样，对于每一列也是这样处理。当犹太学者抄完一页《圣经》时，他们需要把每一行的文字加起来，看看新的校验码是否和原文的相同，然后对每一页进行同样的处理。如果这一页每一行和每一列的校验码和原文完全相同，说明这一页的抄写无误。如果某行的校验码和原文中的对应不上，则说明这一行至少有一个抄写错误。当然，错误对应列的校验码也一定和原文对不上，这样可以很快找到出错的地方。这背后的原理和我们今天的各种校验是相同的。



[CS/+ 数学之美/向量数据库和大模型是否能帮助第二大脑库的抽象语义搜索.md]
=========================================

---
draw:
title: 向量数据库和大模型是否能帮助第二大脑库的抽象语义搜索
tags: []
date created: 2024-06-13
date modified: 2024-11-12
---

可以 run，但是不太 work



[CS/+ 数学之美/图灵测试.md]
===================

---
draw:
title: 图灵测试
tags: []
date created: 2024-06-15
date modified: 2024-11-12
---

excerpt

<!-- more -->

最早提出机器智能设想的是计算机科学之父阿兰•图灵（Alan Turing），1950 年他在《思想》（Mind）杂志上发表了一篇题 " 计算的机器和智能 " 的论文。在论文中，图灵并没有提出什么研究的方法，而是提出了一种验证机器是否有智能的方法：让人和机器进行交流（见图 2.1），如果人无法判断自己交流的对象是人还是机器，就说明这个机器有智能了。这种方法被后人称为图灵测试（Turing Test）。图灵其实是留下了一个问题，而非答案，但是一般认为自然语言的机器处理（现在称作自然语言处理）的历史可以追溯到那个时候，至今已经 60 多年了。



[CS/+ 数学之美/文字.md]
=================

---
draw:
title: 文字
tags: []
date created: 2024-06-15
date modified: 2024-11-12
---

excerpt

<!-- more -->



[CS/+ 数学之美/罗塞塔石碑.md]
====================

---
draw:
title: 罗塞塔石碑
tags: []
date created: 2024-06-15
date modified: 2024-11-12
---



<!-- more -->



[CS/+ 数学之美/翻译.md]
=================

---
draw:
title: 翻译
tags: []
date created: 2024-06-15
date modified: 2024-11-12
---

翻译这件事之所以能达成，仅仅是因为不同的文字系统在记录信息上的能力是等价的。

<!-- more -->

[罗塞塔石碑](罗塞塔石碑.md)



[CS/+ 数学之美/自然语言处理.md]
=====================

---
draw:
aliases: [Natural Language Processing]
title: 自然语言处理
tags: []
date created: 2024-06-15
date modified: 2024-11-12
---

excerpt

<!-- more -->

最初是基于规则，做语法分析、语义分析，遇到了很大的瓶颈。70 年代以后，转向了基于统计的思路，获得了极大成功。
莱特兄弟发明飞机并不是靠仿生学，而是靠空气动力学。



[CS/+ 数学之美/阿拉伯数字.md]
====================

---
draw:
title: 阿拉伯数字
tags: []
date created: 2024-06-15
date modified: 2024-11-12
---

阿拉伯数字是古印度人发明的

<!-- more -->

阿拉伯数字标志着 [数字](数字.md) 和 [文字](文字.md) 的分离，也就是 int、short、boolean、long、double 和 char、String 的本质区别。



[CS/++Vue学习/vue学习$.md]
======================

---
draw:
tags: []
title: vue学习$
date created: 2024-07-17
date modified: 2024-12-27
---

```ts
.
├── dist                # 构建后的生产环境文件目录
│   ├── assets
│   ├── favicon.ico
│   └── index.html

├── package.json        # 项目配置和依赖声明文件
├── package-lock.json   # npm 依赖版本锁定文件

├── src                 # 源代码目录
│   ├── main.js            # 入口函数，要用 router、element-plus 这种都需要在这里写
│   ├── App.vue            # 根 Vue 组件
│   ├── router             # 各页面的跳转逻辑
│   ├── api                # 需要调的后端接口
│   ├── components         # 一些可复用的 vue 组件
│   ├── utils              # js 片段，一些可复用的工具函数
│   ├── assets
│   ├── config
│   ├── directive
│   ├── layout
│   ├── store
│   ├── theme
│   └── views

├── mock                # 放 mock 数据的地方
│   ├── adminTools.js
│   ├── baseTools.js
│   ├── priceTools.js
│   ├── temp.js
│   └── user.js

├── mockProdServer.js
├── index.html
├── node_modules       # npm 安装的依赖包目录，有点像 java 项目的 library
│   ├── @amap
│   ├── @babel
│   ├── vue-router
│   ├── vue-tsc
│   ├── vuex
│   ├── wrappy
│   └── zrender
├── pom.xml
├── public
│   └── favicon.ico
├── scripts
│   └── fix.sh
├── LICENSE
├── README.md
└── vite.config.js
```



[CS/Q1：大模型微调/Untitled.md]
=========================

---
draw:
tags: []
title: Untitled
date created: 2025-01-25
date modified: 2025-01-25
---

1、先学Python

2、再学[神经网络](https://zhida.zhihu.com/search?content_id=709971861&content_type=Answer&match_order=1&q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&zhida_source=entity)（有一些入门的视频教程，对入门很有帮助，如清华大学[马少平](https://zhida.zhihu.com/search?content_id=709971861&content_type=Answer&match_order=1&q=%E9%A9%AC%E5%B0%91%E5%B9%B3&zhida_source=entity)教授的跟我学AI的第一篇）

3、学习Transformer[架构](https://zhida.zhihu.com/search?content_id=709971861&content_type=Answer&match_order=1&q=%E6%9E%B6%E6%9E%84&zhida_source=entity)（不用特别深入，因为是比较复杂的）

4、实践[大语言模型](https://zhida.zhihu.com/search?content_id=709971861&content_type=Answer&match_order=1&q=%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&zhida_source=entity)的安装、微调、应用开发等（如看答主的书）

5、回过头再深入学习Transformer架构

  
  

作者：小五哥  
链接：https://www.zhihu.com/question/10212117702/answer/84285793895  
来源：知乎  
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



[CS/Q1：算法入门/Q3：kaggle 比赛/Coursera Deep Learning.md]
===================================================

---
draw:
tags: []
title: Coursera Deep Learning
date created: 2024-09-23
date modified: 2024-11-12
---

[Coursera: Deep Learning - CS自学指南 (csdiy.wiki)](https://csdiy.wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CS230/)



[CS/Q1：算法入门/Q3：kaggle 比赛/pandas.md]
===================================

---
draw:
tags: []
title: pandas
date created: 2024-10-13
date modified: 2024-12-27
---

pandas 在使用时语法感觉很乱，有什么学习的技巧吗？- 奕高的回答 - 知乎  
https://www.zhihu.com/question/289788451/answer/2167204031

## DataFrame

DataFramed 的本质是`Map<String, List>`，而 ndArray 的本质是 `List<List>`

DataFrame 是 pandas 库中最常用的数据结构之一。你可以把它想象成一个表格或者电子表格,就像 Excel 里的工作表。这个 " 表格 " 有行和列:

1. 每一列代表一种类型的数据 (比如姓名、年龄、成绩等)。
2. 每一行代表一条完整的记录。

NumPy的核心数据结构是ndarray（N-dimensional array，N维数组）。这与pandas的DataFrame有些不同，更像是Java中的数组，但可以有多个维度。

   - 所有元素类型相同，这使得操作更快，NumPy的ndarray更适合数学运算和科学计算。
   - pandas的DataFrame更适合处理表格型数据，特别是当你需要不同类型的数据或带标签的数据时。

实际上，pandas的DataFrame在内部使用了NumPy的ndarray。你可以把DataFrame看作是在ndarray基础上增加了更多功能的数据结构。



[CS/Q1：算法入门/Q3：kaggle 比赛/感知机.md]
================================

---
draw:
aliases: [MLP]
tags: []
title: 感知机
date created: 2024-07-13
date modified: 2024-11-12
---



[CS/Q1：算法入门/Q3：kaggle 比赛/训练赛.md]
================================

---
draw:
title: 训练赛
date created: 2025-02-03
date modified: 2025-02-14
---

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2025%2F02%2F03%2F00-26-31-d0edb57070afa6c36d885e41068db74f-202502030026766-97f7f0.png)



[CS/Q1：算法入门/Q3：kaggle 比赛/逻辑回归.md]
=================================

---
draw:
title: 逻辑回归
date created: 2024-09-22
date modified: 2025-02-15
---

#第一性原理 逻辑回归可以看作是没有隐藏层的[[神经网络]]，即单层感知机，哈哈，而多层感知机通过引入隐藏层和非线性激活，极大地提升了模型的表达能力，可以解决更复杂的分类（或回归）问题。

如果我们想要得到一个概率而不是简单的0或1，那么我们可以用[逻辑回归](逻辑回归.md)

## 1. 模型假设

对于输入特征向量 $\mathbf{x}$ 和参数向量 $\boldsymbol{\theta}$（也可以记作 $w$），逻辑回归先计算线性组合：

$z = \boldsymbol{\theta}^T \mathbf{x}.$

然后通过**sigmoid 函数**将 $z$ 映射到 (0,1) 区间，作为概率输出：

$h_\theta(x) = \sigma(z) = \frac{1}{1+e^{-z}}.$

这里 $h_\theta(x)$ 表示样本属于正类（比如标签为1）的概率。

---

## 2. 损失函数（[[交叉熵]]）

由于输出是概率，所以逻辑回归使用的是**对数损失函数（Log Loss，也叫交叉熵损失）**。对于单个样本，其损失可以表示为：

$J(\theta) = -\Bigl[y \log(h_\theta(x)) + (1-y) \log(1-h_\theta(x))\Bigr],$

其中：

- $y$ 是真实标签（0 或 1），
- $h_\theta(x)$ 是预测的概率。

对于整个训练集（共有 $m$ 个样本），平均损失为：

$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \Bigl[y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))\Bigr].$

---

## 3. 参数优化

和线性回归类似，我们的目标是调整参数 $\boldsymbol{\theta}$ 使得损失函数 $J(\theta)$ 达到最小。常见的方法有梯度下降：

1. **计算梯度**：对 $\theta_j$ 求偏导  
   $\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} \left(h_\theta(x^{(i)}) - y^{(i)}\right)x_j^{(i)}.$

2. **更新参数**：
   $\theta_j:= \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j},$  
   其中 $\alpha$ 是学习率。

通过不断迭代这个过程，就可以找到使得 $J(\theta)$ 最小的参数。

---

## 4. 总结

- **模型输出**：逻辑回归利用 sigmoid 函数将线性组合结果映射到 (0,1)，输出一个概率。
- **损失函数**：采用交叉熵损失（对数损失）来衡量预测概率与真实标签之间的差异。
- **参数优化**：通过梯度下降（或其他优化方法）不断调整参数 $\boldsymbol{\theta}$，直到损失函数最小。

这种方式使得逻辑回归在二分类问题中非常有效，也可以扩展到多分类问题（如使用 softmax 函数）。



[CS/Q1：算法入门/算法入门.md]
====================

---
draw:
title: 算法入门
date created: 2025-01-29
date modified: 2025-02-15
---

> 我也是大一进组，只不过在大一上学期先学习了mit1806，6041，6042j，6006，然后也开始做 DL，学了Stanford CS229 CS230 CS231n CS224n CS131。
>
> 看这些课收获特别大，做作业的时候收获更大，至今很多想法仍然来自于当时看课时的知识。
> 比如说我前几天去推在对抗攻击中怎么混合xxx最好，里面用了很多算子反向传播*时的解析解，而这些都是CS231n assignment2里的。再比如说之前做adversarial patch，能联想到deepdream*上去，然后用deepdream的一些解释来解释adversarial texture为什么在情况A和情况B下差别很大。总之这些资源真的对我帮助非常非常大。
>
> 看完这些课，并认真做完作业，我觉得就可以去选个方向follow论文，去进组。
> 只不过我一开始搞无监督学习，结果对我这种没资源的人来说是个大坑，啥都做不动。。后来转多模态，表征学习，对抗攻击，这些方向也越来越符合我的兴趣。我的兴趣可能比较奇特，就是喜欢对同一个问题从不同角度来解释，尤其是那些解释互相矛盾的时候，一下子就兴奋起来了。。就比如说diffusion，你既可以从ELBO推*，也可以从SDE推。为什么robustnesst和accuracy存在tradeoff，也可以从nonrobust feature的角度考虑，也可以rethinking一下robustness metric等等。
>
> 回归正题，我觉得AI最好的办法就是看这些课，写这些作业（尤其是作业）。这是我见过的最好的资源。我觉得直接去看论文，真的有些离谱，是特别低效的办法。看完这些课，再去选领域follow论文，我觉得不论是从知识的广度来说，还是深度来说，都是最优选择。

下面是一份基于原文核心内容的**要点提炼**，方便你快速了解作为算法/机器学习工程师在面试和日常工作中需要重点关注的领域。主要围绕文中提到的**三大能力**展开：**理论基本功、工程能力、业务理解能力**，并附上文中一些典型问题示例和工具建议。

---

## 一、理论基本功

1. **传统机器学习算法原理**
    
    - 经典三板斧：LR、SVM、GBDT 的原理及内部细节（正则化、损失函数、为什么好/不好等）。
    - 常用库的理解：比如 xgboost 的改进点、优势，以及与其它集成模型的对比。
    - 特征工程相关：如何做特征选择、特征构造，为什么这样做。
2. **深度学习基础**
    
    - **常见网络结构**：CNN、RNN、LSTM、GRU 等核心原理、使用场景及优缺点。
    - **梯度问题**：梯度消失/爆炸的原因和常见的解决方案（比如梯度裁剪、残差网络、正则化等）。
    - **激活函数**：ReLU、Sigmoid、Tanh 等的优缺点；为什么在实际训练中常用 ReLU。
    - **正则化**：L1 和 L2 正则化的区别与联系，为什么有助于防止过拟合。
3. **数学与统计基础**
    
    - 概率统计、线性代数、微积分等在机器学习中的应用。
    - 《统计学习方法》（李航）的相关内容：决策理论、生成模型与判别模型、EM 算法等。

> **提示**：扎实的理论基础能够帮助你在遇到新算法或前沿算法时更快上手并理解本质。

---

## 二、工程能力

1. **模型落地与部署**
    
    - **如何把模型部署到线上**：推理速度、资源占用、吞吐量等都要考量。
    - **模型压缩、剪枝、量化**：尤其是深度学习模型过大的时候，如何减小模型体积并加速推理，量化为 INT8、BF16 等格式的方法。
2. **性能优化与加速**
    
    - **数据预处理的速度**：大量训练数据的读取、清洗、转换往往是瓶颈。
    - **推理速度优化**：通过硬件指令集（如 AVX-512）或高性能计算库（oneDNN、OpenVINO、DAAL 等）实现推理提速。
    - **XGBoost 加速**：在 CPU 上如何使用 Intel 优化版 XGBoost，以及 Intel DAAL 库对数据分析/训练的加速。
3. **通用工程素养**
    
    - **编程能力**：算法岗同样需要考察代码风格、数据结构与算法题（LeetCode 类型）、系统设计等。
    - **版本管理、代码规范、自动化测试** 等通用软件工程的必备知识。

> **提示**：在实际工作中，工程实现与项目落地比"只会调包"复杂得多。对工程环节足够敏感并能提出或实现加速方案，往往是面试官非常看重的能力。

---

## 三、业务理解能力

1. **业务场景题**
    
    - 蔬菜价格预测：如何构造特征？为什么要这样做？
    - 用户画像预测：浏览数据判断用户是否有车，如何结合多种信息？
    - 如何构建可靠的评测集：指标设计、样本分布、线上线下的一致性等。
2. **理解业务需求，输出可行方案**
    
    - 为什么选择某种算法或模型？与业务目标（准确率、召回率、用户体验、实时性等）如何匹配？
    - 在"精度"与"速度"、"模型复杂度"与"易用性"之间做取舍，需要站在业务角度思考。
3. **整体流程的设计思维**
    
    - 面试官希望看到的是：能否从业务问题→数据处理→算法方案→指标评估→线上部署→持续迭代 全流程考虑。

> **提示**：很多情况下"会建模"只是前提，更重要的是能否结合实际场景与数据特点，提出可落地并有价值的解决方案。

---

## 四、常见的面试问题示例（文中提及）

- **深度学习相关**
    
    1. RNN、LSTM、GRU 的特点和区别。
    2. 梯度弥散/爆炸的成因和对应的解决方案。
    3. 常见激活函数（ReLU、Sigmoid、Tanh）的优缺点及应用场景。
    4. 为什么 L2 正则化能防止过拟合？L1 和 L2 的区别。
- **传统 ML 模型原理**
    
    1. LR、SVM、GBDT 的基本原理，损失函数、正则化项如何设计。
    2. xgboost 的改进点，以及它相比其他 GBDT 实现好在哪里。
    3. 特征工程：如何做特征组合、特征筛选、特征重要度评估。
- **工程/性能优化**
    
    1. 若模型太大、推理速度太慢，如何进行压缩、量化、剪枝？
    2. 线上模型如何部署以达到实时或准实时要求？
    3. 数据预处理成为瓶颈后如何提升效率？
    4. 使用英特尔 oneDNN、OpenVINO、DAAL 等工具如何提速？
- **业务场景题**
    
    1. 给你一个预测蔬菜价格的项目，如何选取和构造特征？
    2. 给定某个用户的浏览记录，如何判断此人是否有车？
    3. 如何设计一个稳定、能真实反映模型效果的评测集？

---

## 五、文中提到的工具和优化思路

- **Intel oneAPI 生态**
    
    - **oneDNN**：深度学习的底层优化库，针对 CPU 的各种指令集做了优化。
    - **OpenVINO**：对模型进行量化、压缩、加速推理；也包含对 OpenCV 的性能优化。
    - **DAAL**：主要面向传统机器学习和数据分析的加速（如加速 XGBoost）。
- **模型量化与指令集**
    
    - **AVX-512**：CPU 的矢量化指令，可提升矩阵运算和卷积操作的吞吐量。
    - **BF16 / INT8**：低精度运算在保留较高精度的同时，大幅提升训练和推理速度。

---

## 如何结合文章内容提升自己

1. **理论基础**：补齐统计学习方法、深度学习核心结构和数学的短板；重视《李航-统计学习方法》或斯坦福经典课程（CS229/CS231/CS224）的理论部分。
2. **工程实践**：
    - 多做项目：哪怕是小型项目也要完整跑通数据清洗、模型训练、部署、优化全流程；
    - 学会使用性能分析工具，了解如何做大规模分布式训练/推理、如何做量化/裁剪/部署。
3. **业务场景**：
    - 多问"为什么"：为什么要这样建模？为什么选择这种特征？为什么选择这个评价指标？
    - 尝试把自己的项目包装成一个完整的业务案例，从需求到上线都能说清楚。

---

### 结语

想从事算法/机器学习方向，需要在**算法理论、代码实现、业务理解**上都至少"达标"，然后再根据具体岗位和公司要求进行专项加强。文中提到的各种问题和工具（OpenVINO、oneDNN、DAAL 等）是典型的"工程落地"型面试热点，一定要结合自己的项目经验去理解和掌握。只会调包/炼丹可能无法支撑你走得很远，建议在工程与业务层面多下功夫。祝你求职和学习顺利！



[CS/Q2：做CS的经典lab/CMU 15-445 Database Systems/CMU 15-445  Database Systems.md]
=============================================================================

---
draw:
tags: []
title: CMU 15-445  Database Systems
date created: 2025-01-29
date modified: 2025-01-29
---

关于 Spring 2023 的通关指南，可以参考 [@xzhseh](https://github.com/xzhseh) 的这篇[CMU 15-445/645 (Spring 2023) Database Systems 通关指北](https://zhuanlan.zhihu.com/p/637960746)，里面涵盖了全部你需要的通关道具，和通关方式建议，以及最重要的，我自己在做 Project 的过程中遇到的，看到的，和自己亲自踩过的坑。

顾源:  
作为在cmu上过很多系统课（包括andy的445）的人劝你一句，如果你不读db phd的话，这门课的project真的是没啥用，项目设计的太局限于学术setting，本质还是沉迷于一些算法的实现，which如果你走工业界几乎没用，因为工业级代码里最简单的就是算法本身。劝你多实习，找一个热门/自己也感兴趣的方向，对于这些系统课包括445/6.824，最有价值的其实是lecture里的概念，这些课应该像字典/工具书一样，工作上遇到难题了经常回来看一看，而不是费时间做这些"将将能用"的toy project

Michael Xu (作者):  
有四点，

1. Bustub用到的最难的算法是Deadlock Detector里的DFS，和BPM里的LRU-K
2. 如果读/写Bustub（你所谓的 Toy Project）的代码都觉得吃力，我并不觉得去看工业界的代码提升有多大（反过来也一样，一个能熟练读懂工业界代码的同学，自身就自然知道需不需要做Project）
3. 光看不练，等于白看
4. 已经工作的同学，我相信随便一本工业界的工具书作为词典的作用，比这些Lectures/Notes的作用更大，i.e, DDIA is all you need

遂光者:  
自己作为数据库方向的phd，并且现在也在业界当一个数据库内核开发人员，整体上是比较同意guyuan的说法的。
15-445确实是一门好课，但是Bustub确实是过于toy了，如果想从事数据库内核研发，看Bustub的代码意义不大。pg、mysql、clickhouse的代码可以多看看（虽然mysql的代码比较丑），而且源码分析的博客也挺多的。

当然作者的说法也没错，每个人的需求不一样，很多新人没有大型项目的代码阅读&开发经验，做一个课程pj入门这个领域也挺好的。
总结就是，这个项目适合新人入门。新人入门后如果在这个项目上花很多时间做性能优化/细读这个项目其他模块的源码，则意义不大。去业界实习/看成熟数据库源码会更高效。



[CS/Q2：做CS的经典lab/CS144 Computer Network/CPP/algorithm.md]
=========================================================

---
draw:
tags: []
title: algorithm
date created: 2024-12-31
date modified: 2024-12-31
---

`<algorithm>` 是 C++ 标准库中的一个头文件，提供了大量有用的算法函数。以下是一些常用的算法：

```cpp
#include <algorithm>

// 1. min/max 相关
std::min(a, b);              // 返回较小值
std::max(a, b);              // 返回较大值
std::min({1, 2, 3, 4});      // 返回集合中最小值
std::max({1, 2, 3, 4});      // 返回集合中最大值

// 2. 查找相关
std::find(begin, end, value);           // 查找元素
std::find_if(begin, end, predicate);    // 条件查找
std::binary_search(begin, end, value);  // 二分查找

// 3. 排序相关
std::sort(begin, end);                  // 排序
std::reverse(begin, end);               // 反转

// 4. 复制和填充
std::copy(src_begin, src_end, dest);    // 复制
std::fill(begin, end, value);           // 填充

// 5. 计数和比较
std::count(begin, end, value);          // 计数
std::equal(begin1, end1, begin2);       // 比较
```

在 ByteStream 的实现中，可能会用到：

```cpp
// 例如在 write 函数中限制写入长度
size_t ByteStream::write(const string &data) {
    // 计算可写入的字节数
    size_t len = std::min(data.length(), remaining_capacity());
    // ...
}

// 或在 read 函数中
string ByteStream::read(const size_t len) {
    // 计算实际可读取的字节数
    size_t actual_len = std::min(len, buffer_size());
    // ...
}
```

相当于 Java 中的：

```java
// Java 等价功能
import java.util.Collections;  // 对应一部分算法
import java.util.Arrays;      // 对应另一部分算法

// 使用示例
Math.min(a, b);              // 对应 std::min
Collections.min(list);       // 对应 std::min 的集合版本
Arrays.sort(array);         // 对应 std::sort
Collections.reverse(list);  // 对应 std::reverse
```



[CS/Q2：做CS的经典lab/CS144 Computer Network/CPP/cmake.md]
=====================================================

---
draw:
tags: []
title: CMake
date created: 2024-12-30
date modified: 2025-01-21
---
- **含义**：CMake 是一个跨平台的构建系统生成器（build system generator）。它本身不直接编译你的代码，而是根据你在 `CMakeLists.txt` 中的配置，去生成实际的构建脚本（如 Makefile、Ninja 构建脚本、Visual Studio 工程文件等）。
- **作用**：
    1. **跨平台**：可以在不同操作系统、不同编译器工具链下使用，自动生成对应平台所需的工程文件或构建脚本；
    2. **统一配置**：把所有关于构建的选项（目标文件、依赖库、编译选项等）写在 `CMakeLists.txt` 中，减少不同平台之间的差异化配置；
    3. **依赖管理**：能比较方便地管理第三方库、头文件路径等内容。
- **与 toolchain 的关系**：
    - CMake 只是在高层负责"生成"构建配置，真正的编译、链接、打包还是要调用你本地或指定的 toolchain（如 `gcc`、`clang`、`cl.exe` 等）。
    - 你可以在 CMake 中指定 `-DCMAKE_TOOLCHAIN_FILE=xxx` 来告诉 CMake 该使用哪个特定的工具链文件，从而进行交叉编译或者在不同平台上生成可执行文件。

> **简单类比**：CMake 就像是"工厂流水线"的调度系统。它不生产产品，但它负责告诉每台机器怎么工作、用什么原料、生成什么产物。真正的加工过程还是由 toolchain（各种"机器"）来执行。

cmake 预处理，[make](2%20第二大脑/1%20知识/CS/编程语言/C/make.md)实际编译。

`cmake..` 它相当于一个"预处理"步骤，规划好如何编译整个项目  
`..` 表示 CMakeLists.txt 文件在上一级目录

- cmake 会读取 CMakeLists.txt 中的配置，检查依赖，生成相应的构建规则  
类似于在 Java 中使用：
- Maven 的 `mvn generate-sources`
- 或者 Gradle 的 `gradle build -x test`
- 它们都是在"准备构建环境"，解析项目配置（如 pom.xml 或 build.gradle）
- 类似于你在 IDEA 中第一次导入项目时，它要加载和配置项目的过程

`make` 实际执行编译过程，类似于

- Maven 的 `mvn compile`
- 或者在 IDEA 中点击"Build Project"按钮
- 就是真正的编译代码，生成可运行的程序
- 它会根据 cmake 生成的 Makefile 中的规则，调用编译器（g++）
- 编译源文件，链接库文件，最终生成可执行文件
- 它会自动判断哪些文件需要重新编译（根据文件修改时间）

总的来说：

- CMakeLists.txt 就像 Java 的 pom.xml 或 build.gradle
- cmake 就像 Maven/Gradle 这样的构建工具
- make 就像 IDEA 中的编译按钮

一个典型的工作流程是：

```bash
mkdir build      # 创建构建目录
cd build         # 进入构建目录
cmake ..         # 生成构建规则
make             # 执行编译
```

这种方式的好处是：

- 把构建文件与源代码分开（放在 build 目录）
- 便于清理（直接删除 build 目录即可）
- 支持多种构建配置（可以有多个不同的 build 目录）

## CMake 是 cpp 独有的吗

cmake 不是 C++ 独有的，它是一个通用的构建系统生成器，可以用于多种编程语言：

1. 最常用于：
- C
- C++
- Objective-C
- Fortran

1. 也支持其他语言：
- Java
- Python
- Swift
- Rust
- 等等

但在实际开发中：

- C/C++ 项目最常用 cmake
- Java 项目通常用 Maven/Gradle
- Python 项目通常用 pip/setuptools
- Rust 项目通常用 Cargo

cmake 之所以在 C/C++ 中流行，是因为：

1. C/C++ 需要处理复杂的依赖关系
2. 需要跨平台编译
3. 需要管理不同的编译器选项
4. 历史上缺乏统一的包管理工具

所以虽然 cmake 可以用于其他语言，但它主要还是在 C/C++ 生态系统中占主导地位。你在 CS144 中会大量使用它来管理项目构建。



[CS/Q2：做CS的经典lab/CS144 Computer Network/CPP/size_t.md]
======================================================

---
draw:
tags: []
title: size_t
date created: 2024-12-31
date modified: 2024-12-31
---

`size_t` 是一个无符号整数类型（unsigned integer type），它的具体大小取决于系统架构：

1. 在不同系统上的大小：

```cpp
// 32位系统
size_t = unsigned int       // 4字节 (0 到 4,294,967,295)

// 64位系统
size_t = unsigned long long // 8字节 (0 到 18,446,744,073,709,551,615)
```

1. 为什么使用 `size_t`：
- 专门用于表示大小、长度、计数等非负值
- 保证能够表示系统中任何对象的大小
- 在不同系统上自动选择合适的大小

1. 常见用法：

```cpp
size_t len = str.length();          // 字符串长度
size_t size = vector.size();        // 容器大小
size_t capacity = 1024;             // 缓冲区容量
size_t i = 0;                       // 循环计数器

// 在循环中使用
for (size_t i = 0; i < str.length(); i++) {
    // ...
}
```

1. 对比其他类型：

```cpp
// C++
int           // 有符号，通常4字节，-2,147,483,648 到 2,147,483,647
long          // 有符号，4或8字节（取决于系统）
size_t        // 无符号，4或8字节（取决于系统），总是 >= 0
```

```java
// Java
int           // 4字节，-2,147,483,648 到 2,147,483,647
long          // 8字节，-9,223,372,036,854,775,808 到 9,223,372,036,854,775,807
```

1. 注意事项：

```cpp
// 小心负数！
size_t x = 10;
x = x - 20;    // 危险！会变成一个很大的正数，因为 size_t 是无符号的

// 正确的做法是先检查
if (x >= 20) {
    x = x - 20;
} else {
    // 处理错误情况
}
```

所以总结：

1. `size_t` 不是 `long` 也不是 `int`，而是一个独立的无符号类型
2. 它的大小是平台相关的（4或8字节）
3. 它专门用于表示大小和索引
4. 它总是非负的
5. 在 ByteStream 这样的实现中，用 `size_t` 来表示缓冲区大小和读写长度是最合适的



[CS/Q2：做CS的经典lab/CS144 Computer Network/CPP/命名空间.md]
====================================================

---
draw:
tags: []
title: 命名空间
date created: 2024-12-31
date modified: 2024-12-31
---

`std::` 是 C++ 中的命名空间（namespace）标识符，`std` 是标准库（Standard Library）的命名空间。

对，很好的类比！让我详细对比一下 Java 的全类名和 C++ 的命名空间：

1. 基本对比：

```cpp
// C++
std::vector<int> numbers;
```

```java
// Java 全类名
java.util.ArrayList<Integer> numbers;
```

2. 引入/导入方式：

```cpp
// C++ 的三种方式
// 1. 直接使用全名
std::string name;

// 2. 单个引入
using std::string;
string name;

// 3. 引入整个命名空间（不推荐在头文件中使用）
using namespace std;
string name;
```

```java
// Java 的两种方式
// 1. 直接使用全名
java.util.ArrayList<Integer> list;

// 2. import 导入
import java.util.ArrayList;
ArrayList<Integer> list;
```

3. 嵌套的命名空间/包：

```cpp
// C++ 嵌套命名空间
namespace com {
    namespace myproject {
        namespace util {
            class Array { };
        }
    }
}

// 使用
com::myproject::util::Array arr;

// C++17 更简洁的写法
namespace com::myproject::util {
    class Array { };
}
```

```java
// Java 包
package com.myproject.util;

public class Array {
    // ...
}

// 使用
com.myproject.util.Array arr;
```

4. 实际使用示例：

```cpp
// C++
#include <string>
#include <vector>
#include <iostream>

// 不同库的相似名称类
std::string str;           // 标准库的字符串
boost::string str2;        // Boost库的字符串
myproject::string str3;    // 自定义库的字符串
```

```java
// Java
import java.util.List;
import com.myproject.List;  // 自己的 List 实现

// 使用全类名避免冲突
java.util.List<?> list1;    // Java标准库的List
com.myproject.List list2;   // 自己的List实现
```

主要区别：

1. Java 的包系统：
   - 与文件系统目录结构对应
   - 包名必须与目录结构匹配
   - 一个文件只能属于一个包

2. C++ 的命名空间：
   - 更灵活，不需要与文件系统对应
   - 可以跨文件定义同一个命名空间
   - 可以在一个文件中定义多个命名空间

但是核心目的是一样的：

- 避免命名冲突
- 组织代码结构
- 提供代码的模块化管理



[CS/Q2：做CS的经典lab/CS144 Computer Network/CPP/数据类型.md]
====================================================

---
draw:
tags: []
title: 数据类型
date created: 2025-01-23
date modified: 2025-01-23
---

在 Java 中并没有真正的"无符号"基本整型，与 C++ 的 `unsigned int` 或 `uint16_t` 并不完全一一对应。可以理解为：

1. **`uint16_t` (C++)**
    
    - 这在 C++ 中是"无符号 16 位整数"，取值范围是 `[0, 65535]`。
    - **Java 中并没有无符号 16 位整数**，只有 `short`（有符号，范围 `[-32768, 32767]`）和 `char`（无符号，范围 `[0, 65535]`，但主要用于表示 Unicode 字符）。
    - 如果你只是要存储并能完整表示 `uint16_t` 的数值范围（0 到 65535），**可以使用 `char`**（它是 16 位无符号整型），或者使用 `int` 来存储再进行逻辑上的"无符号"运算（需要自己防止越界和负数问题）。
    - 最常见做法：**若仅需存取 16 位数据本身，可以用 `char`**；若要做很多算术操作、或与其他整型混合运算，通常用 `int` 做"容器"，再用位运算来处理无符号逻辑。
2. **`unsigned int` (C++)**
    
    - 在大多数现代环境中通常是 32 位（C++ 标准只规定它不能短于 16 位，但常见实现是 32 位，取值范围 `[0, 4294967295]`）。
    - **Java 中没有 32 位无符号类型**，`int` 是 32 位**有符号**，范围 `[-2^31, 2^31 - 1]`。
    - 若只是想"按 32 位模式"存储并可做一些位运算，**可以直接用 Java 的 `int`**，但要注意超过 `Integer.MAX_VALUE`（2147483647）时会出现负数表现，需要在逻辑里把它当作"无符号"来解释（比如对外打印时需要 `Integer.toUnsignedLong(value)` 之类的方法）。
    - 如果需要完整存储 0~4294967295 的数值并做加减乘除等算术，**最好用 `long`**。这样就能保证 0～4294967295 不会溢出或变负数。不过需要自己确保不超出 `long` 的范围（但其实 `long` 的范围是 ±9,223,372,036,854,775,807，所以这部分足够放下无符号 32 位了）。

---

## 简要对照表

|C++ 类型|通常位宽|典型取值范围|Java 可能对应（近似）|说明|
|---|---|---|---|---|
|`uint16_t`|16 位|0 ～ 65535|`char`（16 位无符号）或 `int`|`char` 也是 0～65535，但主要是字符类型；若需要大量算术运算，可以用 `int` 并自行处理"无符号"逻辑。|
|`unsigned int`|32 位|0 ～ 4294967295|`int`（32 位有符号）或 `long`|若用 `int` 做位运算要注意负数显示，若要完整保存 0～4,294,967,295 并做算术，常用 `long`。|

简而言之，**Java 无直接等价的无符号整型**，需要根据具体用途选择：

- 只想存储 16 位或 32 位的无符号数据（可能只是做位操作），常用 `char` / `int`。
- 要在数值层面真正容纳无符号 32 位或更大范围的加减乘除，常用 `long`。

---

在 C++ 中，`size_t` 是一个无符号整数类型，通常用于表示大小或内存容量。它的位宽和范围依赖于平台（即系统架构）：

- **32 位系统**：`size_t` 通常是 32 位无符号整数，对应的范围是 `0 ~ 4,294,967,295`。
- **64 位系统**：`size_t` 通常是 64 位无符号整数，对应的范围是 `0 ~ 18,446,744,073,709,551,615`。

在 **Java 中**，并没有直接等价于 `size_t` 的类型。Java 中所有的整数类型都是有符号的，但根据 `size_t` 的使用场景，你可以选择适合的替代类型：

---

## Java 中的近似替代方案

|**C++ 类型**|**用途**|**Java 对应类型**|**说明**|
|---|---|---|---|
|`size_t`|表示大小（通常非负数）|**`int`**（32 位）|适用于绝大多数情况，尤其在内存地址范围小于 2GB 的场景。|
|`size_t`|表示大范围大小（64 位系统）|**`long`**（64 位）|在需要表示超大范围（如数组索引、文件大小等）时，选择 `long`。|



[CS/Q2：做CS的经典lab/CS144 Computer Network/CPP/构造函数.md]
====================================================

---
draw:
tags: []
title: 构造函数
date created: 2025-01-01
date modified: 2025-01-15
---

```cpp
ByteStream::ByteStream(const size_t cap) : 
buffer(), capacity(cap), end_write(false), end_read(false), written_bytes(0), read_bytes(0) {}
```

上面等价于下面

```java
public class ByteStream {
    private Queue<Byte> buffer;
    private final int capacity;
    private boolean endWrite;
    private boolean endRead;
    private int writtenBytes;
    private int readBytes;

    // 构造函数
    public ByteStream(int cap) {
        this.buffer = new LinkedList<>();  // 相当于 C++ 中的 buffer()
        this.capacity = cap;               // 相当于 C++ 中的 capacity(cap)
        this.endWrite = false;             // 相当于 C++ 中的 end_write(false)
        this.endRead = false;              // 相当于 C++ 中的 end_read(false)
        this.writtenBytes = 0;             // 相当于 C++ 中的 written_bytes(0)
        this.readBytes = 0;                // 相当于 C++ 中的 read_bytes(0)
    }
}
```

这是 C++ 的构造函数（Constructor），相当于 Java 中的构造函数。让我们对比一下：

```cpp
// C++ 构造函数
class TCPReceiver {
    TCPReceiver(const size_t capacity) 
        : _reassembler(capacity),    // 初始化成员变量
          _capacity(capacity),       // 初始化成员变量
          _synReceived(false),      // 初始化成员变量
          _finReceived(false),      // 初始化成员变量
          _isn(0)                   // 初始化成员变量
    {}
};
```

```java
// Java 等价代码
public class TCPReceiver {
    private StreamReassembler _reassembler;
    private long _capacity;
    private boolean _synReceived;
    private boolean _finReceived;
    private int _isn;
    
    // 构造函数
    public TCPReceiver(long capacity) {
        this._reassembler = new StreamReassembler(capacity);
        this._capacity = capacity;
        this._synReceived = false;
        this._finReceived = false;
        this._isn = 0;
    }
}
```

主要区别：

1. 初始化语法：

   ```cpp
   // C++ 使用初始化列表（:后面的部分）
   TCPReceiver(const size_t capacity) 
       : _reassembler(capacity),  // 成员初始化列表
         _capacity(capacity)
   {}
   
   // Java 在构造函数体内初始化
   public TCPReceiver(long capacity) {
       this._reassembler = new StreamReassembler(capacity);
       this._capacity = capacity;
   }
   ```

2. 参数传递：

   ```cpp
   // C++ 使用 const 表示参数不会被修改
   TCPReceiver(const size_t capacity)
   
   // Java 没有 const
   public TCPReceiver(long capacity)
   ```

3. 成员变量初始化：

   ```cpp
   // C++ 可以在声明时初始化
   bool _synReceived{false};  // 或在初始化列表中初始化
   
   // Java 可以在声明时初始化
   private boolean _synReceived = false;  // 或在构造函数中初始化
   ```

这个构造函数的作用是：

1. 创建一个新的 TCP 接收器
2. 设置其缓冲区容量
3. 初始化所有状态变量：
   - `_reassembler`: 用于重组数据的组件
   - `_capacity`: 缓冲区容量
   - `_synReceived`: 是否收到 SYN 标志
   - `_finReceived`: 是否收到 FIN 标志
   - `_isn`: 初始序列号

就像是在准备一个新的邮件接收系统：

1. 设置收件箱大小（capacity）
2. 准备一个分类系统（reassembler）
3. 初始化所有状态标记
4. 设置起始编号



[CS/Q2：做CS的经典lab/CS144 Computer Network/CPP/流操作符.md]
====================================================

---
draw:
tags: []
title: 流操作符
date created: 2024-12-31
date modified: 2024-12-31
---

在 C++ 中：

1. `cout` vs Java的 `System.out.println`

   ```cpp
   // C++
   cout << "Hello World";  // 输出到控制台
   
   // Java
   System.out.println("Hello World");  // 等价的Java代码
   ```

2. `<<` 是 C++ 中的"流操作符"（stream operator）
   - 它的作用是将右边的数据"流入"左边的输出流
   - 可以连续使用：

   ```cpp
   // C++
   cout << "Hello" << " " << "World" << endl;
   
   // 等价的 Java 代码
   System.out.println("Hello" + " " + "World");
   ```

3. 一些常见的 C++ 输入输出对比：

   ```cpp
   // C++ 输出
   cout << "Hello";          // 输出不换行
   cout << endl;             // 换行
   cout << "Hello" << endl;  // 输出并换行
   
   // Java 等价代码
   System.out.print("Hello");        // 输出不换行
   System.out.println();             // 换行
   System.out.println("Hello");      // 输出并换行
   ```

4. C++ 还有其他流：

   ```cpp
   cout << "标准输出";   // 标准输出，等价于 System.out
   cerr << "错误输出";   // 错误输出，等价于 System.err
   ```

所以当你看到：

```cpp
while (!socket.eof()) {
    cout << socket.read();
}
```

这相当于 Java 中的：

```java
while (!socket.eof()) {
    System.out.print(socket.read());
}
```

C++ 使用 `<<` 的设计允许更灵活的链式操作，你可以一次输出多个不同类型的值：

```cpp
cout << "Number: " << 42 << " String: " << "Hello" << endl;
```



[CS/Q2：做CS的经典lab/CS144 Computer Network/CS144 Computer Network.md]
==================================================================

---
draw:
tags: []
title: CS144 Computer Network
date created: 2024-09-21
date modified: 2025-01-23
---

## 环境部署

Ubuntu 18.04 [BYO Linux installation (stanford.edu)](https://stanford.edu/class/cs144/vm_howto/vm-howto-byo.html)  
[CS144Winter2024--Chapter0--wsl2+docker环境搭建_cs144环境配置-CSDN博客](https://blog.csdn.net/weixin_73074012/article/details/135847082)

[2021 CS144 实验笔记 | 沧海月明 (inlighting.org)](https://www.inlighting.org/archives/2021-cs144-notes)

用 Clion 里的 remote 开发

## 参考资料

CS 自学指南  
https://github.com/PKUFlyingPig/cs-self-learning  
官网  
https://www.scs.stanford.edu/10au-cs144/  
小林 coding  
https://xiaolincoding.com/network/  
lab 思路  
[CS144 Lab0翻译 | Doraemonzzz](https://doraemonzzz.com/2022/01/30/2022-1-30-CS144-Lab0%E7%BF%BB%E8%AF%91/)  
https://www.cnblogs.com/kangyupl/p/stanford_cs144_labs.html  
[CS 144: Introduction to Computer Networking](https://cs144.github.io/)  
[CS144计算机网络 Lab0 | Kiprey's Blog](https://kiprey.github.io/2021/11/cs144-lab0/)  
[【计算机网络】Stanford CS144 Lab Assignments 学习笔记 - 康宇PL - 博客园 (cnblogs.com)](https://www.cnblogs.com/kangyupl/p/stanford_cs144_labs.html)  
lab 答案  
https://github.com/PKUFlyingPig/CS144-Computer-Network

## Lab

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2025%2F01%2F23%2F17-01-10-fe2b3215855e029d9038a0a0d12e3674-202501231701193-9c99f1.png)

|       | Lab                               | Tasks                                                                        | 认知                                       |
| ----- | --------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------- |
|       | [[Lab0 ByteStream]]               | • 实现字节流(ByteStream)<br>• 熟悉环境设置和基础工具<br>• 主要是热身练习，为后续实验打基础                   | ByteStream的本质是一个 char 的deque 的 service 层 |
| TCP 层 | [[数据流重组 Lab1 Stream Reassembler]] | • 实现TCP的流重组器(StreamReassembler)<br>• 处理乱序到达的数据并重组为有序字节流<br>• 模拟TCP如何处理失序的数据包 |                                          |
|       | [[Lab2 TCP Receiver]]             | • 实现TCP接收端<br>• 处理序列号(Sequence Numbers)<br>• 管理接收窗口(Receive Window)          |                                          |
|       | [[Lab3 TCP Sender]]               | • 实现TCP发送端<br>• 实现重传机制<br>• 实现流量控制                                           |                                          |
|       | [[Lab4 TCP Connection]]           | • 将之前的Sender和Receiver组合<br>• 实现完整的TCP连接<br>• 处理连接的建立和终止                      |                                          |
| IP 层  | [[Lab5 Network Interface]]        | • 实现网络接口层<br>• ARP (Address Resolution Protocol)协议<br>• 以太网帧的处理              |                                          |
|       | [[Lab6 Router]]                   | • 实现路由器的核心功能<br>• 路由表的查找和维护<br>• IP数据包的转发                                    |                                          |
| 额外    | 研究不过的 case，尝试 gdb 做 debug         |                                                                              |                                          |

## Shell 命令

```shell
# 1. 首先，回到项目根目录
cd /Users/liuyishou/usr/projects/CS144-Computer-Network

# 2. 创建（如果不存在）并进入 build 目录
mkdir -p build
cd build

# 3. 运行 cmake
cmake ..

# 4. 编译项目
make format (使编码风格正常化)
make        (确保代码可以编译)

# 5. 测试
make check_lab0 (确保自动测试通过)


# 5. 运行 webget
./apps/webget stanford.edu /class/cs144

```

## 思考

1. 做 lab 的过程和工作中开发需求很像，[TCP](TCP.md)、[IP](IP.md)的理论就像产品经理写的 PRD，lab 代码就像我们开发的需求代码。网上各种网络方面的技术文章就像其他开发人员做完需求写的分享，看他们的分享远不如自己做一遍需求理解的深刻。
2. 完成 Lab 的过程中，一开始可能缺乏语言基础或者不熟悉这种学习模式，这时候，深入理解 git 的各种版本切换就很重要，找一份优质 lab 作业，穿梭在他各个完成阶段中抄作业，就像学霸陪自己学习一样。等基础强化，熟悉模式后可以自己再尝试独立完成一些。
3. 工作原因，不一定能短期做完一套 lab，这时候代码的原子性、健壮性很重要，最好每次做完一个 lab 都把测试用例全部跑过，这样每个小阶段都能有一个清晰的起点和终点。
4. orbstack 演示

To succeed in the labs, you'll need to be comfortable with C++ and debugging tools1. CS144 is a demanding course that requires strong programming skills. If you're struggling with C++, consider reviewing the basics or seeking help from the TAs.

It's also important to remember that the lab assignments are not just specifications to be followed blindly7. They require critical thinking and problem-solving skills. You'll need to understand the concepts behind the code and apply them to different scenarios.

When working on the labs, pay close attention to the different types of sequence numbers used in TCP8. These sequence numbers are crucial for implementing TCP correctly and ensuring reliable data transmission. You'll also need to familiarize yourself with the tools used in the labs, such as the Minnow library4. These tools provide a framework for your implementations and simplify the development process.

Understanding the concept of "bytes in flight" is essential for grasping TCP flow control and congestion control9. This concept refers to the amount of data that has been sent but not yet acknowledged. Managing bytes in flight is crucial for preventing network congestion and ensuring efficient data transfer.

Finally, pay attention to the endianness of data in network programming10. Endianness refers to the order in which bytes are stored in memory. Different computer architectures may use different endianness, and it's crucial to handle this correctly when sending and receiving data over the network.

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F23%2F11-51-40-d83ae1e55a6a6ef17e6e6b169e730006-202409231151421-2ce186.png)



[CS/Q2：做CS的经典lab/CS144 Computer Network/EOF.md]
===============================================

---
draw:
tags: []
title: EOF
date created: 2024-12-30
date modified: 2024-12-30
---

EOF (End Of File) 是"文件结束"的意思。在网络编程中，EOF 表示数据流的结束。

在你的 webget 程序中：

```cpp
while (!socket.eof()) {
    cout << socket.read();
}
```

这段代码的工作原理是：

1. `socket.eof()` 检查是否到达了数据流的末尾：
   - 当服务器完成数据发送并关闭连接时，客户端的 socket 会收到 EOF
   - 在 HTTP 中，这发生在服务器发送完所有响应数据后

2. 为什么需要检查 EOF：
   - HTTP 响应可能很大，不能一次性读取完
   - 需要循环读取直到没有更多数据
   - EOF 信号告诉我们服务器已经发送完所有数据

3. 实际场景举例：

```Java
客户端                    服务器
   |                        |
   |--- HTTP 请求 --------->|
   |                        |
   |<-- 响应第一部分 -------|
   |<-- 响应第二部分 -------|
   |<-- 响应第三部分 -------|
   |<-- EOF 信号 ------------|  (服务器关闭连接)
   |                        |
```

这就是为什么我们建议在发送完请求后调用 `socket.shutdown(SHUT_WR)`：

```cpp
socket.write(message);
socket.shutdown(SHUT_WR);  // 告诉服务器我们已经发送完请求
```

这样：

1. 服务器知道请求已经完成
2. 服务器会发送响应
3. 发送完响应后关闭连接
4. 客户端收到 EOF，知道所有数据都已接收



[CS/Q2：做CS的经典lab/CS144 Computer Network/Lab0 ByteStream.md]
===========================================================

---
draw:
tags: []
title: Lab0 ByteStream
date created: 2024-12-30
date modified: 2025-01-21
---

## 目标

实现了一个带容量限制的字节流 ByteStream，这为后续的 TCP 流量控制、可靠传输奠定基础。

   - write：向流中写入数据
   - read：从流中读取数据
   - peek_output：预览数据但不移除
   - pop_output：移除数据但不返回
   - 流量控制：通过 capacity 限制缓冲区大小
   - 状态跟踪：记录已读/已写字节数，检查流是否结束

## 通过 capacity 限制 是怎么体现出来的

这种容量限制机制模拟了 TCP 中的流量控制：

发送方不能无限制地发送数据  
接收方有一个固定大小的接收窗口  
当接收窗口满时，发送方必须等待接收方处理数据后才能继续发送

### write

```Java
【ByteStream 的缓冲区示意图】

capacity = 10 (总容量)
buffer.size() = 4 (已使用空间)
canWrite = capacity - buffer.size() = 6 (剩余可写空间)

+---+---+---+---+---+---+---+---+---+---+
| A | B | C | D |   |   |   |   |   |   |
+---+---+---+---+---+---+---+---+---+---+
|<--- 已使用 --->|<----- 剩余空间 ----->|
|<--------------- capacity ------------->|


假设要写入的数据：data = "HELLO" (长度为5)
canWrite = 6 (剩余空间)
canRead = min(6, 5) = 5 (实际可写入长度)

写入后的状态：
+---+---+---+---+---+---+---+---+---+---+
| A | B | C | D | H | E | L | L | O |   |
+---+---+---+---+---+---+---+---+---+---+
```

再举一个例子，如果要写入的数据比剩余空间大：

```Java
当前状态：
+---+---+---+---+---+---+---+---+---+---+
| A | B | C | D |   |   |   |   |   |   |
+---+---+---+---+---+---+---+---+---+---+

要写入：data = "HELLOWORLD" (长度为10)
canWrite = 6 (剩余空间)
canRead = min(6, 10) = 6 (只能写入6个字符)

写入后：
+---+---+---+---+---+---+---+---+---+---+
| A | B | C | D | H | E | L | L | O | W |
+---+---+---+---+---+---+---+---+---+---+
                    |<---- 实际写入 ---->|
```

计算逻辑：

1. `canWrite = capacity - buffer.size()`
   - 计算还有多少空闲空间可以写入
   - 就是总容量减去已使用空间

2. `canRead = min(canWrite, data.size())`
   - 比较剩余空间和要写入的数据长度
   - 取两者中的较小值作为实际可写入长度
   - 这样可以避免缓冲区溢出

这就像往一个固定大小的水杯里倒水：

- capacity 是水杯的容量
- buffer.size() 是已经装的水量
- canWrite 是还能装多少水
- data.size() 是想要倒入的水量
- canRead 是实际能倒入的水量

## 参考

https://doraemonzzz.com/2022/01/30/2022-1-30-CS144-Lab0%E7%BF%BB%E8%AF%91/#4-%E5%86%85%E5%AD%98%E4%B8%AD%E5%8F%AF%E9%9D%A0%E7%9A%84%E5%AD%97%E8%8A%82%E6%B5%81

1. 在你的提交中，请只对`webget.cc`和`libsponge`顶层的源代码（`byte_stream.hh`和`byte_stream.cc`）进行修改。请不要修改`libsponge/util`中的任何测试或帮助程序。
2. 在提交任何作业之前，请按顺序运行这些：
    - (a) `make format`(使编码风格正常化)
    - (b) `make`(确保代码可以编译)
    - (c) `make check_lab0`(确保自动测试通过)
3. 完成编辑`writeups/lab0.md`，写上这项作业花了你多少时间，以及任何其他评论。
4. 准备提交时，请按照[https://cs144.github.io/submit](https://cs144.github.io/submit)。在提交之前，请确保你已经提交了你想要的一切。
5. 如有任何问题，请在周二晚上的实验课上尽快告知课程负责人，或在Piazza上发表问题。祝您好运，欢迎来到CS144!



[CS/Q2：做CS的经典lab/CS144 Computer Network/Lab2 TCP Receiver.md]
=============================================================

---
draw:
tags: []
title: Lab2 TCP Receiver
date created: 2024-12-30
date modified: 2025-01-17
---

## 参考

https://doraemonzzz.com/2021/12/27/2021-12-27-CS144-Lab2%E7%BF%BB%E8%AF%91/

https://doraemonzzz.com/2021/12/27/2021-12-27-CS144-Lab2/

## TCPReceiver

实现`TCPReceiver`。它将

- (1) 从它的对等方接收段，
- (2) 使用你的`StreamReassembler`重新组装ByteStream，并计算
- (3) 确认号(ackno)，和
- (4) 窗口大小。

在TCP中，**确认(acknowledgment)**意味着，"接收方需要的下一个字节的索引是什么，这样它才能重新组装更多的ByteStream？"这告诉发送方它需要发送或重新发送哪些字节。
**流量控制**意味着，"接收方感兴趣并愿意接收哪些索引范围"（通常作为其剩余容量的函数）。这会告诉发送方允许发送多少。

## wrap, unwarp

// 为了知道当前真正的数据在整条传输过程中的绝对位置，
// 并且要选取最合理的那一个圈数，尽量贴近我们已有的 checkpoint  
// 就像当我们看到时钟显示"9点"，并且知道现在是"下午2点"时，我们更可能认为这 "9点"是指同一天的上午，而不是第二天的上午。

下面给出的解释希望能帮助你用"5 位"和"10 位"这样更小的数字范围来直观地理解 CS144 Lab2 中 `wrap` 和 `unwrap` 这两个函数在做什么。实际代码里用的是 32 位无符号整数进行"环绕"，但把位数缩小后更容易想象那个"环绕"的过程。

---

## 一、什么是"环绕"？

### 1. 场景类比

- **5 位场景**
    - 5 位无符号整数可以表示的范围是 0～31，共 32 个值。
    - 如果我们把一个比较大的"绝对序号"压缩到这 5 位里，就相当于取模 32（因为 2^5 = 32）。
    - 当数值超过 31 再往上加时，就会从 0 重新开始，形成"环绕"(wrap around)。比如说：
        - 31+1≡0 (mod 32)31 + 1 \equiv 0 \ (\text{mod } 32)
        - 31+2≡1 (mod 32)31 + 2 \equiv 1 \ (\text{mod } 32)
    - 这就像一个表盘只有 32 个刻度，指针走满 32 格又回到起点。
- **10 位场景**
    - 10 位无符号整数可以表示的范围是 0～1023，共 1024 个值。
    - 同理，如果我们有一个较大的数，把它"压缩"到 10 位，就是对 1024 取模。超出 1023 的数值也会再次环绕，从 0 继续。

### 2. 在 Lab2 中的角色

在 Lab2 里，`WrappingInt32` 就是一个 **32 位** 的环绕计数器。但为了便于理解，我们先用"小规模"的 5 位或 10 位来做类比演示。

- **`wrap`**：将一个 64 位的"绝对序号"nn 映射到一个环绕的 32 位(这里举例用 5 位/10 位)。
- **`unwrap`**：反过来，根据一个环绕的 32 位数值 nn，推算出它所对应的、**最接近给定检查点(checkpoint)** 的 64 位"绝对序号"。

---

## 二、`wrap` 函数：把大数"压缩"进环绕计数

代码：

```cpp
WrappingInt32 wrap(uint64_t n, WrappingInt32 isn) {
    return isn + uint32_t(n);
}
```

用 5 位或 10 位去理解：

1. `isn`（Initial Sequence Number）相当于初始偏移量。
2. `n` 是一个很大的 64 位数（绝对序号）。
3. 把 `n` 转成和 `WrappingInt32` 同样大小（在真实场景下是 32 位），然后再加上 `isn` 进行环绕。

### 1. 用 5 位举例

- 设 `isn = 7` (当然也必须在 0～31 范围)。
- `n = 37`（这时相当于一个 64 位的大数，但为了演示用小数即可）。
- 如果仅仅对 5 位取模，`n % 32 = 37 % 32 = 5`。
- 我们再加上初始偏移量 `isn = 7`：7+5=12 (mod 32) 7 + 5 = 12 \ (\text{mod } 32)
- 结果就是 `12`，表示这个环绕后的值最终落在 5 位整数里的 `12` 这个位置。

### 2. 用 10 位举例

- 设 `isn = 100` (必须在 0～1023)。
- `n = 1500`（还是为了演示）。
- 先做 nmod  1024=1500mod  1024=476n \mod 1024 = 1500 \mod 1024 = 476。
- 再加上 `isn = 100`：100+476=576 (mod 1024) 100 + 476 = 576 \ (\text{mod } 1024)
- 结果 `576` 就是 10 位环绕后的数值。

---

## 三、`unwrap` 函数：找回最接近的"绝对序号"

代码（简化注释过的思路）：

```cpp
uint64_t unwrap(WrappingInt32 n, WrappingInt32 isn, uint64_t checkpoint) {
    // 1) 先把 n 和 isn 的差转换到 uint64_t 上
    // 2) 再在可能的几个“区间”里，挑一个最接近 checkpoint 的值

    uint64_t tmp;
    if (n - isn < 0) {
        tmp = uint64_t(n - isn + (1ULL << 32));
    } else {
        tmp = uint64_t(n - isn);
    }

    // 如果 tmp 本身已经 >= checkpoint，就返回 tmp
    // 否则在其高位（每隔 2^32 一个区间）上加相应偏移，找到最接近 checkpoint 的那一个
    if (tmp >= checkpoint) 
        return tmp;

    tmp |= ((checkpoint >> 32) << 32);   // 对齐到 checkpoint 相同的高位
    while (tmp <= checkpoint) 
        tmp += (1ULL << 32);

    uint64_t tmp1 = tmp - (1ULL << 32);
    return (checkpoint - tmp1 < tmp - checkpoint) ? tmp1 : tmp;
}
```

这段逻辑听上去比较拗口，我们用 5 位或 10 位的思路来做比喻，就会更好理解。

### 1. 用 5 位场景"unwrap"举例

- **背景**：现在我们的"环绕计数器"只有 5 位能表示，也就是只能表示 0～31。
- **参数**：
    - `n`：已经过环绕的序号（0～31 之间），比如这里是 `n=12`；
    - `isn`：还是那个初始偏移量，比如我们仍然用 `isn=7`；
    - `checkpoint`：一个最近用到的"绝对序号"（64 位的大数），比如我们现在假设是 `100`（当然 100 已经大于 31 了，它是"绝对编号"）。

具体过程：

1. **先算出最基础的差值**
    

    tmp=(n−isn)mod  32 \text{tmp} = (n - isn) \mod 32

    - 因为是 5 位的，所以实际操作会像对 32 取模那样。如果 `(n - isn)` 是负数，我们就加上一圈（即 +32）让它变成一个非负值。
    - 假设 `n=12`, `isn=7`，则 `n - isn = 5`，这时是正数，就不用加 32。
    - 所以 `tmp = 5`。
2. **结合 checkpoint 的高位，拼出一个 64 位候选值**
    
    - 在 5 位里，`tmp = 5` 仅仅表示在 0～31 这个小范围里是 5。
    - 但在"绝对世界"里，这个 5 可能是：
        - 5
        - 5 + 32
        - 5 + 64
        - 5 + 96
        - … 甚至一直加 32 (因为 2^5 = 32)，直到我们能"追上"或"超过" `checkpoint=100`。
    - 代码里 `tmp |= ((checkpoint >> 32) << 32);` 这个操作对应的是"先将 `tmp` 的高 32 位对齐到跟 `checkpoint` 相似的区域"。不过在 5 位的类比里，其实对应的是"尝试对齐到跟 checkpoint 相同的某个大区间，然后每次加 32 才能跳到下一个区间"。
3. **找出最接近 checkpoint 的那个值**
    
    - 一旦我们找到了一个"候选的 tmp"比 `checkpoint` 大，那么再往回退一圈（减去 32）就能得到另一个候选值 `tmp1`，这样我们就有两个候选值：
        - `tmp`（稍微超出或刚好等于 checkpoint）
        - `tmp1`（比 checkpoint 小一点或很接近）
    - 对比这两个候选值和 checkpoint 的距离，选取差值更小的那个，就是最终返回的"绝对序号"。

拿我们举的 `checkpoint=100` 来说：

- 先得到 `tmp = 5`；
- 然后一步步加 32：
    - `5 + 32 = 37`（还小于 100），继续加：
    - `37 + 32 = 69`（还小于 100），继续加：
    - `69 + 32 = 101`（终于大于 100）
- 此时可以拿到两个候选值：`101` 和 `101 - 32 = 69`。
    - `|100 - 69| = 31`
    - `|101 - 100| = 1`
    - 明显 101 离 100 更近，所以就返回 101 这个结果。

这样，"unwrap" 就把原先只有 5 位的 `n=12` 转回了在绝对序号里离 100 最近的那个真实值 **101**。

### 2. 用 10 位场景"unwrap"举例

- **背景**：现在我们的环绕计数器是 10 位，也就是 0～1023。
- **参数**：
    - `n`：环绕后的序号(0～1023)，比如是 `n = 576`；
    - `isn`：初始偏移量，比如 `100`；
    - `checkpoint`：可能是一个较大的"绝对序号"，比如 `5000`。

1. 先计算 `tmp = (n - isn) mod 1024`。若为负，就加 1024。
2. 将 `tmp` 贴合到与 `checkpoint` 相同或相近的高位区间（因为 10 位取模只是底下 10 位的值，前面高位可以是许多倍的 1024）。
3. 找到"最贴近 checkpoint" 的那个绝对值。

假设最后发现最接近 `checkpoint=5000` 的值是 `4984`，那这个就是 `unwrap` 返回的结果。

---

## 四、总结

1. **`wrap`**：做的事就像"模运算 + 初始偏移量"，把一个大数折叠进小小的环状空间（在实际代码中是 32 位，但我们用 5 位或 10 位来类比更易理解）。
2. **`unwrap`**：则是根据你给定的环绕后的序号，加上 `isn` 这个偏移，再去"对齐"到最靠近 `checkpoint` 的那个绝对值。因为环绕会有无数个"间隔（加或减 2^32）"可能都对应相同的环绕值，所以要挑一个和 `checkpoint` 最近的。

在 5 位/10 位的小例子里，可以用"加或减 2^5 / 2^{10}"的方式去找到距离最近的那个绝对序号。在代码层面，就是利用了移位或循环加减来模拟这种"找正确圈数"的过程。

这样一来，你就能形象地理解：

- 为什么要有"环绕"（因为 TCP 的序号是 32 位，数据多了以后会溢出回到 0）。
- 为什么还要把 32 位的环绕数"还原"到 64 位的大数（为了知道当前真正的数据在整条传输过程中的绝对位置，并且要选取最合理的那一个圈数，尽量贴近我们已有的 `checkpoint`）。

希望这个用 5 位、10 位作类比的说明能让你对 CS144 Lab2 中的 `wrap` / `unwrap` 核心逻辑有更直观的理解。祝学习顺利!

----

## 绝对序号

下面给出一个形象化（直观）的解释，帮助大家理解为什么要在计算绝对序号时，对比 `± (1ul << 32)` 后选择与最近一次 `checkpoint` 最接近的那个。

---

### 1. "32 bits 的 seqno 会循环"这个特性

#### 场景比喻：12 小时制的时钟

- **传统的 TCP sequence number 是 32 bits，会发生"回绕（wrap around）"。**  
    就像一个只有 12 小时刻度的时钟，会在指针转满一圈后，从 `11:59` 跳回到 `0:00`，继续循环使用。
- 所以你看到的"时钟显示"**不是唯一**的，它每 12 小时就会从头开始，无法直接区分今天的 10:00 和昨天的 10:00。

#### 问题：如何确定"真正的时间"？

- 如果只知道当前时间是 "10:00"，并不知道现在是上午还是晚上，单看时针本身并不能确定具体是哪一天或哪一次循环。
- **但我们有一个"基准点（checkpoint）"**：比如我们知道"最近一次我们确认的时间是 9:58 AM"，那么我们就可以猜测：现在这 10:00 很可能就是在"9:58 AM 之后的 2 分钟"，而**不会**是隔了 12 个小时以后的 10:00 PM。

---

### 2. "绝对序号"就是不回绕的时间线

- **对比"12 小时制时钟"**：你可能用一个 24 小时制或干脆用一个"Unix 时间戳"来记录不同时刻，它不会循环回绕，也不会重复。
- 在 TCP 协议里，**我们自己维护一个"64 bits 不回绕"的 absolute sequence number**，对应真实的"从数据流开始到现在"的字节位置计数。
- 当对方发来一个 32 bits 的 seqno（就是"12 小时制的时间"），要把它折算到我们自己的 64 bits 时间轴（就是"Unix 时间戳"）。

---

### 3. 为什么要选"与 checkpoint 最接近的那个"？

#### 原理 / 直觉

- "seqno" 可能是 "时钟"上的一个读数，例如 "10:00"。
    
- 我们当前维护的 "checkpoint" 表示我们上一次计算出的 **absolute seqno**（相当于"上次确认好的 Unix 时间戳"）。
    
- 当我们再看到一个新的 "10:00" 时，为了转换到"Unix 时间戳"上，就要判断：
    
    - 它是"9:58 AM" 后 2 分钟吗？
    - 还是其实已经过了 12 个小时，变成了"10:00 PM"？
    - 甚至可能过了 24 小时、36 小时...（只是极少见/几乎不可能发生那么大延迟）
- **正常情况下**，两个分段（segment）相邻到达的序列号不会相差一个 **2^31**（＝INT32_MAX）以上的量，除非真的隔了很多小时、甚至几天才到达，这在常规网络里非常罕见。
    

#### 数学实现

- 对于给定的 `seqno_32`（32 bits seqno），我们先用已知的 `checkpoint`（64 bits absolute seqno）来猜测：
    - 先把 `seqno_32` 转成一个基础的 `abs_seqno_candidate`，比如直接 `(seqno_32 - ISN) + 0`；
    - 再分别尝试加/减 `1 << 32`，形成多个可能的候选值：
        1. candidate0=base\text{candidate}_0 = \text{base}
        2. candidate1=base+232\text{candidate}_1 = \text{base} + 2^{32}
        3. candidate−1=base−232\text{candidate}_{-1} = \text{base} - 2^{32}
- 选出其中与 `checkpoint` **差的绝对值最小**的那一个，就代表"最有可能的绝对时间（绝对序号）"。

**直观理解**：

- 就像"现在是 10:00"，可能对应今天上午 10:00、今晚 10:00、明天 10:00...
- 但根据你"刚才是 9:58"的记录，肯定选"上午 10:00"（只差 2 分钟）最合理，而不是"隔 12 小时"或更久以后。

---

### 4. 结论

在实现时：

1. **存储一个"checkpoint"（上一次确认好的 64 bits absolute seqno）**。
2. 每次接收新的 32 bits `seqno` 时，算出几个可能的 64 bits 候选值。
3. **选择距离 `checkpoint` 最接近的候选值**。
4. 这样就可以将一个会回绕的 32 bits 序号，映射到一个不回绕的 64 bits **absolute seqno** 上。

**形象总结**：

- 32 bits 的序号就像一个"12 小时制时钟"不停地转圈；
- `checkpoint` 则是上一次你确认了"确切的时间"。
- 当你再看到一个钟面读数时，就把它对应到最接近你上次确认时间的那一点上，这样就能**唯一确定**它在时间轴上的位置。

这就是我们为何要在计算出的绝对序号（absolute seqno）上下 **±1×232\pm 1 \times 2^{32}** 做比较，选择离 `checkpoint` 最近值的根本原因。



[CS/Q2：做CS的经典lab/CS144 Computer Network/Lab3 TCP Sender.md]
===========================================================

---
draw:
tags: []
title: Lab3 TCP Sender
date created: 2024-12-30
date modified: 2025-01-23
---

## 参考资料

https://doraemonzzz.com/2022/01/08/2022-1-8-CS144-Lab3/#  
Lab-3-the-TCP-sender

###


### 零窗口探测

这是一个特殊的"探测"机制！

在 TCP 中，这种情况被称为 Zero Window Probing（零窗口探测）。当出现以下情况时：

1. `_recx_windowsize == 0`：接收方说"我的接收窗口已经满了，不能再收了"
2. `remainWindowSize == 0`：计算出来的剩余窗口大小也是 0

这时候发送方会尝试发送 1 个字节的数据作为"探测"，为什么要这样做？

- 防止死锁：如果接收方的窗口变大了但通知包丢了，双方就会卡住
- 发送方："你窗口满了我就不发了"
- 接收方："我窗口已经有空间了，但你怎么不发数据？"

就像你去倒水：

1. 对方说："我的水桶满了！"
2. 你还是会试着滴一滴水
3. 这样如果对方水桶已经空了，就能告诉你："哦，现在可以倒水了"

这就是为什么即便接收窗口为 0，代码还是会设置 `remainWindowSize = 1`，给一个"试探"的机会。这是 TCP 协议中的一个重要机制，保证通信不会死锁。



[CS/Q2：做CS的经典lab/CS144 Computer Network/Lab4 TCP Connection.md]
===============================================================

---
draw:
tags: []
title: Lab4 TCP Connection
date created: 2024-12-30
date modified: 2025-01-21
---

## 参考



[CS/Q2：做CS的经典lab/CS144 Computer Network/Lab5 Network Interface.md]
==================================================================

---
draw:
tags: []
title: Lab5 Network Interface
date created: 2024-12-30
date modified: 2024-12-30
---



[CS/Q2：做CS的经典lab/CS144 Computer Network/Lab6 Router.md]
=======================================================

---
draw:
tags: []
title: Lab6 Router
date created: 2024-12-30
date modified: 2024-12-30
---



[CS/Q2：做CS的经典lab/CS144 Computer Network/Untitled.md]
====================================================

---
draw:
tags: []
title: Untitled
date created: 2024-12-31
date modified: 2024-12-31
---



[CS/Q2：做CS的经典lab/CS144 Computer Network/r n.md]
===============================================

---
draw:
tags: []
title: r n
date created: 2024-12-31
date modified: 2024-12-31
---

`\n` 和 `\r\n` 的区别主要与不同操作系统的换行符历史有关：

1. `\n`（Line Feed，LF）
   - Unix/Linux/Mac OS X 使用的换行符
   - 只移动到下一行

2. `\r`（Carriage Return，CR）
   - 早期 Mac OS（9之前）使用的换行符
   - 只将光标移动到行首

3. `\r\n`（CR+LF）
   - Windows 使用的换行符
   - HTTP 协议规范要求使用的换行符
   - 先回到行首，再移动到下一行

形象的理解：

```Java
假设有一个打字机：

Hello World
     ^      <- 光标在这里

\r 的效果：  回车，光标回到行首
Hello World
^          <- 光标回到行首

\n 的效果：  换行，光标下移一行
Hello World
     ^
     |      <- 光标直接下移

\r\n 的效果：先回车再换行
Hello World
^
|          <- 光标先回到行首再下移
```

在 HTTP 协议中：

```http
GET /path HTTP/1.1\r\n
Host: example.com\r\n
\r\n
```

必须使用 `\r\n` 是因为：

1. HTTP 协议最早制定时，为了兼容不同操作系统
2. 成为了协议的一部分，所有 HTTP 客户端和服务器都必须遵守
3. 如果只用 `\n`，某些服务器可能会拒绝请求

但在一般的编程中：

```cpp
// 在大多数现代系统中，这两种写法效果是一样的
cout << "Hello\n";
cout << "Hello\r\n";
```

除非是在处理特定协议（如 HTTP）或者跨平台文件操作，否则使用 `\n` 就足够了。



[CS/Q2：做CS的经典lab/CS144 Computer Network/数据流重组 Lab1 Stream Reassembler.md]
=========================================================================

---
draw:
tags: []
title: 数据流重组 Lab1 Stream Reassembler
date created: 2024-12-30
date modified: 2025-01-15
---

## 参考

https://doraemonzzz.com/2022/01/30/2022-1-30-CS144-Lab1%E7%BF%BB%E8%AF%91/

## 目标

TCP发送方将其字节流分为短段（每个子串不超过1,460字节），以便它们各自在一个数据报中。但网络可能会对这些数据报进行重新排序，或丢弃它们，或多次传送它们。接收方必须将这些片段重新组合成它们开始时的连续的字节流。

## 直观理解

### 故事背景：组装书页

想象有一家图书装订厂，需要把一堆**散页**的书页（很可能"乱序"地送到厂里）**组合**成一本**有序**的书。每一页都有自己的**页码**（类似我们的 `index`），我们最终要把它们**按页码顺序**排列好，然后**装订**成一本完整的书（类似输出 `ByteStream`）。

在这个故事中：

1. **散页** = 分段的字节数据（`push_substring` 传入的 `data`）。
2. **页码** = 数据对应的逻辑下标（参数 `index`）。
3. **装订好的书** = 最终输出流（`_output`）。
4. **装订厂的工作台** = `StreamReassembler` 的内部缓冲区（`buffer` + `bitmap`）。
5. **最大可工作台容量** = 代码里的 `capacity`。

---

### 缓冲区（buffer + bitmap）就是一本"占位"的大本子

- 你可以把 `StreamReassembler` 内部的 `buffer` 想象成一个**摆满空白插槽的大本子**：它有固定的"页数"（对应 `capacity`），这些页面现在都是空白（或填着 `'\0'`），等待我们把真正的书页插入进来。
- 为了标记哪些页面已经插好了"真正的内容"，哪些页面还是空的，`bitmap` 里存了**是否有效**的标识位（真 / 假）。当我们把一页（一个字节）插进去以后，就把对应位置的标识位设为 `true`；还没插入的地方则是 `false`。

---

### 收到新散页：如何"插入"到工作台上

#### 1. 确定散页对应的位置（index 与 offset）

- 当我们收到一个新的散页（字符串 `data`），还带着它在整本书中的起始页码 `index`，先要判断：
    - 如果 `index` 在我们当前需要的下标（`unass_base`）**之后**，说明这是**更后面的书页**，需要放到工作台上相应的"插槽"处（要计算和 `unass_base` 的偏移量 `offset`）。
    - 如果 `index` 比当前需要的下标要早，但它的末尾部分仍然延伸到我们需要的区域，那么就只取它还没被我们"组装走"的那部分插入工作台。

#### 2. 不能装就丢吗？——容量检查

- `capacity` 相当于工作台能放下的插槽总数，但是**并不是**我们实时可用的容量，还要考虑已经写到 `_output` 但仍旧占着"坑"的缓冲区。如果没有空间了，多出的那部分就等于"溢出"（实际代码里你可能会直接忽略或截断，不会放进来）。

#### 3. 真正插入

- 我们拿着散页 `data` 的内容，一字节一字节去对应到 `buffer[i + offset]` 中，同时把 `bitmap[i + offset]` 设为 `true` 表示这里已经有了有效数据。
- `unass_size` 就相当于我们"已经拿到但还没正式装订"的散页字节总数。

---

### check_contiguous()：能装订多少就装订多少

- 每当插入新的散页后，可能会触发"已经可以连续组合起来"的部分（就像书页1、2、3 已经都到位了，那就可以马上把它们**装订**进书中）。
    
- `check_contiguous()` 就是干这件事的：
    
    1. 它会从 `buffer` 的最前面（对应我们要找的 `unass_base` 下标起），**如果数据是连着的**（`bitmap.front()` 为 `true`），就把这一部分字节拿出来连成 `tmp`。
    2. 一直拿到遇到"空"的位置（`false`）或者到 `buffer` 尾部，就停止。
    3. 然后把这串连续有效的内容（`tmp`）一次性写入 `_output`（相当于把这几页纸装订到书里去了）。
    4. 同时，我们也要把 `unass_base` 和 `unass_size` 更新：因为这部分数据已经变成**正式装订**的页面了，不能再占着 `buffer` 里了。
- 在代码里，你能看到会 `pop_front()` 一些元素，并把 `unass_base` 往后挪、把 `unass_size` 减少，就是做了"已经装订好就剥离工作台"的工作。
    

---

### eof 与结束判断

- 当我们收到一个标识 `eof = true` 时，就说明这是我们期望的最后一批散页，但不代表我们**马上**结束。只有当**全部**需要的字节（`unass_size == 0`）都装订进书里了，才可以**正式收尾** (`_output.end_input()`)。

---

### 小结

1. **buffer + bitmap**：就像一个"临时存放散页"的工作台。
    - `buffer[i]` 存某字节。
    - `bitmap[i]` 标记那个字节是否有效。
2. **unass_base**：表示我已经装订好的最末端下标（下一页要找的书页页码）。
3. **push_substring(...)**：想办法把新到的散页放到工作台合适的位置，同时避免溢出。
4. **check_contiguous()**：检查是不是可以拿一些**连续的**书页，装订到书中 `_output`。
5. **unassembled_bytes()**：告诉你工作台上还有多少散页没被正式装订。
6. **eof**：当标记最后一批散页到了，但若工作台上还有欠缺的页面，就还不能结束。等到全部装订完才 end。

---

你可以把这段逻辑与书的装订过程联系起来，就会发现"收到乱序页 -> 暂存到工作台相应位置 -> 连续的部分一次装订 -> 直到最后"整个流程十分自然。这样，你对代码里 `bytestream`、`buffer`、`bitmap` 以及 `unass_base`、`check_contiguous()` 做的事情就有更直观的把握了。祝学习愉快！

在这个 StreamReassembler 中，buffer 和 bitmap 是用来处理乱序数据的两个关键数据结构：

1. buffer（缓冲区）

```cpp
std::deque<char> buffer; //!< The unassembled strings
```

- 存储还未能按序重组的数据内容
- 使用 deque 是因为需要在两端进行操作（前端取出数据，后端添加数据）
- 大小为 capacity，未使用的位置用 '\0' 填充

1. bitmap（位图）

```cpp
std::deque<bool> bitmap; //!< buffer bitmap
```

- 标记 buffer 中对应位置是否已经存储了有效数据
- true 表示该位置已有数据
- false 表示该位置尚未收到数据
- 与 buffer 一一对应

工作原理：

1. 当收到新数据片段时：

```cpp
buffer[i + offset] = data[i];  // 存储数据
bitmap[i + offset] = true;     // 标记该位置已有数据
```

1. 检查连续数据时（check_contiguous）：

```cpp
while (bitmap.front()) {  // 如果第一个位置有数据
    tmp += buffer.front();  // 取出数据
    buffer.pop_front();     // 移除已处理数据
    bitmap.pop_front();     // 移除对应标记
    // 在末尾补充空位
    buffer.push_back('\0');
    bitmap.push_back(false);
}
```

这样设计的好处：

1. 可以处理乱序到达的数据片段
2. 避免重复处理相同位置的数据
3. 能够快速判断哪些位置的数据已经到达
4. 方便检查是否有连续的数据可以输出

例如，假设收到以下片段：

```Java
index: 5, data: "def"
index: 0, data: "abc"
```

buffer和bitmap会帮助正确组装成"abcdef"。

## 新数据在 unass 点后

让我用 ASCII 图解释 `min(len, _capacity - _output.buffer_size() - offset)` 这个计算：

```Java
【整体视图】
capacity = 10 (总容量)


假设收到新数据："HELLO" (len=5)，从位置8开始 (offset=2)：
|<-------------- capacity=10 ------------>|
+---+---+---+---+---+---+---+---+---+---+
| A | B | C | D | E | F |   |   | H | E |  新数据位置示意
+---+---+---+---+---+---+---+---+---+---+
|<--- buffer_size=6 --->|
                        |offset|
                               |-可用空间-|


计算可用空间：
_capacity = 10            (总容量)
_output.buffer_size() = 6 (已使用空间)
offset = 2               (新数据的偏移量)
可用空间 = 10 - 6 - 2 = 2  (只能放入 "HE"，"LLO" 放不下)

real_len = min(len=5, 可用空间=2) = 2
```

再举个例子：

```Java
情况1：缓冲区较空
capacity = 10
+---+---+---+---+---+---+---+---+---+---+
| A | B |   |   |   |   |   |   |   |   |
+---+---+---+---+---+---+---+---+---+---+
|<->| buffer_size=2
     |->| offset=1
        |<---- 可用空间=7 ---->|

len = 5 ("HELLO")
real_len = min(5, 7) = 5  // 全部数据都能放入


情况2：缓冲区较满
capacity = 10
+---+---+---+---+---+---+---+---+---+---+
| A | B | C | D | E | F | G | H |   |   |
+---+---+---+---+---+---+---+---+---+---+
|<-------- buffer_size=8 -------->|
                                  |->| offset=1
                                      |->| 可用空间=1

len = 5 ("HELLO")
real_len = min(5, 1) = 1  // 只能放入一个字符
```

这个计算就像是在玩俄罗斯方块：

1. `capacity` 是游戏屏幕的总高度
2. `buffer_size` 是已经堆积的方块高度
3. `offset` 是新方块要放的位置到顶部的距离
4. `real_len` 是实际能放下的新方块数量

这样可以确保：

1. 不会超过总容量
2. 考虑了已使用空间
3. 考虑了新数据的位置
4. 取最小值保证安全

### 新数据与 unass 有部分重叠的情况

```Java
假设当前状态：
unass_base = 5 (从位置5开始未重组)
收到数据："WORLD" (len=5) 从位置3开始 (index=3)

【初始状态】
         index=3
            ↓
+---+---+---+---+---+---+---+---+---+---+
| A | B | C | W | O | R | L | D |   |   |  新数据
+---+---+---+---+---+---+---+---+---+---+
            |<----- data="WORLD" ----->|
                    ↑
               unass_base=5
        
offset = unass_base - index = 5 - 3 = 2
意味着要跳过 "WO"，只处理 "RLD"

【计算实际可用长度】
len = 5 (WORLD的长度)
len - offset = 3 (要处理的实际长度，即"RLD")
real_len = min(3, capacity - buffer_size)

【处理后】
+---+---+---+---+---+---+---+---+---+---+
| A | B | C | D | E | R | L | D |   |   |
+---+---+---+---+---+---+---+---+---+---+
buffer:   数据内容
bitmap:  
  1   1   1   1   1   1   1   1   0   0
                      ↑   ↑   ↑
                  新存储的数据
```

再举个更复杂的例子：

```Java
【情况1：部分重叠，部分超出容量】
capacity = 10
unass_base = 5
收到："HELLOWORLD" (index=3)

+---+---+---+---+---+---+---+---+---+---+
| A | B | C | H | E | L | L | O | W | O |  当前缓冲区
+---+---+---+---+---+---+---+---+---+---+
            |<------ 新数据 "HELLOWORLD" ------>|
                    ↑
                unass_base=5
                
offset = 5 - 3 = 2 (跳过"HE")
len - offset = 8 (要处理"LLOWORLD")
real_len = min(8, 剩余容量)


【情况2：有重叠且有已存在数据】
+---+---+---+---+---+---+---+---+---+---+
| A | B | C | D | E | x | x | O | W | O |  当前缓冲区
+---+---+---+---+---+---+---+---+---+---+
bitmap:   1   1   1   1   1   0   0   1   1   1
                        ↑   ↑   ↑
                        要填充的空位
```

代码逻辑解释：

```cpp
// 1. 计算偏移量
offset = unass_base - index;  // 跳过重叠部分

// 2. 计算实际可处理长度
real_len = min(len - offset, _capacity - _output.buffer_size());

// 3. 如果无法完全存储，取消 EOF 标记
if (real_len < len - offset) {
    _eof = false;
}

// 4. 存储数据，但跳过已有数据
for (size_t i = 0; i < real_len; i++) {
    if (bitmap[i]) continue;  // 跳过已有数据
    buffer[i] = data[i + offset];  // 存储新数据
    bitmap[i] = true;  // 标记为已使用
    unass_size++;  // 更新未重组数据大小
}
```

这就像拼拼图：

1. 找到新片段应该放的位置
2. 跳过已经有的部分
3. 把新的部分放进去
4. 标记哪些位置已经填好了



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Assignment 1 Exploring Word Vectors.md]
===========================================================================================

---
draw:
tags: []
title: Assignment 1 Exploring Word Vectors
date created: 2025-01-23
date modified: 2025-01-29
---



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Assignment 2 word2vec.md]
=============================================================================

---
draw:
title: Assignment 2 word2vec
date created: 2025-01-24
date modified: 2025-02-14
---
- [干了啥](#%E5%B9%B2%E4%BA%86%E5%95%A5)
- [naive-softmax](#naive-softmax)
	- [[softmax](softmax.md)](#%5Bsoftmax%5D(softmax.md))
	- [[[损失函数]]](#%5B%5B%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%5D%5D)
	- [将损失函数看作交叉熵](#%E5%B0%86%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9C%8B%E4%BD%9C%E4%BA%A4%E5%8F%89%E7%86%B5)
- [negative sampling](#negative%20sampling)
- [如果 K 等于 V-1 呢？](#%E5%A6%82%E6%9E%9C%20K%20%E7%AD%89%E4%BA%8E%20V-1%20%E5%91%A2%EF%BC%9F)
- [数学知识点](#%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E7%82%B9)

## 干了啥

Word2vec 是一个软件包实际上包含：**两个算法**：continuous bag-of-words（CBOW）和 skip-gram。

- CBOW 是根据中心词周围的上下文单词来预测该词的词向量。
- skip-gram 则相反，是根据中心词预测周围上下文的词的概率分布。

在实现 skip-gram 的 word2vec 时，我们需要为每一个"中心词 (center word)"及其上下文"外词 (outside word)"计算损失并做梯度更新。**有两种主要方法**可以定义和计算这个损失：

1. **naive-softmax**：对整个词汇表做 softmax，计算开销与"词汇量"线性相关。
2. **negative sampling**：只对一个正样本 + K 个负样本做 sigmoid，计算开销与"K+1"线性相关，远小于词汇量。

在此 Lab 中会先介绍 naive-softmax（直观但计算量大），然后介绍 negative sampling（更高效），最后代码通常会使用 negative sampling，因为它在大词表时极大地减少了计算量。

## naive-softmax

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2025%2F01%2F24%2F02-41-00-1081680bceefbfc461cd91f06f5bf6a3-202501240241797-345bdb.png)

$$ P(O = o | C = c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in \text{Vocab}} \exp(u_w^T v_c)} $$

在 Word2Vec 模型（特别是 Skip-gram 和 CBOW 模型）中用于计算 **给定中心词 (Context word) `c` 的情况下，生成目标词 (Output word) `o` 的概率** 的 Softmax 函数。

- **P(O = o | C = c)**: 在给定中心词 `c` 的条件下，目标词为 `o` 的概率。
- **o**: 目标词 (output word)。
- **c**: 中心词，也称为上下文词 (context word)。
- **Vocab**: 词汇表，即所有词的集合。
- **v\_c**: 中心词 `c` 的 **上下文词向量 (context word vector)**。在 Word2Vec 模型中，每个词都有两个向量表示：作为中心词的向量和作为目标词的向量。`v_c` 是当词 `c` 作为中心词时使用的向量。
- **u\_o**: 目标词 `o` 的 **目标词向量 (output word vector)**。`u_o` 是当词 `o` 作为目标词时使用的向量。
- **u\_o<sup>T</sup> v\_c**: 向量 `u_o` 和 `v_c` 的 **点积 (dot product)**。点积衡量了两个向量的相似度。点积越大，向量越相似。
- **exp(x)**: 指数函数，`e` 的 `x` 次方。使用指数函数确保概率值始终为正数。
- **∑<sub>w ∈ Vocab</sub> exp(u\_w<sup>T</sup> v\_c)$**: **归一化项 (normalization term)**。分母是对所有词汇表 `Vocab` 中的词 `w`，计算 `exp(u_w^T v_c)` 并求和。这个求和过程确保了公式计算出的所有可能的输出词的概率总和为 1，符合概率的定义。

**实际例子**

为了简化，我们使用一个非常小的词汇表：

**词汇表 Vocab = { "king", "queen", "man", "woman", "royal" }**

假设我们已经通过某种方式（比如训练 Word2Vec 模型）得到了每个词的向量表示。为了方便演示，我们假设这些向量是二维的，并且我们随意设定一些数值 (实际的词向量维度会更高，例如 100, 200, 300 维)：

| 词语    | 目标词向量 u (作为输出词) | 上下文词向量 v (作为中心词) |
|:------ |:----------------------- |:----------------------- |
| king    | `u_king` =[2, 1]| `v_king` =[2, 1]|
| queen   | `u_queen` =[2, 2]| `v_queen` =[2, 2]|
| man     | `u_man` =[1, 1]| `v_man` =[1, 1]|
| woman   | `u_woman` =[1, 2]| `v_woman` =[1, 2]|
| royal   | `u_royal` =[3, 3]| `v_royal` =[3, 3]|

**例子计算：给定中心词 "king"，计算目标词为 "queen" 的概率 $P(O = "queen" | C = "king")$**

1. **计算点积 $u\_queen<sup>T</sup> v\_king$**:
    - `u_queen` =[2, 2]
    - `v_king` =[2, 1]
    - `u_queen<sup>T</sup> v_king` = (2 \* 2) + (2 \* 1) = 4 + 2 = 6

2. **计算分子 exp(u\_queen<sup>T</sup> v\_king)**:

    - `exp(6)` ≈ 403.43

3. **计算分母 ∑<sub>w ∈ Vocab</sub> exp(u\_w<sup>T</sup> v\_king)**: 我们需要计算词汇表中每个词 `w` 与中心词 "king" 的点积，并取指数，然后求和。

    - `u_king<sup>T</sup> v_king` = (2 \* 2) + (1 \* 1) = 5 => `exp(5)` ≈ 148.41
    - `u_queen<sup>T</sup> v_king` = 6 => `exp(6)` ≈ 403.43 (已计算过)
    - `u_man<sup>T</sup> v_king` = (1 \* 2) + (1 \* 1) = 3 => `exp(3)` ≈ 20.09
    - `u_woman<sup>T</sup> v_king` = (1 \* 2) + (2 \* 1) = 4 => `exp(4)` ≈ 54.60
    - `u_royal<sup>T</sup> v_king` = (3 \* 2) + (3 \* 1) = 9 => `exp(9)` ≈ 8103.08

    分母 ∑<sub>w ∈ Vocab</sub> exp(u\_w<sup>T</sup> v\_king) ≈ 148.41 + 403.43 + 20.09 + 54.60 + 8103.08 ≈ 8729.61

4. **计算概率 P(O = "queen" | C = "king")**:

    - `P(O = "queen" | C = "king")` = `exp(u_queen<sup>T</sup> v_king)` / ∑<sub>w ∈ Vocab</sub> exp(u\_w<sup>T</sup> v\_king)
    - `P(O = "queen" | C = "king")` ≈ 403.43 / 8729.61 ≈ 0.046

**矩阵表示**

为了更好地理解公式，我们可以用矩阵来表示。

假设我们的词汇表大小为 V (这里 V=5)，词向量维度为 d (这里 d=2)。

1. **输出词向量矩阵 U (Output Word Vectors Matrix)**:

    这是一个 V x d 的矩阵，每一行代表一个词作为 **目标词** 的向量表示。

    ```Java
    U = | u_king^T   |   = | 2  1 |
        | u_queen^T  |     | 2  2 |
        | u_man^T    |     | 1  1 |
        | u_woman^T  |     | 1  2 |
        | u_royal^T  |     | 3  3 |
    ```

2. **上下文词向量矩阵 V (Context Word Vectors Matrix)**:

    这是一个 V x d 的矩阵，每一列代表一个词作为 **中心词** 的向量表示。为了方便矩阵乘法，我们通常使用 V<sup>T</sup>，即转置后的矩阵，这样每一行代表一个词作为中心词的向量。

    ```Java
    V^T = | v_king   v_queen   v_man   v_woman   v_royal |
          |-------------------------------------------------|
          |  2       2        1       1        3        |
          |  1       2        1       2        3        |
    ```

    (注意，这里为了视觉上和U矩阵对应，我把 V<sup>T</sup> 画成了每一行代表一个词，实际上在计算中，更常见的是 V 的每一列代表一个词，然后计算 U * V<sup>T</sup>)

3. **计算所有可能的点积 (Conceptual Matrix Multiplication):**

    虽然公式中我们是针对单个中心词 `c` 进行计算，但如果我们想一次性计算 **所有中心词作为输入，所有词作为输出** 的概率（概念上，实际训练中通常是 batch 计算），我们可以想象一个矩阵乘法的过程。

    如果我们计算矩阵乘积 `U * V^T`，我们会得到一个 V x V 的矩阵，其中 **第 i 行第 j 列的元素 (i, j)** 就代表了 **u<sub>word\_i</sub><sup>T</sup> v<sub>word\_j</sub>** 的点积 (未取指数)。

    ```Java
    U * V^T =  | u_king^T   |   *  | v_king   v_queen   v_man   v_woman   v_royal |
               | u_queen^T  |      |-------------------------------------------------|
               | u_man^T    |      |  2       2        1       1        3        |
               | u_woman^T  |      |  1       2        1       2        3        |
               | u_royal^T  |

          =  | u_king^T * v_king    u_king^T * v_queen   ...  u_king^T * v_royal  |
             | u_queen^T * v_king   u_queen^T * v_queen  ...  u_queen^T * v_royal |
             | ...                                       ...                         |
             | u_royal^T * v_king   u_royal^T * v_queen  ...  u_royal^T * v_royal |

          =  |  5   6   3   4   9 |   (第一行，中心词 "king" 与所有词的点积)
             |  6   8   4   6  12 |   (第二行，中心词 "queen" 与所有词的点积)
             |  3   4   2   3   6 |
             |  4   6   3   5   9 |
             |  9  12   6   9  18 |
    ```

### [softmax](softmax.md)

1. **Softmax 应用 (按行进行 Softmax):**

    为了得到概率 `P(O = o | C = c)`，我们需要对点积矩阵的 **每一行** 应用 Softmax 函数。例如，对于中心词 "king" (对应矩阵的第一行):

    - 我们对第一行的每个元素取指数：`[exp(5), exp(6), exp(3), exp(4), exp(9)]` ≈ `[148.41, 403.43, 20.09, 54.60, 8103.08]`
    - 然后，将每个指数值除以 **该行所有指数值的总和** (归一化)：分母 ≈ 8729.61 (我们在上面已经计算过)。

    这样，矩阵的每一行都将转换为概率分布，**行索引代表中心词，列索引代表目标词，矩阵中的值代表给定中心词，目标词出现的概率**。

**总结**

- 公式 `P(O = o | C = c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in \text{Vocab}} \exp(u_w^T v_c)}` 使用 Softmax 函数将词向量的点积转换为概率。
- 矩阵 U 代表所有词作为目标词的向量，矩阵 V 代表所有词作为中心词的向量。
- 概念上，矩阵乘法 `U * V^T` 可以帮助我们理解如何一次性计算所有可能的词对之间的点积。
- Softmax 函数按行应用于点积矩阵，得到最终的概率矩阵。

### [[损失函数]]

在 Word2Vec 中，我们的目标是最大化在给定中心词的情况下，观察到实际上下文词的概率。换句话说，我们希望模型预测的概率分布尽可能接近真实的概率分布。为了达到这个目标，我们需要定义一个损失函数来衡量模型预测与真实情况之间的差距。

对于单个词对 (c, o)，损失函数通常使用 **负对数似然 (Negative Log-Likelihood)**：

$$ J_{naive-softmax}(v_c, o, U) = - \log P(O = o | C = c) $$

这个公式的含义是：

- **J<sub>naive-softmax</sub>(v<sub>c</sub>, o, U)**：损失函数，它依赖于中心词向量 v<sub>c</sub>、目标词 o 和所有输出词向量的矩阵 U。
- **-log**：[[负对数]]。使用负对数的原因是：
    - 最大化概率 P(O = o | C = c) 等价于最小化其负对数 -log P(O = o | C = c)。
    - 对数函数可以将连乘转换为连加，方便计算和优化。
    - 对数函数在概率接近 0 时梯度变化较大，有助于模型更好地学习。
- **P(O = o | C = c)**：给定中心词 c，目标词为 o 的概率，就是你提供的 softmax 公式。

**结合你的例子解释损失函数**

我们继续使用你提供的例子：

- **词汇表 Vocab = { "king", "queen", "man", "woman", "royal" }**
- 以及你设定的词向量和计算出的概率：`P(O = "queen" | C = "king")` ≈ 0.046

现在，假设在我们的训练数据中，当中心词是 "king" 时，实际的上下文词（也就是我们希望模型预测的词）确实是 "queen"。这意味着对于这个特定的词对 ("king", "queen")，我们希望 `P(O = "queen" | C = "king")` 尽可能接近 1。

使用损失函数计算：

$$ J_{naive-softmax}(v_{king}, "queen", U) = - \log P(O = "queen" | C = "king") $$

$$ J_{naive-softmax}(v_{king}, "queen", U) = - \log (0.046) $$

$$ J_{naive-softmax}(v_{king}, "queen", U) ≈ 3.08 $$

这个 3.08 就是这个词对 ("king", "queen") 的损失值。

**损失值的意义**  
![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2025%2F01%2F24%2F02-49-01-64570d34a45b5335478bd695dac62849-202501240249762-278b41.png)

- **损失值越大，表示模型预测得越差。** 例如，如果模型预测 `P(O = "queen" | C = "king")` 非常小（比如 0.001），那么损失值就会非常大（-log(0.001) ≈ 6.91）。
- **损失值越小，表示模型预测得越好。** 如果模型预测 `P(O = "queen" | C = "king")` 非常接近 1（比如 0.9），那么损失值就会非常小（-log(0.9) ≈ 0.11）。

**训练过程**

在 Word2Vec 的训练过程中，我们会遍历大量的词对 (c, o)，并计算每个词对的损失值。然后，使用优化算法（例如梯度下降）来调整词向量 (u<sub>w</sub> 和 v<sub>w</sub>)，使得所有词对的平均损失值尽可能小。通过不断地调整词向量，模型最终能够学习到词语之间的语义关系。

**总结**

损失函数 `-log P(O = o | C = c)` 的作用是衡量模型预测的概率与真实概率之间的差距。通过最小化损失函数，我们可以训练 Word2Vec 模型，使其能够更好地预测给定中心词的上下文词。你提供的例子很好地说明了如何使用 softmax 公式计算概率，而损失函数则是在此基础上衡量模型预测的质量。

这段话解释了 Word2Vec 中使用的损失函数（通常是负对数似然）可以被视为真实分布和预测分布之间的交叉熵。下面我来详细解释这段话的含义，并结合前面的例子进行说明。

### 将损失函数看作交叉熵

这段话的核心思想是将 Word2Vec 的损失函数与交叉熵联系起来。交叉熵是信息论中一个重要的概念，用于衡量两个概率分布之间的差异。

**关键概念**

1. **真实分布 y：** 在 Word2Vec 中，真实分布 y 是一个 one-hot 向量。这意味着：
    
    - 对于给定的中心词 c，只有一个词是真正的上下文词 o。
    - 在 y 向量中，对应于真实上下文词 o 的位置上的值为 1，其他位置上的值都为 0。
    

    例如，如果中心词是 "king"，真实的上下文词是 "queen"，而词汇表是 { "king", "queen", "man", "woman", "royal" }，那么 y 向量就是[0, 1, 0, 0, 0]。

    
2. **预测分布 ŷ：** 预测分布 ŷ 是模型根据中心词 c 预测出的上下文词的概率分布。
    
    - ŷ 向量中的每个元素表示模型预测某个词作为上下文词的概率。
    - ŷ 向量是通过 softmax 函数计算得到的，即 P(O | C = c)。
    

    例如，根据前面的例子，我们计算出 `P(O = "queen" | C = "king")` ≈ 0.046。假设模型预测出的完整概率分布是 ŷ =[0.1, 0.046, 0.2, 0.3, 0.354]，分别对应 "king", "queen", "man", "woman", "royal" 的概率。

    
3. **交叉熵：** 交叉熵用于衡量两个概率分布之间的差异。对于离散概率分布，交叉熵的计算公式如下：
    

    H(y, ŷ) = - Σ<sub>i</sub> y<sub>i</sub> log(ŷ<sub>i</sub>)

    

    其中，y<sub>i</sub> 和 ŷ<sub>i</sub> 分别表示在第 i 个事件上的真实概率和预测概率。

    

**将交叉熵应用于 Word2Vec**

在 Word2Vec 中，我们将交叉熵应用于真实分布 y 和预测分布 ŷ。由于 y 是一个 one-hot 向量，所以交叉熵的计算可以简化为：

H(y, ŷ) = - log(ŷ<sub>o</sub>)

其中，ŷ<sub>o</sub> 是预测分布 ŷ 中对应于真实上下文词 o 的概率值。

这正好就是我们之前提到的负对数似然损失函数！

**例子说明**

继续使用之前的例子：

- 中心词 c = "king"
- 真实上下文词 o = "queen"
- 真实分布 y =[0, 1, 0, 0, 0]
- 预测分布 ŷ =[0.1, 0.046, 0.2, 0.3, 0.354]

计算交叉熵：

H(y, ŷ) = - (0 * log(0.1) + 1 * log(0.046) + 0 * log(0.2) + 0 * log(0.3) + 0 * log(0.354))

H(y, ŷ) = - log(0.046) ≈ 3.08

这与我们之前计算的负对数似然损失值完全一致。

**这段话的意义**

这段话说明了 Word2Vec 的损失函数可以从信息论的角度进行解释，即通过最小化预测分布和真实分布之间的交叉熵来训练模型。这种解释更加深刻地揭示了 Word2Vec 的工作原理：模型的目标是学习一个概率分布，使其尽可能接近真实的上下文分布。

**总结**

这段话将 Word2Vec 的损失函数与交叉熵联系起来，提供了一个更广阔的视角来理解 Word2Vec 的训练过程。通过最小化交叉熵，模型能够学习到词语之间的语义关系，并能够根据中心词有效地预测上下文词。希望以上解释能够帮助你理解这段话的含义。

## negative sampling

**回顾之前的例子**

- **词汇表 Vocab = { "king", "queen", "man", "woman", "royal" }**
- 我们计算了 `P(O = "queen" | C = "king")` ≈ 0.046，使用了 naive softmax。
- 我们也计算了使用负对数似然的损失函数：`J<sub>naive-softmax</sub>(v<sub>king</sub>, "queen", U)` ≈ 3.08

**Negative Sampling 的核心思想**

Negative sampling 的核心思想是：与其计算整个词汇表的 softmax（计算量巨大），不如只关注 **一个正样本** 和 **几个负样本**。

- **正样本 (Positive Sample):** 实际出现在中心词上下文中的词。在我们的例子中，("king", "queen") 是一个正样本。
- **负样本 (Negative Sample):** 没有出现在中心词上下文中的词。我们需要随机抽取一些词作为负样本。

**Negative Sampling 的步骤**

1. **选择正样本：** 假设我们的训练数据中有一个词对 ("king", "queen")。
    
2. **选择负样本：** 我们需要从词汇表中随机选择 K 个词作为负样本。K 是一个超参数，通常取 5-20。为了简化，我们假设 K=2，并且我们随机选择了 "man" 和 "royal" 作为负样本。
    
3. **构建数据集：** 我们现在有了一个新的数据集，包含一个正样本和两个负样本：

```Java

| 中心词 (c) | 目标词 (o) | 标签 (Label) |
|:--------- |:--------- |:----------- |
| "king"    | "queen"   | 1          |
| "king"    | "man"     | 0          |
| "king"    | "royal"   | 0          |
	
	标签 1 表示正样本，标签 0 表示负样本。
```

1. **使用 Sigmoid 函数：** 对于每个样本，我们使用 sigmoid 函数来预测目标词是否是中心词的上下文：

$$ P(O = o | C = c) = \sigma(u_o^T v_c) = \frac{1}{1 + \exp(-u_o^T v_c)} $$

其中 σ 是 sigmoid 函数。

1. **定义损失函数：** 对于 negative sampling，损失函数定义为：正样本的损失+负样本的损失  

$$ J_{neg-sample}(v_c, o, U) = -\log(\sigma(u_o^T v_c)) - \sum_{k=1}^K \log(\sigma(-u_k^T v_c)) $$

σ（小写希腊字母 sigma）通常用来表示 sigmoid 函数。
对于正样本，我们希望模型预测其为正样本的概率尽可能高。而要使 $\sigma(u_o^T v_c)$ 接近 1，我们需要使 ​$u_o^T v_c$ 尽可能大（[Sigmoid函数](Sigmoid函数.md)图像）

所以：

- 第一项是正样本的损失。我们希望 σ(uoT​vc​) 尽可能接近 1（此时负指数为 0）。    
- 第二项是 K 个负样本的损失。我们希望$\log(\sigma(-u_k^T v_c))$尽可能接近 0，也就是 $\sigma(-u_k^T v_c)$ 尽可能接近 1，因为 $σ(−x)=1−σ(x)$，所以也就是 $\sigma(u_o^T v_c)$ 尽可能接近 0。

**例子计算**

我们来计算上面例子中三个样本的损失：

1. **正样本 ("king", "queen", 1):**
- `u_queen^T v_king` = 6 (之前计算过)     - `σ(6)` ≈ 0.9975     - −log(σ(6)) ≈ -log(0.9975) ≈ 0.0025
1. **负样本 ("king", "man", 0):**
- `u_man^T v_king` = 3 (之前计算过)     - `σ(-3)` ≈ 0.0474     - −log(σ(−3)) ≈ -log(0.0474) ≈ 3.047
1. **负样本 ("king", "royal", 0):**
- `u_royal^T v_king` = 9 (之前计算过)     - `σ(-9)` ≈ 0.000123     - −log(σ(−9)) ≈ -log(0.000123) ≈ 8.91

**总损失**

对于这个中心词 "king"，总的 negative sampling 损失是这三个样本损失的总和：

总损失 ≈ 0.0025 + 3.047 + 8.91 ≈ 11.96

**与 Naive Softmax 的比较**

- **计算量：** Naive softmax 需要计算整个词汇表的 softmax，计算量与词汇表大小 V 成正比。而 negative sampling 只需计算 K+1 个 sigmoid 函数，计算量与 K 成正比，通常 K 远小于 V，因此计算效率大大提高。
- **损失函数：** Naive softmax 使用负对数似然和 softmax 函数，而 negative sampling 使用 sigmoid 函数和略有不同的损失函数。

**总结**

Negative sampling 通过只关注少量正负样本，大大降低了 Word2Vec 的计算复杂度，使其能够在大规模语料库上进行训练。它使用 sigmoid 函数将问题转化为二分类问题，并使用相应的损失函数进行优化。希望这个结合你之前例子的讲解能够帮助你更好地理解 negative sampling。

## 如果 K 等于 V-1 呢？

你提出了一个很好的问题：如果负采样中的 K（负样本的数量）等于 V-1（词汇表大小减 1），会发生什么？在这种情况下，负采样的计算量还会比朴素 softmax 小吗？

**分析**

- **朴素 softmax：** 计算复杂度是 O(V)，其中 V 是词汇表的大小。这意味着计算量与词汇表大小成正比。
- **负采样：** 计算复杂度是 O(K+1)，其中 K 是负样本的数量。在这种情况下，K = V-1，所以计算复杂度变成了 O(V)。

因此，当 K = V-1 时，负采样的计算复杂度与朴素 softmax *相同*，都是 O(V)。

**为什么通常 K 远小于 V？**

负采样的主要目的是为了解决朴素 softmax 在大规模词汇表上的计算瓶颈。在实际应用中，词汇表通常非常大（例如，几万到几十万甚至更大），而负样本的数量 K 通常设置得比较小（例如，5 到 20）。

- **计算效率：** 通过使 K 远小于 V，负采样的计算量可以显著降低，从而提高训练速度。
- **经验结果：** 经验表明，使用相对较小的 K 值也能取得良好的训练效果。

**K = V-1 的情况实际上退化成了什么？**

当 K = V-1 时，我们实际上是在对除了正样本之外的所有词都进行了负采样。这意味着对于每个中心词，我们都在考虑它与词汇表中所有其他词的关系。这在某种程度上类似于朴素 softmax 的思想，即考虑所有可能的输出词。

然而，即使在这种情况下，负采样仍然与朴素 softmax 有一些关键区别：

1. **损失函数不同：** 负采样使用 sigmoid 函数进行二分类，而朴素 softmax 使用 softmax 函数进行多分类。这意味着它们的优化目标和梯度计算方式是不同的。
2. **计算方式不同：** 即使计算量相同，负采样在实现上也是分别计算每个正负样本的损失，然后求和。而朴素 softmax 需要一次性计算整个词汇表的概率分布。这在内存使用和并行计算方面可能存在差异。

**总结**

当 K = V-1 时，负采样的计算复杂度与朴素 softmax 相同，都是 O(V)。但负采样的主要优势在于当 K 远小于 V 时，可以显著降低计算量，提高训练效率。在实际应用中，通常会选择较小的 K 值，以达到效率和效果的平衡。即使 K=V-1，负采样在损失函数和计算方式上仍然与朴素 softmax 有区别。

因此，虽然理论上 K 可以等于 V-1，但在实践中，这样做既没有必要，也失去了负采样方法的核心优势。通常情况下，选择远小于 V 的 K 值是更明智的选择。

## 数学知识点

1. 概率论相关：
- 条件概率 (Conditional Probability)
- 概率分布 (Probability Distribution)
- 经验分布 (Empirical Distribution)
- [[最大似然估计]]

1. 线性代数相关：
- 向量 (Vectors)
- 矩阵 (Matrices)
- 点积/内积 (Dot Product)
- 转置 (Transpose，出现在 u^T v 形式)

1. 微积分相关：
- [[偏导数]] (Partial Derivatives)
- [[梯度]] (Gradients)
- [Sigmoid函数](Sigmoid函数.md)及其导数
- 链式法则 (Chain Rule，在求导过程中隐含使用)

1. 其他数学概念：
- [[softmax]]
- [[交叉熵]] (Cross Entropy)
- 对数函数 (Logarithm，出现在损失函数中)
- one-hot 向量
- 指数函数 (Exponential Function，在 softmax 和 sigmoid 中)

1. 优化相关：
- 损失函数 (Loss Function)
- 负采样 (Negative Sampling)
- 最小化问题 (Minimization)

## 代码

**2. (a) (12 分) word2vec.py - 词向量模型组件实现**

此部分的目标是在 word2vec.py 文件中实现 word2vec 模型的核心组成部分，包括 sigmoid 函数、基于朴素 Softmax 的损失函数和梯度计算、基于负采样的损失函数和梯度计算，以及 Skip-gram 模型。

- **sigmoid(x) 函数**
    - **功能:** 实现 Sigmoid 函数，其数学表达式为 σ(x) = 1 / (1 + e^(-x))。该函数将任意实数输入映射到 (0, 1) 区间，常用于将线性输出转换为概率值。
    - **实现要求:** 编写 Python 函数 sigmoid(x)，接受一个数值或 NumPy 数组作为输入 x，并返回应用 Sigmoid 函数后的结果。需使用 NumPy 库以保证效率。
        
- **naiveSoftmaxLossAndGradient(centerWordVec, outsideWordIdx, outsideVectors, dataset) 函数**
    - **功能:** 实现基于朴素 Softmax 的损失函数及其梯度计算。该函数用于计算给定中心词向量 centerWordVec 和目标外部词索引 outsideWordIdx 时的 Softmax 损失，并计算损失函数关于中心词向量和所有外部词向量的梯度。
    - **原理:** 朴素 Softmax 损失旨在最大化目标外部词在给定中心词条件下的条件概率 P(O=o|C=c)。损失函数通常定义为负对数似然，即 J_naive-softmax(vc, o, U) = -log P(O=o|C=c)。梯度计算需要根据链式法则，分别计算损失函数对中心词向量 vc 和外部词向量矩阵 U 的偏导数。
    - **计算复杂度:** 朴素 Softmax 的计算复杂度与词汇表大小成正比，因此对于大型词汇表，计算成本较高。
    - **实现要求:** 编写 Python 函数 naiveSoftmaxLossAndGradient，输入包括中心词向量、外部词索引、外部词向量矩阵和数据集对象。函数应返回损失值、中心词向量的梯度 gradCenterVec 以及外部词向量矩阵的梯度 gradOutsideVecs。需使用 NumPy 库进行高效的矩阵和向量运算，并参考书面部分的公式进行梯度推导和实现。
        
- **negSamplingLossAndGradient(centerWordVec, outsideWordIdx, outsideVectors, dataset, k=10) 函数**
    - **功能:** 实现基于负采样的损失函数及其梯度计算。负采样是一种近似 Softmax 的方法，旨在提高训练效率。该函数计算给定中心词向量 centerWordVec 和目标外部词索引 outsideWordIdx，以及 k 个负样本时的负采样损失，并计算损失函数关于中心词向量、目标外部词向量和负样本词向量的梯度。
    - **原理:** 负采样损失函数旨在区分真实的正样本（中心词-外部词对）和随机抽取的负样本。损失函数由两部分组成：一部分最大化正样本的 Sigmoid 输出，另一部分最小化负样本的 Sigmoid 输出。损失函数形式为 J_neg-sample(vc, o, U) = -log(σ(uo^T vc)) - Σ_{k=1}^K log(σ(-uk^T vc))。梯度计算需分别针对中心词向量、目标外部词向量和每个负样本词向量进行。
    - **计算效率:** 负采样显著降低了计算复杂度，使其适用于大型词汇表的训练。
    - **实现要求:** 编写 Python 函数 negSamplingLossAndGradient，输入包括中心词向量、外部词索引、外部词向量矩阵、数据集对象和负样本数量 k。函数应返回损失值、中心词向量的梯度 gradCenterVec、目标外部词向量的梯度 gradOutsideVecs 以及负样本词向量的梯度 (可以累加到 gradOutsideVecs 中)。需使用 NumPy 和 Sigmoid 函数的导数进行计算，并参考书面部分的公式。数据集对象可能提供负样本抽样的方法。
        
- **skipgram(centerWordIdx, outsideWordIdxs, centerWordVectors, outsideVectors, dataset, lossType='naive') 函数**
    - **功能:** 实现 Skip-gram 模型的前向传播和梯度计算。该函数针对给定的中心词索引 centerWordIdx 和一组外部词索引 outsideWordIdxs，使用指定的损失函数类型 (lossType) 计算总损失和梯度。
    - **原理:** Skip-gram 模型的目标是根据中心词预测其上下文词。对于一个中心词及其上下文窗口，模型需要遍历窗口内的每个外部词，计算损失并累积梯度。损失函数类型可以是 'naive' (朴素 Softmax) 或 'negSample' (负采样)。总损失是窗口内所有中心词-外部词对损失之和。梯度也需要对窗口内所有词对的梯度进行累加。
    - **实现要求:** 编写 Python 函数 skipgram，输入包括中心词索引、外部词索引列表、中心词向量矩阵、外部词向量矩阵、数据集对象和损失函数类型。函数应根据 lossType 调用相应的损失函数和梯度计算函数 (naiveSoftmaxLossAndGradient 或 negSamplingLossAndGradient)，遍历 outsideWordIdxs 中的每个外部词索引，累加损失和梯度。最终返回总损失 totalLoss，中心词向量矩阵的梯度 gradCenterVecs 和外部词向量矩阵的度 gradOutsideVecs。
**测试 word2vec.py:** 完成上述函数实现后，运行 python word2vec.py 脚本进行单元测试，验证各函数的实现是否符合预期。

**2. (b) (4 分) sgd.py - 随机梯度下降优化器实现**

- **sgd(initialParameters, gradientFunction, step, batchSize, alpha=0.01) 函数**
    - **功能:** 实现随机梯度下降 (SGD) 优化算法。该函数接收初始模型参数 initialParameters、梯度计算函数 gradientFunction、当前迭代步数 step、批量大小 batchSize 和学习率 alpha。函数执行一次 SGD 更新，根据梯度更新模型参数，并返回更新后的参数和损失值。
    - **原理:** SGD 是一种迭代优化算法，用于最小化损失函数。每次迭代，SGD 使用梯度计算函数 gradientFunction 计算当前参数下的损失和梯度，然后根据学习率 alpha 和梯度方向更新参数：parameters = parameters - alpha * gradients。
    - **实现要求:** 编写 Python 函数 sgd，输入包括初始参数、梯度函数、迭代步数、批量大小和学习率。函数应调用 gradientFunction 获取损失和梯度，并根据 SGD 更新规则更新 initialParameters。返回更新后的参数 updatedParameters 和损失值 loss。
**测试 sgd.py:** 运行 python sgd.py 脚本进行单元测试，验证 SGD 优化器的实现是否正确。

**2. (c) (4 分) run.py - 模型训练与可视化**

- **get_datasets.sh 脚本:** 运行 sh get_datasets.sh 下载 Stanford Sentiment Treebank (SST) 数据集，该数据集用于训练词向量。
- **run.py 脚本:** 运行 python run.py 执行以下步骤：
    1. **数据加载:** 加载 SST 数据集。
    2. **参数初始化:** 随机初始化中心词向量矩阵和外部词向量矩阵。
    3. **训练循环:** 进行指定迭代次数 (例如 40,000 次) 的 SGD 训练。在每次迭代中：
        - 从数据集中随机选择一个语境窗口。
        - 调用 skipgram 函数计算损失和梯度。
        - 调用 sgd 函数更新词向量参数。
    4. **可视化:** 使用 t-SNE 降维算法将高维词向量降至二维，并绘制散点图，保存为 word_vectors.png 文件。可视化目的是为了直观地观察学习到的词向量空间，例如语义相似的词是否在图中距离较近。
        
- **分析 word_vectors.png:** 训练完成后，分析生成的 word_vectors.png 图像。观察图中词向量的分布，例如是否存在语义相关的词簇，并对观察结果进行简要描述。
    

此部分无需编写代码，只需运行脚本并分析结果。重点在于理解训练流程和分析可视化结果。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Assignment 3 Dependency Parsing.md]
=======================================================================================

---
draw:
title: Assignment 3 Dependency Parsing
date created: 2025-01-24
date modified: 2025-02-06
---
- [理论](#%E7%90%86%E8%AE%BA)
	- [Part 1: Machine Learning & Neural Networks（8 分）](#Part%201:%20Machine%20Learning%20&%20Neural%20Networks%EF%BC%888%20%E5%88%86%EF%BC%89)
	- [Part 2: Neural Transition-Based Dependency Parsing（44 分）](#Part%202:%20Neural%20Transition-Based%20Dependency%20Parsing%EF%BC%8844%20%E5%88%86%EF%BC%89)
		- [训练数据](#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE)
		- [2(a) 理解并列出完整的转移序列 (4 分)](#2(a)%20%E7%90%86%E8%A7%A3%E5%B9%B6%E5%88%97%E5%87%BA%E5%AE%8C%E6%95%B4%E7%9A%84%E8%BD%AC%E7%A7%BB%E5%BA%8F%E5%88%97%20(4%20%E5%88%86))
		- [2(b) 解析一个 n 词句子，需要多少步？(2 分)](#2(b)%20%E8%A7%A3%E6%9E%90%E4%B8%80%E4%B8%AA%20n%20%E8%AF%8D%E5%8F%A5%E5%AD%90%EF%BC%8C%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%AD%A5%EF%BC%9F(2%20%E5%88%86))
		- [2(c) 实现 `PartialParse` 的两个函数 (6 分)](#2(c)%20%E5%AE%9E%E7%8E%B0%20%60PartialParse%60%20%E7%9A%84%E4%B8%A4%E4%B8%AA%E5%87%BD%E6%95%B0%20(6%20%E5%88%86))
		- [2(d) 实现 `minibatch_parse` 函数 (8 分)](#2(d)%20%E5%AE%9E%E7%8E%B0%20%60minibatch_parse%60%20%E5%87%BD%E6%95%B0%20(8%20%E5%88%86))
		- [2(e) 训练并实现神经网络依存解析器 (12 分)](#2(e)%20%E8%AE%AD%E7%BB%83%E5%B9%B6%E5%AE%9E%E7%8E%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BE%9D%E5%AD%98%E8%A7%A3%E6%9E%90%E5%99%A8%20(12%20%E5%88%86))
		- [2(f) 错误解析分析 (12 分)](#2(f)%20%E9%94%99%E8%AF%AF%E8%A7%A3%E6%9E%90%E5%88%86%E6%9E%90%20(12%20%E5%88%86))
- [实操](#%E5%AE%9E%E6%93%8D)
	- [1. 准备代码目录](#1.%20%E5%87%86%E5%A4%87%E4%BB%A3%E7%A0%81%E7%9B%AE%E5%BD%95)
	- [2. 创建并激活 Python 环境](#2.%20%E5%88%9B%E5%BB%BA%E5%B9%B6%E6%BF%80%E6%B4%BB%20Python%20%E7%8E%AF%E5%A2%83)
		- [2.1 安装 Miniconda/Anaconda](#2.1%20%E5%AE%89%E8%A3%85%20Miniconda/Anaconda)
		- [2.2 创建虚拟环境 & 安装 PyTorch](#2.2%20%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%20&%20%E5%AE%89%E8%A3%85%20PyTorch)
	- [3. 测试 & 开发：`parser_transitions.py`](#3.%20%E6%B5%8B%E8%AF%95%20&%20%E5%BC%80%E5%8F%91%EF%BC%9A%60parser_transitions.py%60)
	- [4. 训练模型：`parser_model.py` & `run.py`](#4.%20%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%9A%60parser_model.py%60%20&%20%60run.py%60)
		- [4.1 模型实现](#4.1%20%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0)
		- [4.2 开始训练](#4.2%20%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83)
	- [5. 查看 & 提交结果](#5.%20%E6%9F%A5%E7%9C%8B%20&%20%E6%8F%90%E4%BA%A4%E7%BB%93%E6%9E%9C)
	- [6. 可能出现的问题与排查](#6.%20%E5%8F%AF%E8%83%BD%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E6%8E%92%E6%9F%A5)
	- [7. 总结](#7.%20%E6%80%BB%E7%BB%93)
	- [训练过程](#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B)
		- [1. 初始化与数据加载](#1.%20%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD)
		- [2. 训练过程（TRAINING）](#2.%20%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%88TRAINING%EF%BC%89)
		- [3. 测试阶段（TESTING）](#3.%20%E6%B5%8B%E8%AF%95%E9%98%B6%E6%AE%B5%EF%BC%88TESTING%EF%BC%89)
		- [总结](#%E6%80%BB%E7%BB%93)

## 理论

### Part 1: Machine Learning & Neural Networks（8 分）

这是一些**基础理论题**，主要围绕 Adam 优化器和 Dropout 这两种神经网络技术展开。

1. **Adam Optimizer (4分)**  
	(a)(i) 关于动量的直觉:
	- 引入动量可以通过对梯度做"滑动平均",平滑掉单次梯度中的噪声,使参数更新更稳定。
	- 更稳定的更新方向让模型能沿着一致的方向前进,避免在优化过程中来回震荡,从而更容易找到最优解。
	

	(a)(ii) 关于自适应学习率的直觉:

	- 用 $\sqrt{v}$ 缩放是为了自动调节不同参数的更新步长,其中 $v$ 记录了梯度平方的滑动平均。
	- 梯度经常较小的参数会得到较大更新(因为除以小的 $\sqrt{v}$);梯度经常较大的参数会得到较小更新(因为除以大的 $\sqrt{v}$)。
	- 这种自适应机制让每个参数都能以合适的速度学习,避免某些参数更新过快或过慢,使训练更稳定高效。

2. **Dropout (4分)**  
	(b)(i) 系数关系:  
	$\gamma = \frac{1}{1 - p_{\text{drop}}}$  
	原因:为了保证经过dropout后的输出期望值等于原始输出,需要用这个系数来放大保留的神经元输出。

	(b)(ii) 训练与预测时的区别:

	- 训练时使用dropout是为了让网络学会在缺少部分信息时也能正常工作,防止过拟合。
	- 预测时不使用dropout是为了利用网络的全部能力,获得最稳定的结果。因为训练时已经通过缩放保证了输出期望不变,所以可以直接使用完整网络。

这部分你需要**写在书面报告（Assignment 3 [written]）里**，回答上述问题（只需简要解释直觉即可）。

下面给出对这两个问题的简明解答，用简单易懂的语言说明直觉和原理：

- **为什么引入 `m`(动量)可以减少更新的波动?**  
    在随机梯度下降中,每次计算的梯度都有一定的噪声,可能方向不稳定。通过对梯度做一个"滑动平均"(即用 `m` 来记录过去多个梯度的平均),单次梯度中的噪声就会被平滑掉,得到的平均梯度变化更平稳。
    
- **为什么这种"低方差"更新可以帮助训练更稳定地收敛?**  
    当更新的方向不再因为单个样本的噪声而剧烈波动时,模型参数在更新过程中就能沿着更一致的方向前进,避免在"崎岖"的损失曲面上跳来跳去,从而更稳定地接近最优解。

- **为什么用 $\sqrt{v}$ 来缩放更新?**  
    Adam 会维护一个 $v$ 来记录梯度平方的滑动平均,代表各个参数的梯度幅度。如果直接用梯度来更新,不同参数的更新步长可能差异很大。用 $\sqrt{v}$ 来缩放,每个参数的更新步长就会自动调节:
    - 对于梯度较大(变化快)的参数,$v$ 较大,因此更新步长就会缩小;
    - 对于梯度较小(变化平缓)的参数,$v$ 较小,因此更新步长相对较大。
- **哪些参数会得到更大或更小的更新?**
    - **较大更新:** 当一个参数的梯度长期都比较小,即 $v$ 小,这样除以 $\sqrt{v}$ 后,更新步长相对变大。
    - **较小更新:** 当一个参数的梯度经常很大,即 $v$ 大,那么更新时除以较大的 $\sqrt{v}$ 后,更新步长就会变小。
- **这样做对学习有什么好处?**  
    这种自适应的更新方式可以使每个参数都按照其"敏感性"进行调节:
    - 对于容易剧烈变化的参数,自动减小步长,避免"跳过"最优点;
    - 对于变化较平缓的参数,自动增大步长,帮助快速收敛。
        总的来说,这能使整个训练过程更稳定、更高效,避免某些参数更新过大或过小导致训练不平衡。
- **(b)(i) 求出系数 $\gamma$ 与丢弃概率 $p_{\text{drop}}$ 的关系**

	在 Dropout 中,我们对隐藏层 $h$ 的每个神经元随机生成一个 0/1 的掩码 $d$,其中每个元素为 1 的概率为 $1 - p_{\text{drop}}$,为 0 的概率为 $p_{\text{drop}}$。为了保证经过 Dropout 之后的输出 $h_{\text{drop}} = \gamma \, d \odot h$ 的期望值仍然等于原始的 $h$,我们要求:

	$\mathbb{E}[h_{\text{drop}}]= \gamma \, \mathbb{E}[d]\, h = h$

	

	而由于 $\mathbb{E}[d]= 1 - p_{\text{drop}}$,所以我们必须有:

	

	$\gamma \, (1 - p_{\text{drop}}) = 1 \quad \Longrightarrow \quad \gamma = \frac{1}{1 - p_{\text{drop}}}$

	
	**简要说明**:这意味着我们在训练时丢弃一部分神经元,为了保持整体输出水平不变,需要将剩下的神经元的输出放大 $\frac{1}{1 - p_{\text{drop}}}$ 倍。

- **(b)(ii) 为什么训练时用 Dropout,预测时不使用?**
	- **训练时使用 Dropout:**  
	    想象一下,训练时我们随机"关掉"部分神经元,这就像让网络中的每个神经元都不能总是依赖其它固定的神经元,而是需要学会在缺少一些信息的情况下也能工作。这种"打乱"会迫使网络学到更稳健、分布更均匀的特征,从而有效减少过拟合。
	    
	- **预测/推断时不使用 Dropout:**  
	    在预测时,我们希望利用整个网络的全部能力获得最稳定的结果,因此不再随机丢弃神经元,而是使用全部神经元进行计算。因为在训练期间我们已经通过缩放(即乘以 $\gamma = \frac{1}{1 - p_{\text{drop}}}$)来保证整体输出均值一致,预测时直接用全网络可以得到更准确、确定的结果。

	**形象的比喻**:
	- 训练时使用 Dropout 就像在足球比赛中让部分球员临时下场训练,这样其他球员就不得不学会独立作战,从而提高整体团队的应变能力;
	- 而在正式比赛(预测时),你当然希望所有球员都上场,发挥团队的全部实力来争取胜利。

---

### Part 2: Neural Transition-Based Dependency Parsing（44 分）

这一部分是整个作业的核心：**实现一个基于神经网络的依存句法解析器**，并分析一些错误解析案例。

- **输入特征改用嵌入向量** (embedding)。
    - 例如，把当前栈顶 2-3 个单词、缓冲区前面 2-3 个单词等，都变成对应的词向量。
    - 同时可能还会把它们的词性、已经建立好的依存标签等也映射成 embedding。
    - 把所有这些向量**拼接 (concatenate)** 在一起，作为神经网络的输入。
- **神经网络计算**
    - 常见做法：输入层 → 若干隐藏层 (多层感知器 MLP) → 输出层 (softmax)。
    - 输出层会给出对三个动作 (Shift / Left-Arc / Right-Arc) 以及其对应依存标签的概率分布。
    - 训练时，用交叉熵损失 (cross entropy) + 反向传播 (backpropagation) 来更新网络参数，包括嵌入矩阵和全连接层。
- **贪心解码**
    - 在真实解析时，我们从初始状态开始，**每一步都选置信度最高（[softmax](softmax.md)后的最大值）的动作**，直到到达终止状态，从而得到依存树。
    

#### 训练数据

- 第二列 FROM 就是原句子
- 第八列 DEPREL 就是专家标注出的金标准（**真实的依存树**）
- 我们的神经网络依存解析器的目标是预测出一系列状态机转换操作，并使得**预测的依存树**和真实的依存树之间的 loss 最小

```markdown
| ID | FORM      | LEMMA    | UPOS  | XPOS | FEATS | HEAD | DEPREL  | DEPS | MISC |
|----|-----------|----------|-------|------|-------|------|---------|------|------|
| 1  | I         | I        | PRON  | PRP  | _     | 2    | nsubj   | _    | _    |
| 2  | parsed    | parse    | VERB  | VBD  | _     | 0    | root    | _    | _    |
| 3  | this      | this     | DET   | DT   | _     | 4    | det     | _    | _    |
| 4  | sentence  | sentence | NOUN  | NN   | _     | 2    | dobj    | _    | _    |
| 5  | correctly | correctly| ADV   | RB   | _     | 2    | advmod  | _    | _    |
```

- **ID**：每个单词在句子中的位置编号。
- **FORM**：单词的实际形式。
- **LEMMA**：单词的基本形式（例如 "parsed" 的词元为 "parse"）。
- **UPOS**：通用词性标签，例如 PRON（代词）、VERB（动词）、DET（限定词）、NOUN（名词）、ADV（副词）。
- **XPOS**：细粒度词性标签（这里用英语常见标注，如 PRP, VBD, DT, NN, RB）。
- **FEATS**：形态学特征（此处没有额外信息，用 _ 表示）。
- **HEAD**：依存头的编号。例如，"I" 依附于编号为 2 的单词 "parsed"；"sentence" 是 "parsed" 的直接宾语；"this" 是 "sentence" 的限定词；"correctly" 修饰 "parsed"。
- **DEPREL**：依存关系标签，例如 nsubj（主语）、root（根）、det（限定词）、dobj（直接宾语）、advmod（状语）。
- **DEPS** 和 **MISC**：通常为空，用 _ 表示。

这种标注下，整句的依存树大致结构为：

- "parsed" 是句子的根节点；
- "I" 是 "parsed" 的主语；
- "sentence" 是 "parsed" 的直接宾语，其中 "this" 作 "sentence" 的限定词；
- "correctly" 作为副词修饰 "parsed"。

#### 2(a) 理解并列出完整的转移序列 (4 分)

给定示例句 "I parsed this sentence correctly"，以及一张目标依存树，你需要：

- **一步步** 写出 parser 的状态转移（stack、buffer、所用的 transition 以及新增的依存关系）。
- 作业里已经示例了前面三步，你需要把后续步骤完整列出来，直到解析结束。
写在了[依存句法分析](依存句法分析.md)

#### 2(b) 解析一个 n 词句子，需要多少步？(2 分)

- 用简洁的理由说明，对长度为 n 的句子，为什么需要特定数量的 SHIFT / LEFT-ARC / RIGHT-ARC 步骤才能完成依存解析。

> 因为每个词都得从 buffer 到 stack，然后又都得从 stack 到 arcs，所以 n × 2 = 2n 次

#### 2(c) 实现 `PartialParse` 的两个函数 (6 分)

- 你需要在 `parser_transitions.py` 里完成类 `PartialParse` 的 `init` 与 `parse_step` 方法，来实现"单步"转移的逻辑。
- 可以用 `python parser_transitions.py part_c` 进行基本测试（但并非完整测试）。

#### 2(d) 实现 `minibatch_parse` 函数 (8 分)

- 同样在 `parser_transitions.py` 里，按照给定算法"**小批量解析**"多句话。
- 你要用 `model`（提供的一个示例模型）去预测下一步应该做哪种转移，然后对这一小批的 partial parses 同步执行转移。
- 测试命令：`python parser_transitions.py part_d`。

#### 2(e) 训练并实现神经网络依存解析器 (12 分)

- 核心是 `parser_model.py` 和 `run.py` 文件中的若干 TODO：
    1. **`parser_model.py`**
        - `__init__`: 初始化网络，包括手写一个 **Embedding 层**、一个 **线性映射**层（hidden 层），再加一个 **输出层**（logits），以及相应的激活函数 ReLU 等。
        - `embedding_lookup`: 给定词索引列表，把它们映射成对应的 embedding 向量，并拼接（concatenate）。
        - `forward`: 前向传播，输出 logits，再用 softmax 得到预测。
    2. **`run.py`**
        - `train_for_epoch` 和 `train`: 完成训练流程，包括把数据分批、前向/反向传播、更新参数，循环多 epoch。
- 训练完成后，脚本会在 dev/test 数据集上计算 **UAS (Unlabeled Attachment Score)**，并输出结果。
- 你需要写在报告里：
    - 你训练后在 dev set 上的最好 UAS 分数
    - 以及在 test set 上的 UAS 分数
- 参考提示：
    - 你可以先 `python run.py -d` (debug 模式) 进行快速训练，看看是否能到 ~65+ 的 UAS；
    - 如果要跑完整训练，大约需要 **1 小时**，最好在 debug 确认没大问题后再跑全量。

#### 2(f) 错误解析分析 (12 分)

作业给出了四个句子的**错误**依存树，每个句子只有一个错误，对应四种常见错误类型：

1. **Prepositional Phrase Attachment Error (介词短语附着错误)**
2. **Verb Phrase Attachment Error (动词短语附着错误)**
3. **Modifier Attachment Error (修饰语附着错误)**
4. **Coordination Attachment Error (并列结构附着错误)**

对每个句子，你需要：

- 指出**错误类型**是哪一种
- 给出**错误的依存关系**（即解析器产出的错误）
- 给出**正确的依存关系**（应该正确附着到哪个词）

例子里也演示了如果句子里有个介词短语，attach 到了错误的 head，需要改成 attach 到正确的 head。

这部分也写在**书面报告**中提交。

## 实操

### 1. 准备代码目录

1. **确认文件结构**  
    你应该有一个类似 `assignment3/` 的文件夹，其中包含：
    
    - `parser_transitions.py`
    - `parser_model.py`
    - `run.py`
    - `collect_submission.sh`
    - 其他辅助文件（比如 `utils/` 目录内的 `parser_utils.py`，以及训练所需的数据等）
2. **主要脚本作用**
    
    - `parser_transitions.py`：里边实现了 **PartialParse** 类及 `minibatch_parse` 函数，需要先完成/测试它们。
    - `parser_model.py`：核心模型定义（Embedding + Forward + Loss 等），需要用 PyTorch 实现；**注意**不允许直接使用 `torch.nn.Linear` 和 `torch.nn.Embedding`，要自己定义。
    - `run.py`：训练（`train()`）、评估并输出最终结果的主要入口。
    - `collect_submission.sh`：打包提交文件用的脚本。

确保自己的作业代码目录与官方需求一致。

---

### 2. 创建并激活 Python 环境

#### 2.1 安装 Miniconda/Anaconda

如果你的机器上还没有 Python 或 Conda，先去 [Miniconda](https://docs.conda.io/en/latest/miniconda.html) 或 [Anaconda](https://www.anaconda.com/products/individual) 官网下载、安装。安装完成后，在命令行（Terminal / Anaconda Prompt）即可使用 `conda` 命令。

#### 2.2 创建虚拟环境 & 安装 PyTorch

本作业要求安装 **PyTorch 1.4.0**（且 CUDA 选项设为 None，即 CPU 版）和 `tqdm` 等依赖。操作示例：

```bash
# 进入作业目录
cd path/to/assignment3

# 创建并激活一个新的 Conda 环境，名字自定义，比如 dp-env
conda create -n dp-env python=3.7
conda activate dp-env

# 安装 PyTorch 1.4.0（CPU 版）
# 注意如果你想在带 GPU 的机器上训练，可安装相应 CUDA 版本，如 pytorch==1.4.0 cudatoolkit=10.1
pip install torch==1.4.0

# 安装 tqdm
pip install tqdm
```

如果官方给了其他依赖（比如 `numpy`, `matplotlib`, `pytest` 等），也一并 `pip install` 或 `conda install`。

**注意**：在作业中，有些版本可能写成"请从 pytorch.org 手动下载并安装特定版本"。如果你想确保跟作业环境一致，就按照作业说明操作，或者用下面的命令（假设只用 CPU 版）：

```bash
pip install https://download.pytorch.org/whl/cpu/torch-1.4.0%2Bcpu-cp37-cp37m-linux_x86_64.whl
```

(以上只是一个示例 CPU 版 wheel，具体要看官方链接。)

---

### 3. 测试 & 开发：`parser_transitions.py`

1. **实现并测试**  
    在 `parser_transitions.py` 文件中，你需要完成：
    
    - `PartialParse` 类的 `__init__` 和 `parse_step()`
    - `minibatch_parse()` 函数
2. **本地测试**  
    作业给了一个命令来做"基本的（非全面）单元测试"：

    ```bash
    python parser_transitions.py part_c
    ```

    用来测试第 (c) 小问实现的部分（`init` & `parse_step`）。

    

    再执行：

    ```bash
    python parser_transitions.py part_d
    ```

    用来测试第 (d) 小问实现的部分（`minibatch_parse`）。

    

如果所有测试通过，你的输出里会出现类似 "All tests passed for part (c)/(d)" 的提示。

---

### 4. 训练模型：`parser_model.py` & `run.py`

#### 4.1 模型实现

- 在 `parser_model.py` 中，你需要按照作业提示实现**三大部分**：
    
    1. **`__init__`**：初始化参数（比如 `self.embedding_weights`, `self.W`, `self.b1`, `self.U`, `self.b2` 等）。
    2. **`embedding_lookup()`**：自己写 embedding lookup 逻辑，不能直接用 `nn.Embedding`。
    3. **`forward()`**：把输入特征通过 embedding、ReLU 等网络层，得到分类预测 logits，再用 softmax 算出概率。
- **注意** 不要用 `torch.nn.Linear` 或 `torch.nn.Embedding`，要自己写。可以用 `torch.mm`、`torch.matmul`、`torch.add` 之类的基础操作。
    
- 作业里一般会有简单的单元测试命令，比如：

    ```bash
    python parser_model.py -e  # 测试 embedding_lookup()
    python parser_model.py -f  # 测试 forward()
    python parser_model.py -ef # 一起测试
    ```

    如果通过了，会显示类似 "Embedding Sanity Check Passed" 或 "Forward Sanity Check Passed"。

    

#### 4.2 开始训练

完成 `parser_model.py` 后，切换到 `run.py` 文件：

- 里边通常有 `train_for_epoch()`、`train()` 等函数需要完成。请按照注释把它们实现好。
- 然后在命令行执行：

    ```bash
    python run.py
    ```

    这会开始**完整训练**，用 Penn Treebank (UD) 训练集进行训练，用 dev set 做验证，再输出 dev/test 的 UAS 指标。

**常见可选参数**：

- `python run.py -d`：**Debug 模式**，只用很少的数据快速跑几轮，便于调试，不用等太久。
    - 在 Debug 模式下，如果一切正常，训练 Loss 通常能低于 0.2，UAS 能到 65+。
- **正式跑**：去掉 `-d`，用完整数据训练，可能需要 ~1 小时（依赖硬件性能）。最终能得到大约 87+ 的 UAS。

训练完成后，会打印出最终在 dev set 或 test set 上的 **Unlabeled Attachment Score (UAS)**。

---

### 5. 查看 & 提交结果

1. **查看最佳 UAS**  
    训练结束后，终端会显示"Best Dev UAS = XX.XX" 以及 "Test UAS = YY.YY"。这就是你要在作业报告里提交的指标。
    
2. **生成提交文件**  
    如果作业要求在 Gradescope 等平台提交一个 zip，则通常运行：

    ```bash
    sh collect_submission.sh
    ```

    它会在当前目录下生成 `assignment3.zip`。

    
3. **提交到服务器**
    
    - 登录 Gradescope，找到 "Assignment 3[coding]"，上传 `assignment3.zip`。
    - 将你解答好的 PDF 或书面解答部分，上传到 "Assignment 3[written]" 并在 Gradescope 上做题号标记即可。

---

### 6. 可能出现的问题与排查

1. **找不到 `tqdm`**：
    - 确认已经在当前激活的环境 (`dp-env`) 内执行 `pip install tqdm`。
2. **PyTorch 版本不匹配**：
    - 如果你安装的是更高版本的 PyTorch，也许能跑，但若有兼容问题可按作业指定的 1.4.0 版本。
3. **训练速度过慢**：
    - 如果在 CPU 上训练完整数据，可能需要一两个小时以上。可以多开 debug 模式先检查逻辑。
4. **无 GPU 却意外安装了 CUDA**：
    - 可能导致报错 "no cuda device available"。可直接安装 CPU 版或者在代码里把 `device` 设置为 CPU。

---

### 7. 总结

- **最简流程**：
    1. `conda create -n dp-env python=3.7 && conda activate dp-env`
    2. `pip install torch==1.4.0 tqdm`
    3. `python parser_transitions.py part_c` / `part_d` 检查实现的过渡函数
    4. `python parser_model.py -e/-f` 检查 model 的 embedding/forward
    5. `python run.py -d`（快速 debug），再 `python run.py`（完整训练）看 Dev/Test UAS
    6. `sh collect_submission.sh` 生成 zip，上传到 Gradescope

祝你一切顺利，成功跑通作业！如果遇到报错，可以把出错信息（Traceback）贴出来，方便进一步排查。

### 训练过程

下面我逐步解释这个训练过程以及输出信息，帮助你更好地理解整个流程：

---

#### 1. 初始化与数据加载

- **INITIALIZING**  
    开始阶段程序先做一些初始化工作，包括读取数据、构建解析器、加载预训练词向量等。
    - **Loading data... took 0.66 seconds**  
        程序读取数据花费了 0.66 秒。
    - **Building parser... took 0.34 seconds**  
        构建依赖解析器（比如初始化数据结构和模型配置）花费了 0.34 秒。
    - **Loading pretrained embeddings... took 0.92 seconds**  
        加载预训练词向量（这些词向量是提前训练好的用于表示单词的数字向量）。
    - **Vectorizing data... took 0.50 seconds**  
        将数据转换为数值（向量）的形式，以便输入模型。
    - **Preprocessing training data... took 12.98 seconds**  
        对训练数据进行预处理，花费了 12.98 秒。这一步通常涉及生成训练样本、构建部分解析的状态等。

---

#### 2. 训练过程（TRAINING）

整个训练过程分为多个 **epoch**（训练周期），这里共训练 10 个 epoch。

##### 每个 Epoch 的主要步骤

1. **训练阶段**
    
    - 输出例如 `Epoch 1 out of 10` 表示当前是第 1 个周期。
    - 进度条（例如：`1848/1848[00:14<00:00, 127.90it/s]`）显示这一轮训练共处理了 1848 个小批次（batch），整个 epoch 用时大约 14 秒，每秒大约处理 127 个 batch。
    - **Average Train Loss:**  
        例如在 Epoch 1 显示 `Average Train Loss: 0.1827`，表示这一轮中模型的平均损失（误差）大约为 0.183。损失越低，说明模型的预测与真实情况的差距越小。
2. **验证阶段（Evaluating on dev set）**
    
    - 每个 epoch 结束后，模型会在开发集（dev set）上评估性能。
    - 输出例如 `- dev UAS: 83.82` 表示开发集上的 **Unlabeled Attachment Score (UAS)** 为 83.82。UAS 是评估依赖解析器性能的指标，数值越高说明解析效果越好。
    - 当本次 epoch 得到的 UAS 比之前所有 epoch 的 UAS 都要高时，输出 `New best dev UAS! Saving model.` 意味着当前模型被认为是最好的，因此保存下来以备后续测试使用。
3. **随着 Epoch 增加**
    
    - 你可以看到损失逐渐下降（从 0.1827 到 0.0662），而 UAS 分数逐步上升，从 83.82 到 88.90（最高）。
    - 这说明随着训练的进行，模型不断学习和改进，预测的依赖关系更加准确。

---

#### 3. 测试阶段（TESTING）

- 在所有 epoch 结束后，程序会加载在验证集上表现最好的模型。
- **FutureWarning**  
    输出中的警告（关于 `torch.load` 的 FutureWarning）只是提醒在未来的 PyTorch 版本中可能需要更改加载方式，不影响当前运行结果。
- 程序在测试集上进行最终评估，并显示例如 `- test UAS: 89.05`。这表示在测试集上，模型的 UAS 分数为 89.05，说明模型在未知数据上的表现也相当不错。

---

#### 总结

- **初始化阶段**：加载数据、构建模型、预处理数据等。
- **训练阶段**：模型经过 10 个 epoch 的训练，每个 epoch 都会遍历所有训练数据，并在每个周期后在开发集上评估模型性能。随着训练，损失逐步下降，UAS 分数逐步提高。
- **测试阶段**：最后加载在开发集上表现最好的模型，并在测试集上进行评估，得到了 89.05 的 UAS 分数。

这个输出整体说明你的模型训练过程运行正常，并且模型的性能逐步提高，最终在测试集上获得了很好的解析效果。希望这些解释对你有所帮助！

下面给你一个简要的**任务概览**，帮助你理清整个作业需要完成的内容及交付物。在实际完成作业时，你需要同时写一些"**理论部分解答**"（相当于 written 作业）和"**代码实现**"（coding 作业），并按照助教要求提交。

---



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Assignment 3.2 Neural Transition-Based Dependency Parsing.md]
=================================================================================================================

---
draw:
tags: []
title: Assignment 3.2 Neural Transition-Based Dependency Parsing
date created: 2025-01-24
date modified: 2025-01-24
---



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Assignment 3.md]
====================================================================

---
draw:
tags: []
title: Assignment 3
date created: 2025-01-24
date modified: 2025-01-26
---

下面给你一个简要的**任务概览**，帮助你理清整个作业需要完成的内容及交付物。在实际完成作业时，你需要同时写一些"**理论部分解答**"（相当于 written 作业）和"**代码实现**"（coding 作业），并按照助教要求提交。

---

## Part 1: Machine Learning & Neural Networks（8 分）

这是一些**基础理论题**，主要围绕 Adam 优化器和 Dropout 这两种神经网络技术展开。

1. **Adam Optimizer (4 分)**
    
    - (a)(i) 解答关于动量 (momentum) 的直觉：
        - 为什么引入 `m`（动量）可以减少更新的波动？
        - 为什么这种"低方差"更新可以帮助训练更稳定地收敛？
    - (a)(ii) 解答关于自适应学习率 (adaptive learning rate) 的直觉：
        - 为什么用 v\sqrt{v} 来缩放更新？
        - 哪些参数会得到更大或更小的更新？
        - 这样做对学习有什么好处？
2. **Dropout (4 分)**
    
    - (b)(i) 求出 dropout 中的系数 γ\gamma 跟 pdropp_{drop}（丢弃概率）的关系，并简要说明原因。
    - (b)(ii) 为什么在**训练**时要使用 dropout，而在**推断/预测**（evaluation）时则不使用？

这部分你需要**写在书面报告（Assignment 3 [written]）里**，回答上述问题（只需简要解释直觉即可）。

---

## Part 2: Neural Transition-Based Dependency Parsing（44 分）

这一部分是整个作业的核心：**实现一个基于神经网络的依存句法解析器**，并分析一些错误解析案例。你需要完成以下工作：

### 2(a) 理解并列出完整的转移序列 (4 分)

给定示例句 "I parsed this sentence correctly"，以及一张目标依存树，你需要：

- **一步步** 写出 parser 的状态转移（stack、buffer、所用的 transition 以及新增的依存关系）。
- 作业里已经示例了前面三步，你需要把后续步骤完整列出来，直到解析结束。

### 2(b) 解析一个 n 词句子，需要多少步？(2 分)

- 用简洁的理由说明，对长度为 n 的句子，为什么需要特定数量的 SHIFT / LEFT-ARC / RIGHT-ARC 步骤才能完成依存解析。

### 2(c) 实现 `PartialParse` 的两个函数 (6 分)

- 你需要在 `parser_transitions.py` 里完成类 `PartialParse` 的 `init` 与 `parse_step` 方法，来实现"单步"转移的逻辑。
- 可以用 `python parser_transitions.py part_c` 进行基本测试（但并非完整测试）。

### 2(d) 实现 `minibatch_parse` 函数 (8 分)

- 同样在 `parser_transitions.py` 里，按照给定算法"**小批量解析**"多句话。
- 你要用 `model`（提供的一个示例模型）去预测下一步应该做哪种转移，然后对这一小批的 partial parses 同步执行转移。
- 测试命令：`python parser_transitions.py part_d`。

### 2(e) 训练并实现神经网络依存解析器 (12 分)

- 核心是 `parser_model.py` 和 `run.py` 文件中的若干 TODO：
    1. **`parser_model.py`**
        - `__init__`: 初始化网络，包括手写一个 **Embedding 层**、一个 **线性映射**层（hidden 层），再加一个 **输出层**（logits），以及相应的激活函数 ReLU 等。

            > 注意作业要求**不允许**直接用 `torch.nn.Embedding`、`torch.nn.Linear`，而是需要手动写矩阵乘法和参数初始化。

        - `embedding_lookup`: 给定词索引列表，把它们映射成对应的 embedding 向量，并拼接（concatenate）。
        - `forward`: 前向传播，输出 logits，再用 softmax 得到预测。
    2. **`run.py`**
        - `train_for_epoch` 和 `train`: 完成训练流程，包括把数据分批、前向/反向传播、更新参数，循环多 epoch。
- 训练完成后，脚本会在 dev/test 数据集上计算 **UAS (Unlabeled Attachment Score)**，并输出结果。
- 你需要写在报告里：
    - 你训练后在 dev set 上的最好 UAS 分数
    - 以及在 test set 上的 UAS 分数
- 参考提示：
    - 你可以先 `python run.py -d` (debug 模式) 进行快速训练，看看是否能到 ~65+ 的 UAS；
    - 如果要跑完整训练，大约需要 **1 小时**，最好在 debug 确认没大问题后再跑全量。

### 2(f) 错误解析分析 (12 分)

作业给出了四个句子的**错误**依存树，每个句子只有一个错误，对应四种常见错误类型：

1. **Prepositional Phrase Attachment Error (介词短语附着错误)**
2. **Verb Phrase Attachment Error (动词短语附着错误)**
3. **Modifier Attachment Error (修饰语附着错误)**
4. **Coordination Attachment Error (并列结构附着错误)**

对每个句子，你需要：

- 指出**错误类型**是哪一种
- 给出**错误的依存关系**（即解析器产出的错误）
- 给出**正确的依存关系**（应该正确附着到哪个词）

例子里也演示了如果句子里有个介词短语，attach 到了错误的 head，需要改成 attach 到正确的 head。

这部分也写在**书面报告**中提交。

---

## 最终提交

### 1. 代码提交（Assignment 3 [coding]）

1. 你在 `parser_transitions.py`、`parser_model.py` 和 `run.py` 里完成的所有实现。
2. 作业官方提供了 `collect_submission.sh` 脚本来打包你的代码到 `assignment3.zip`。
3. 把 `assignment3.zip` 上传到 Gradescope 的 "**Assignment 3[coding]**"。

### 2. 书面报告提交（Assignment 3 [written]）

你对 Part 1 (Adam + Dropout)、Part 2 (a)(b)(f) 等**问答题**、**错误解析分析**、**模型性能汇报**等写在一个 PDF（或学校要求的格式）里，上传到 "**Assignment 3 [written]**"。

---

## 总结

- **Part 1 (written)**：回答关于 Adam 和 Dropout 的理论题。
- **Part 2 (written + coding)**：
    1. (a)～(b)：手动写依存解析转移序列 & 解析步数；
    2. (c)～(d)：在 `parser_transitions.py` 完成 "单步 + 小批量" 解析逻辑（coding）；
    3. (e)：实现并训练神经网络解析器（在 `parser_model.py` + `run.py` 里 coding），并汇报 UAS 分数（written）；
    4. (f)：分析并给出 4 个句子的错误附着 (attachment) 类型与纠正（written）。

只要你**按作业要求实现并回答上述所有部分**，就完成了 Assignment 3 的全部要求。祝顺利完成！

以下是CS 224n Assignment #3 的主要任务分解和实现要点，帮助你高效完成作业：

---

### **1. 理论部分（8分）**

#### **(a) Adam优化器**

- **问题 i**：解释动量的作用  
  **答案要点**：
  - **动量（Momentum）** 通过维护梯度的移动平均（`m`），使更新方向更稳定，减少随机梯度下降中的震荡。
  - **低方差的好处**：参数更新方向更一致，加速收敛，尤其在损失函数曲率复杂（如峡谷地形）时避免震荡。
- **问题 ii**：自适应学习率的影响  
  **答案要点**：
  - **参数更新幅度**：梯度较小的参数会获得更大的更新（因为 `v` 较小，分母 `sqrt(v)` 较小）。
  - **帮助学习的原因**：自适应调整不同参数的学习率，避免梯度较小的参数被忽略，适合稀疏数据。

#### **(b) Dropout**

- **问题 i**：计算 `γ`  
  **答案要点**：
  - `γ = 1 / (1 - p_drop)`  
  - **推导**：`E[h_drop]= γ * (1 - p_drop) * h = h` ⇒ `γ = 1 / (1 - p_drop)`
- **问题 ii**：训练与评估的区别  
  **答案要点**：
  - **训练时应用**：增强模型鲁棒性，防止过拟合（通过随机失活模拟集成学习）。
  - **评估时不应用**：保持确定性输出，避免随机性干扰预测结果。

---

### **2. 依赖解析实现（44分）**

#### **(a) 解析步骤分析（4分）**

- **句子**："I parsed this sentence correctly"  
  **解析步骤示例**（需补充完整）：

  | 堆栈 (Stack) | 缓冲区 (Buffer) | 新增依赖 (New Dependency) | 操作 (Transition) |
  |--------------|------------------|---------------------------|-------------------|
  |[ROOT]|[I, parsed,...]| -                         | 初始状态          |
  |[ROOT, I]|[parsed,...]| -                         | SHIFT             |
  |[ROOT, parsed]|[this,...]| parsed → I                | LEFT-ARC          |

#### **(b) 解析步骤数（2分）**

- **答案**：`2n - 1` 步（n为句子词数）
  **解释**：每个词需一次SHIFT进入堆栈，每个依赖需一次ARC操作（共n-1次）。

#### **(c) 实现 `PartialParse` 类（6分）**

- **关键函数**：

  ```python
  class PartialParse:
      def __init__(self, sentence):
          self.stack = ["ROOT"]
          self.buffer = list(sentence)
          self.dependencies = []
      
      def parse_step(self, transition):
          if transition == "SHIFT":
              self.stack.append(self.buffer.pop(0))
          elif transition == "LEFT-ARC":
              dependent = self.stack.pop(-2)
              head = self.stack[-1]
              self.dependencies.append((head, dependent))
          elif transition == "RIGHT-ARC":
              dependent = self.stack.pop()
              head = self.stack[-1]
              self.dependencies.append((head, dependent))
  ```

#### **(d) 实现 `minibatch_parse`（8分）**

- **算法核心**：分批处理未完成的解析状态，动态移除已完成解析。
  **代码框架**：

  ```python
  def minibatch_parse(sentences, model, batch_size):
      partial_parses = [PartialParse(s) for s in sentences]
      unfinished_parses = partial_parses.copy()
      while unfinished_parses:
          batch = unfinished_parses[:batch_size]
          transitions = model.predict(batch)
          for parse, trans in zip(batch, transitions):
              parse.parse_step(trans)
          unfinished_parses = [p for p in unfinished_parses if not p.is_complete()]
      return [p.dependencies for p in partial_parses]
  ```

#### **(e) 实现神经网络模型（12分）**

- **关键组件**：
  - **嵌入层（Embedding）**：手动实现（禁止使用 `torch.nn.Embedding`）。
  - **线性层（Linear）**：手动实现矩阵乘法（禁止使用 `torch.nn.Linear`）。

  **前向传播代码示例**：

  ```python
  class ParserModel(nn.Module):
      def __init__(self, embeddings, n_features, hidden_size, n_classes):
          super().__init__()
          self.embed_size = embeddings.shape[1]
          self.embeddings = nn.Parameter(torch.tensor(embeddings))
          self.W = nn.Parameter(torch.randn(n_features * self.embed_size, hidden_size))
          self.b1 = nn.Parameter(torch.zeros(hidden_size))
          self.U = nn.Parameter(torch.randn(hidden_size, n_classes))
          self.b2 = nn.Parameter(torch.zeros(n_classes))
      
      def embedding_lookup(self, w):
          return self.embeddings[w].view(-1, self.embed_size * w.shape[1])
      
      def forward(self, w):
          x = self.embedding_lookup(w)
          h = torch.relu(torch.matmul(x, self.W) + self.b1)
          logits = torch.matmul(h, self.U) + self.b2
          return logits
  ```

- **训练与评估**：
  - **目标**：在开发集（dev set）上达到UAS > 87，测试集（test set）接近92.5。
  - **调参技巧**：调整隐藏层维度（`hidden_size`）、Adam超参数（`β1`, `β2`）、学习率（`α`）。

#### **(f) 错误分析（12分）**

- **错误类型与修正示例**：
  **i. 句子**：

  ```text
  I disembarked and was heading to a wedding fearing my death .
  ```

  - **错误类型**：Verb Phrase Attachment Error  
  - **错误依赖**：`fearing → wedding`  
  - **正确依赖**：`fearing → heading`

  **ii. 句子**：

  ```text
  It makes me want to rush out and rescue people from dilemmas of their own making .
  ```

  - **错误类型**：Coordination Attachment Error  
  - **错误依赖**：`rescue → people`  
  - **正确依赖**：`rescue → rush`

---

### **3. 提交要求**

- **代码**：实现 `parser_transitions.py` 和 `parser_model.py`，确保通过测试。
- **报告**：记录开发集和测试集的UAS分数（目标：开发集 > 87，测试集接近论文的92.5）。
- **错误分析**：对每个句子明确错误类型、错误依赖和正确依赖。

---

### **关键调试技巧**

1. **梯度检查**：使用 `torch.autograd.gradcheck` 验证手动实现的梯度。
2. **维度对齐**：确保所有张量操作维度匹配（如嵌入向量拼接后的输入维度）。
3. **超参数调整**：若训练不稳定，尝试降低学习率（如从 `0.001` 开始）。

通过以上步骤，你将系统完成依赖解析器的实现与理论分析。遇到具体问题时，可进一步讨论代码实现细节或理论推导！



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Assignment 4 RNNs.md]
=========================================================================

---
draw:
title: Assignment 4 RNNs
date created: 2025-01-25
date modified: 2025-02-06
---
- [实操](#%E5%AE%9E%E6%93%8D)
	- [1. 本地开发和调试](#1.%20%E6%9C%AC%E5%9C%B0%E5%BC%80%E5%8F%91%E5%92%8C%E8%B0%83%E8%AF%95)
	- [2. 在 Azure GPU 虚拟机上部署和训练](#2.%20%E5%9C%A8%20Azure%20GPU%20%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%8A%E9%83%A8%E7%BD%B2%E5%92%8C%E8%AE%AD%E7%BB%83)
	- [3. 模型测试和后续工作](#3.%20%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95%E5%92%8C%E5%90%8E%E7%BB%AD%E5%B7%A5%E4%BD%9C)
	- [4. 小贴士](#4.%20%E5%B0%8F%E8%B4%B4%E5%A3%AB)
- [理论](#%E7%90%86%E8%AE%BA)
	- [Assignment 概览](#Assignment%20%E6%A6%82%E8%A7%88)
	- [Part 1: Neural Machine Translation with RNNs (45 分)](#Part%201:%20Neural%20Machine%20Translation%20with%20RNNs%20(45%20%E5%88%86))
		- [(a) (2 分) 在 `utils.py` 里实现 `pad_sents` 函数](#(a)%20(2%20%E5%88%86)%20%E5%9C%A8%20%60utils.py%60%20%E9%87%8C%E5%AE%9E%E7%8E%B0%20%60pad_sents%60%20%E5%87%BD%E6%95%B0)
		- [(b) (3 分) 在 `model_embeddings.py` 里实现 `__init__` 函数](#(b)%20(3%20%E5%88%86)%20%E5%9C%A8%20%60model_embeddings.py%60%20%E9%87%8C%E5%AE%9E%E7%8E%B0%20%60__init__%60%20%E5%87%BD%E6%95%B0)
		- [(c) (4 分) 在 `nmt_model.py` 的 `__init__` 中初始化各层](#(c)%20(4%20%E5%88%86)%20%E5%9C%A8%20%60nmt_model.py%60%20%E7%9A%84%20%60__init__%60%20%E4%B8%AD%E5%88%9D%E5%A7%8B%E5%8C%96%E5%90%84%E5%B1%82)
		- [(d) (8 分) 在 `nmt_model.py` 中实现 `encode()` 函数](#(d)%20(8%20%E5%88%86)%20%E5%9C%A8%20%60nmt_model.py%60%20%E4%B8%AD%E5%AE%9E%E7%8E%B0%20%60encode()%60%20%E5%87%BD%E6%95%B0)
		- [(e) (8 分) 在 `nmt_model.py` 中实现 `decode()` 函数](#(e)%20(8%20%E5%88%86)%20%E5%9C%A8%20%60nmt_model.py%60%20%E4%B8%AD%E5%AE%9E%E7%8E%B0%20%60decode()%60%20%E5%87%BD%E6%95%B0)
		- [(f) (10 分) 在 `nmt_model.py` 中实现 `step()` 函数](#(f)%20(10%20%E5%88%86)%20%E5%9C%A8%20%60nmt_model.py%60%20%E4%B8%AD%E5%AE%9E%E7%8E%B0%20%60step()%60%20%E5%87%BD%E6%95%B0)
		- [(g) (3 分) （书面问题）关于 `generate_sent_masks()` 生成的 `enc_masks`](#(g)%20(3%20%E5%88%86)%20%EF%BC%88%E4%B9%A6%E9%9D%A2%E9%97%AE%E9%A2%98%EF%BC%89%E5%85%B3%E4%BA%8E%20%60generate_sent_masks()%60%20%E7%94%9F%E6%88%90%E7%9A%84%20%60enc_masks%60)
		- [(i) (4 分) 训练完成后，测试并报告 BLEU 分数](#(i)%20(4%20%E5%88%86)%20%E8%AE%AD%E7%BB%83%E5%AE%8C%E6%88%90%E5%90%8E%EF%BC%8C%E6%B5%8B%E8%AF%95%E5%B9%B6%E6%8A%A5%E5%91%8A%20BLEU%20%E5%88%86%E6%95%B0)
		- [(j) (3 分) （书面问题）比较 dot-product、multiplicative、additive attention 的优劣](#(j)%20(3%20%E5%88%86)%20%EF%BC%88%E4%B9%A6%E9%9D%A2%E9%97%AE%E9%A2%98%EF%BC%89%E6%AF%94%E8%BE%83%20dot-product%E3%80%81multiplicative%E3%80%81additive%20attention%20%E7%9A%84%E4%BC%98%E5%8A%A3)
	- [Part 2: Analyzing NMT Systems (30 分)](#Part%202:%20Analyzing%20NMT%20Systems%20(30%20%E5%88%86))
		- [(a) (12 分) 给定 6 个**已知翻译错误**的例子](#(a)%20(12%20%E5%88%86)%20%E7%BB%99%E5%AE%9A%206%20%E4%B8%AA**%E5%B7%B2%E7%9F%A5%E7%BF%BB%E8%AF%91%E9%94%99%E8%AF%AF**%E7%9A%84%E4%BE%8B%E5%AD%90)
		- [(b) (4 分) 观察你自己模型在 test set 上的输出](#(b)%20(4%20%E5%88%86)%20%E8%A7%82%E5%AF%9F%E4%BD%A0%E8%87%AA%E5%B7%B1%E6%A8%A1%E5%9E%8B%E5%9C%A8%20test%20set%20%E4%B8%8A%E7%9A%84%E8%BE%93%E5%87%BA)
		- [(c) (14 分) 手动计算 BLEU 的题目](#(c)%20(14%20%E5%88%86)%20%E6%89%8B%E5%8A%A8%E8%AE%A1%E7%AE%97%20BLEU%20%E7%9A%84%E9%A2%98%E7%9B%AE)
	- [最终提交](#%E6%9C%80%E7%BB%88%E6%8F%90%E4%BA%A4)

## 实操

### 1. 本地开发和调试

**(1) 设置本地虚拟环境**

- 确保你已经安装了 Anaconda（或 Miniconda）。
- 在项目根目录下执行下面的命令来创建并激活本地虚拟环境：

    ```bash
    conda env create --file local_env.yml
    conda activate <your_env_name>  # 替换 <your_env_name> 为 env.yml 中指定的环境名称
    ```

**(2) 生成词汇表**

- 在终端中运行下面的命令来生成所需的词汇表文件：

    ```bash
    sh run.sh vocab
    ```

    该命令会根据数据预处理，生成训练和测试过程中需要用到的词汇表。

**(3) 本地调试训练代码**

- 为了避免在 GPU 上浪费宝贵的时间，先在本地运行简化版的训练脚本。运行：

    ```bash
    sh run.sh train_local
    ```

- 观察程序运行至少到 iter 10 或 iter 20，确保代码能够顺利执行且不会崩溃。如果在这一阶段发现问题，可以及时调试修改。

---

### 2. 在 Azure GPU 虚拟机上部署和训练

**(4) 准备并上传代码到 Azure VM**

- 按照 CS224n Azure Guide 和 "Managing Code Deployment to a VM" 指南配置你的 Azure 虚拟机（确保分配了 GPU 资源）。
- 使用 `scp` 或其它文件传输工具将你本地测试通过的代码上传到 VM 上。

**(5) 安装 GPU 版依赖**

- 在 VM 上进入你的项目目录，运行以下命令安装 GPU 环境所需的依赖包：

    ```bash
    pip install -r gpu_requirements.txt
    ```

**(6) 利用 tmux 运行长时间训练任务**

- 为防止 SSH 连接断开导致训练中断，建议使用 tmux。首先创建一个 tmux 会话：

    ```bash
    tmux new -s nmt
    ```

- 在 tmux 会话内，启动正式的训练（大约需要 4 小时）：

    ```bash
    sh run.sh train
    ```

- 如果需要临时退出 tmux 会话而不终止任务，可以按下 `Ctrl+B` 后再按 `D`（或者直接输入 `tmux detach`）。

**(7) 监控训练进程**

- 你可以随时通过下面的命令重新连接到该会话以查看训练日志：

    ```bash
    tmux attach -t nmt
    ```

---

### 3. 模型测试和后续工作

**(8) 测试模型**

- 当训练完成后（约 4 小时后），在同一个 tmux 会话或新的终端中运行：

    ```bash
    sh run.sh test
    ```

- 该命令会加载训练好的模型，对测试数据进行翻译，并计算 BLEU 分数。翻译结果一般会保存在 `outputs/test_outputs.txt` 文件中。

**(9) 分析和撰写报告**

- 根据测试结果，查阅输出文件，回答 Assignment #4 中关于 NMT 系统分析的书面题目。
- 确保代码部分和书面答案部分都按照要求打包提交到 GradeScope。

---

### 4. 小贴士

- **本地开发先行**：在正式上传到 VM 前，务必在本地充分调试代码，确保基本功能（如 pad_sents、encode、decode、step 等函数）正确实现。
- **合理管理资源**：在不使用 VM 时及时关闭以节省 GPU 使用时间和费用。
- **参考文档**：如果遇到任何问题，建议查阅 CS224n 提供的 Azure Guide、Practical Guide to VMs，以及课程网站上提供的相关文档。

按照以上步骤，你就可以一步步地跑起 Assignment #4 了。祝你训练顺利，取得好成绩！

## 理论

### Assignment 概览

本次作业分为两大部分：

1. **Neural Machine Translation with RNNs (45 分)**  
    以**西语 → 英语**的翻译为例，使用 **双向 LSTM Encoder + 单向 LSTM Decoder**，以及 **multiplicative attention** 来实现一个 Seq2Seq 的 NMT 系统。
    - 你需要**在代码中实现**各个模块（embedding、encode、decode、attention 等），并进行训练和测试，最终得到一个 BLEU 分数。
    - 其中还包含两个**简短的书面分析问题**（(g) 关于 mask 的作用，(j) 关于不同 attention 机制的优劣）。
2. **Analyzing NMT Systems (30 分)**  
    主要是**书面分析题**，包括：
    - 给出已有模型输出的翻译错误，并说明**错误类型**、**原因**以及**改进方法**；
    - 手动计算 BLEU 分数，比较不同参考译文数量时 BLEU 分数的差异，以及讨论 BLEU 指标 vs. 人工评测的优缺点。

这两部分**相互独立**：如果你在第一部分遇到瓶颈，仍然可以先做第二部分的**written**分析题。

---

### Part 1: Neural Machine Translation with RNNs (45 分)

在此部分，你需要主要在 **`nmt_model.py`** 和 **`model_embeddings.py`**（以及一些辅助文件，如 `utils.py`）里完成对应的函数。完成后，在你的代码环境里跑 `sh run.sh train_local` 或者在 Azure VM 上跑 `sh run.sh train` 进行训练（大约需 4 小时 GPU 时间）。

#### (a) (2 分) 在 `utils.py` 里实现 `pad_sents` 函数

- **功能**：对一个 batch 中的句子进行补齐（padding），保证它们有相同长度，方便后续张量运算。
- 你只需在 `pad_sents` TODO 处完成并返回补齐后的结果。

#### (b) (3 分) 在 `model_embeddings.py` 里实现 `__init__` 函数

- **功能**：分别创建**源语言（source）**和**目标语言（target）**的嵌入层 (Embedding layer)。
- 作业要求**不直接使用** `nn.Embedding` 的高层封装，而是要自己写或至少自己初始化相关参数。

#### (c) (4 分) 在 `nmt_model.py` 的 `__init__` 中初始化各层

- **功能**：
    - 建立 NMT 模型需要的层（**LSTM、投影层、dropout**等）
    - 调用 `ModelEmbeddings` 来创建 embeddings
    - 这些层会在后面的 `encode()`、`decode()`、`step()` 等函数里被调用。

#### (d) (8 分) 在 `nmt_model.py` 中实现 `encode()` 函数

- **功能**：
    1. 把补齐后的源语句转换为张量 X；
    2. 送入双向 LSTM（Bi-LSTM），得到 encoder hidden states hench_{enc}；
    3. 线性映射 encoder 最后的 hidden state/cell state 用于初始化 decoder 的 hdec0h_{dec}^0 和 cdec0c_{dec}^0；
- 可以用 `python sanity_check.py 1d` 做一个初步测试。

#### (e) (8 分) 在 `nmt_model.py` 中实现 `decode()` 函数

- **功能**：
    1. 把目标语言词向量（以及前一步的输出向量）拼接，形成 decoder 的输入；
    2. 多步（time step）调用 `step()` 函数，直到处理完目标句所有 token；
    3. 收集 decoder 的输出，用来计算预测分布等。
- 可以用 `python sanity_check.py 1e` 做初步测试。

#### (f) (10 分) 在 `nmt_model.py` 中实现 `step()` 函数

- **功能**：
    1. Decoder LSTM 单步更新 (h_t, c_t)
    2. 计算对所有 encoder hidden states 的 attention 分数 e_t，并做 softmax 得到 attention 分布 α_t
    3. 根据 α_t 做加权求和得到 context 向量 a_t
    4. 将 a_t 和 h_t 拼接、通过一个线性 + tanh + dropout 得到 combined output o_t
    5. 用 `o_t` 最终预测一个词分布
- 可以用 `python sanity_check.py 1f` 做初步测试。

#### (g) (3 分) （书面问题）关于 `generate_sent_masks()` 生成的 `enc_masks`

- 题目让你用**三四句话**说明：
    1. mask 在 attention 计算中起到什么作用？
    2. 为什么在计算注意力分数时，必须这样用 mask 去屏蔽 padding token？

- **mask 的作用**：在 attention 计算中，mask 用于屏蔽输入中那些仅用于填充、没有实际语义信息的位置，确保模型在计算注意力分布时只关注有效的词汇。
- **屏蔽 padding token 的必要性**：因为 padding token 并不携带实际信息，如果不屏蔽，这些位置可能会获得不合理的注意力得分，导致 softmax 后分配非零的概率，从而干扰上下文向量的计算和后续预测。因此，通过将 padding token 对应的注意力得分置为负无穷，保证它们在 softmax 后的概率为 0，使模型只聚焦于真正有用的输入信息。

#### (i) (4 分) 训练完成后，测试并报告 BLEU 分数

- **做法**：
    1. 在 Azure VM 上完整训练（`sh run.sh train`），大约 4 小时；
    2. 训练完后执行 `sh run.sh test` 计算 BLEU；
    3. 在报告中**写出最终的 BLEU**，需要**大于 21**；
- 这部分也记得写在**Written**报告中。

#### (j) (3 分) （书面问题）比较 dot-product、multiplicative、additive attention 的优劣

- **要求**：
    1. 说出 dot-product 相比 multiplicative 的优点和缺点（各 1 点）；
    2. 说出 additive 相比 multiplicative 的优点和缺点（各 1 点）。

把这些回答写在**Written**报告里。

---

### Part 2: Analyzing NMT Systems (30 分)

这一部分**完全是书面分析题**。重点是观察某些翻译错误、分析原因和改进思路，以及手动计算 BLEU。

#### (a) (12 分) 给定 6 个**已知翻译错误**的例子

- 对每个例子：
    1. **指出** NMT 翻译出现了什么错误；
    2. **可能的原因**（模型不足 or 特定语言现象）是什么；
    3. **提出一个改进方案**（可以是模型结构、超参数调整、更多训练数据、改 attention 等）。

#### (b) (4 分) 观察你自己模型在 test set 上的输出

- 你在 part 1 训练后，生成的翻译保存在 `outputs/test_outputs.txt`。
- 需要**找出 2 处翻译错误**（并且和 part (a) 里给出的例子类型不同），对每个错误：
    1. 写下**源句**（在 `test.es`）、**参考译文**（在 `test.en`）、**模型译文**；
    2. 分析错误类型、可能原因、改进方案。

#### (c) (14 分) 手动计算 BLEU 的题目

- 让你根据给出的公式(Modified n-gram precision, brevity penalty 等)对 1 个句子进行 BLEU 计算；
- 分别在有两个参考译文 vs. 只有一个参考译文时，对比两个候选翻译 c1、c2 的 BLEU 分数；
- 问你是否同意 BLEU 的判断、为什么单一参考翻译有局限、BLEU 相比人工评测的优缺点等：

1. (i) (5 分) **同时有两条参考译文** r1,r2r_1, r_2，计算 c1、c2 的 BLEU；谁得分更高，是否符合直觉？
2. (ii) (5 分) **只剩下一条**参考译文r1r_1，再次算 c1、c2 的 BLEU；结果如何，对比之前有何变化？
3. (iii) (2 分) **为何仅有单一参考**对 NMT 评价可能存在问题？
4. (iv) (2 分) BLEU vs 人工评测：**列举 2 个优点和 2 个缺点**。

将这些**详细计算过程与答案**写在**Written**报告里。

---

### 最终提交

作业需要**两次**提交到 GradeScope：

1. **Assignment 4[coding]**
    - 在你的 Azure VM 上执行 `collect_submission.sh`，会打包出 `assignment4.zip`；
    - 下载到本地后上传到 Gradescope。
    - 里面应该包含你写的**`nmt_model.py`、`model_embeddings.py`、`utils.py`** 等代码。
2. **Assignment 4[written]**
    - 把**所有文字分析回答**（包括 Part 1(g), Part 1(j), Part 2(a)(b)(c)）整理成一个 PDF（或学校要求的排版），上传到相应的 Written 作业入口。
    - 在 Gradescope 上"**tag 对应页面**"时，记得把每个题目对应到 PDF 中正确的页码，否则可能被扣分。

完成以上流程，你就完成了 CS224n Assignment BLEU 分数并顺利完成作业！



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Assignment 4.md]
====================================================================

---
draw:
title: Assignment 4
date created: 2025-01-25
date modified: 2025-02-14
---

下面给你一个简要的「**任务清单**」，帮助你快速了解 **Assignment #4** 的所有要求，并知道需要在哪些文件里完成哪些功能，以及有哪些书面回答的题目要写在 Written 作业里并提交。

---

## Assignment 概览

本次作业分为两大部分：

1. **Neural Machine Translation with RNNs (45 分)**  
    以**西语 → 英语**的翻译为例，使用 **双向 LSTM Encoder + 单向 LSTM Decoder**，以及 **multiplicative attention** 来实现一个 Seq2Seq 的 NMT 系统。
    
    - 你需要**在代码中实现**各个模块（embedding、encode、decode、attention 等），并进行训练和测试，最终得到一个 BLEU 分数。
    - 其中还包含两个**简短的书面分析问题**（(g) 关于 mask 的作用，(j) 关于不同 attention 机制的优劣）。
2. **Analyzing NMT Systems (30 分)**  
    主要是**书面分析题**，包括：
    
    - 给出已有模型输出的翻译错误，并说明**错误类型**、**原因**以及**改进方法**；
    - 手动计算 BLEU 分数，比较不同参考译文数量时 BLEU 分数的差异，以及讨论 BLEU 指标 vs. 人工评测的优缺点。

这两部分**相互独立**：如果你在第一部分遇到瓶颈，仍然可以先做第二部分的**written**分析题。

---

## Part 1: Neural Machine Translation with RNNs (45 分)

在此部分，你需要主要在 **`nmt_model.py`** 和 **`model_embeddings.py`**（以及一些辅助文件，如 `utils.py`）里完成对应的函数。完成后，在你的代码环境里跑 `sh run.sh train_local` 或者在 Azure VM 上跑 `sh run.sh train` 进行训练（大约需 4 小时 GPU 时间）。

### (a) (2 分) 在 `utils.py` 里实现 `pad_sents` 函数

- **功能**：对一个 batch 中的句子进行补齐（padding），保证它们有相同长度，方便后续张量运算。
- 你只需在 `pad_sents` TODO 处完成并返回补齐后的结果。

### (b) (3 分) 在 `model_embeddings.py` 里实现 `__init__` 函数

- **功能**：分别创建**源语言（source）**和**目标语言（target）**的嵌入层 (Embedding layer)。
- 作业要求**不直接使用** `nn.Embedding` 的高层封装，而是要自己写或至少自己初始化相关参数。

### (c) (4 分) 在 `nmt_model.py` 的 `__init__` 中初始化各层

- **功能**：
    - 建立 NMT 模型需要的层（**LSTM、投影层、dropout**等）
    - 调用 `ModelEmbeddings` 来创建 embeddings
    - 这些层会在后面的 `encode()`、`decode()`、`step()` 等函数里被调用。

### (d) (8 分) 在 `nmt_model.py` 中实现 `encode()` 函数

- **功能**：
    1. 把补齐后的源语句转换为张量 X；
    2. 送入双向 LSTM（Bi-LSTM），得到 encoder hidden states hench_{enc}；
    3. 线性映射 encoder 最后的 hidden state/cell state 用于初始化 decoder 的 hdec0h_{dec}^0 和 cdec0c_{dec}^0；
- 可以用 `python sanity_check.py 1d` 做一个初步测试。

### (e) (8 分) 在 `nmt_model.py` 中实现 `decode()` 函数

- **功能**：
    1. 把目标语言词向量（以及前一步的输出向量）拼接，形成 decoder 的输入；
    2. 多步（time step）调用 `step()` 函数，直到处理完目标句所有 token；
    3. 收集 decoder 的输出，用来计算预测分布等。
- 可以用 `python sanity_check.py 1e` 做初步测试。

### (f) (10 分) 在 `nmt_model.py` 中实现 `step()` 函数

- **功能**：
    1. Decoder LSTM 单步更新 (h_t, c_t)
    2. 计算对所有 encoder hidden states 的 attention 分数 e_t，并做 softmax 得到 attention 分布 α_t
    3. 根据 α_t 做加权求和得到 context 向量 a_t
    4. 将 a_t 和 h_t 拼接、通过一个线性 + tanh + dropout 得到 combined output o_t
    5. 用 `o_t` 最终预测一个词分布
- 可以用 `python sanity_check.py 1f` 做初步测试。

### (g) (3 分) （书面问题）关于 `generate_sent_masks()` 生成的 `enc_masks`

- 题目让你用**三四句话**说明：
    1. mask 在 attention 计算中起到什么作用？
    2. 为什么在计算注意力分数时，必须这样用 mask 去屏蔽 padding token？

这些答案写在**Assignment 4 [written]**提交的报告中。

### (i) (4 分) 训练完成后，测试并报告 BLEU 分数

- **做法**：
    1. 在 Azure VM 上完整训练（`sh run.sh train`），大约 4 小时；
    2. 训练完后执行 `sh run.sh test` 计算 BLEU；
    3. 在报告中**写出最终的 BLEU**，需要**大于 21**；
- 这部分也记得写在**Written**报告中。

### (j) (3 分) （书面问题）比较 dot-product、multiplicative、additive attention 的优劣

- **要求**：
    1. 说出 dot-product 相比 multiplicative 的优点和缺点（各 1 点）；
    2. 说出 additive 相比 multiplicative 的优点和缺点（各 1 点）。

把这些回答写在**Written**报告里。

---

## Part 2: Analyzing NMT Systems (30 分)

这一部分**完全是书面分析题**。重点是观察某些翻译错误、分析原因和改进思路，以及手动计算 BLEU。

### (a) (12 分) 给定 6 个**已知翻译错误**的例子

- 对每个例子：
    1. **指出** NMT 翻译出现了什么错误；
    2. **可能的原因**（模型不足 or 特定语言现象）是什么；
    3. **提出一个改进方案**（可以是模型结构、超参数调整、更多训练数据、改 attention 等）。

### (b) (4 分) 观察你自己模型在 test set 上的输出

- 你在 part 1 训练后，生成的翻译保存在 `outputs/test_outputs.txt`。
- 需要**找出 2 处翻译错误**（并且和 part (a) 里给出的例子类型不同），对每个错误：
    1. 写下**源句**（在 `test.es`）、**参考译文**（在 `test.en`）、**模型译文**；
    2. 分析错误类型、可能原因、改进方案。

### (c) (14 分) 手动计算 BLEU 的题目

- 让你根据给出的公式(Modified n-gram precision, brevity penalty 等)对 1 个句子进行 BLEU 计算；
- 分别在有两个参考译文 vs. 只有一个参考译文时，对比两个候选翻译 c1、c2 的 BLEU 分数；
- 问你是否同意 BLEU 的判断、为什么单一参考翻译有局限、BLEU 相比人工评测的优缺点等：

1. (i) (5 分) **同时有两条参考译文** r1,r2r_1, r_2，计算 c1、c2 的 BLEU；谁得分更高，是否符合直觉？
2. (ii) (5 分) **只剩下一条**参考译文r1r_1，再次算 c1、c2 的 BLEU；结果如何，对比之前有何变化？
3. (iii) (2 分) **为何仅有单一参考**对 NMT 评价可能存在问题？
4. (iv) (2 分) BLEU vs 人工评测：**列举 2 个优点和 2 个缺点**。

将这些**详细计算过程与答案**写在**Written**报告里。

---

## 最终提交

作业需要**两次**提交到 GradeScope：

1. **Assignment 4[coding]**
    
    - 在你的 Azure VM 上执行 `collect_submission.sh`，会打包出 `assignment4.zip`；
    - 下载到本地后上传到 Gradescope。
    - 里面应该包含你写的**`nmt_model.py`、`model_embeddings.py`、`utils.py`** 等代码。
2. **Assignment 4[written]**
    
    - 把**所有文字分析回答**（包括 Part 1(g), Part 1(j), Part 2(a)(b)(c)）整理成一个 PDF（或学校要求的排版），上传到相应的 Written 作业入口。
    - 在 Gradescope 上"**tag 对应页面**"时，记得把每个题目对应到 PDF 中正确的页码，否则可能被扣分。

完成以上流程，你就完成了 CS224n Assignment 4。祝你一切顺利，得到理想的 BLEU 分数并顺利完成作业！



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Assignment 5 CNN.md]
========================================================================

---
draw:
title: Assignment 5 CNN
date created: 2025-01-26
date modified: 2025-02-06
---

这项实验主要让你设计、实现并训练一个基于字符级别的神经机器翻译系统，具体来说包括以下几个方面：

1. **字符级卷积编码器**：你需要用字符级的卷积神经网络（CNN）来替代传统的词汇查找方式生成词嵌入，这样可以更好地捕捉词内部的形态信息，并处理未登录词（）的问题。
    
2. **字符级LSTM解码器**：当词级解码器生成时，你将通过字符级的长短时记忆网络（LSTM）逐字符生成正确的目标词，从而提高翻译质量，特别是对于稀有词或形态变化丰富的语言。
    
3. **模型集成与训练调试**：你需要将上述模块集成到完整的NMT系统中，从数据预处理、模型搭建、训练到测试进行全面的调试和验证，包括在小数据集上验证模型过拟合以确认实现正确，再在实际数据上进行训练（可能需要数小时）。
    
4. **实验分析**：通过对比字符级模型和传统词级模型的输出及嵌入空间（例如利用TensorFlow的Embedding Projector），你将分析两种方法在处理词形变化和语义捕捉上的不同表现。
    

总的来说，这个实验旨在让你深入理解并实践如何利用子词（字符）信息来构建更健壮的机器翻译系统，从而应对词汇覆盖率、未登录词等问题，同时体验模块化设计、调试和大规模训练的全过程。

下面给你一个「**任务概览**」，帮助你理解 **CS224n: Assignment #5[updated]** 的整体要求，包括代码部分（coding）需要实现的功能，以及书面部分（written）需要回答的分析问题。和之前的作业类似，你将分别在 Gradescope 上提交 **Assignment 5[coding]** 和 **Assignment 5[written]**。注意，这个作业编码量相对大，并且我们提供的辅助检查（sanity-check）非常有限，你需要投入更多时间自测和调试。

---

## 作业整体简介

在这次作业中，你会在 **Assignment #4** 的基础上，改进并扩展 NMT（神经机器翻译）系统，实现**字符级（subword-level）**处理。具体来说，你会让模型在以下两处使用字符级信息：

1. **Character-based CNN Encoder**（Part 1）
   取代之前对词的简单 lookup embedding，改用一个卷积网络（Conv1D）来结合字符嵌入，从而得到每个单词的向量表示。
   - 模型结构包含：字符嵌入、卷积 + max-pooling、highway network、最后 dropout，得到单词级向量。
   - 理想情况下，这可以让模型在遇到罕见词、未登录词（OOV）时表现更好，因为可以学习到字符层面的拼写信息。

2. **Character-based LSTM Decoder**（Part 2）
   当主力的**词级解码器**输出 `<UNK>` 时，转而调用一个**字符级 LSTM** 来"拼出"目标词。
   - 训练时，字符解码器会对所有目标词进行训练（而非只对 `<UNK>` 词）。
   - 推断时，如果词解码器产生 `<UNK>`，就用字符解码器根据之前解码器的隐藏状态，逐字母生成目标词。

最后在 **Part 3** 里，你会对自己训练好的模型做一些分析，包括比较字符级嵌入和传统 Word2Vec 的差异，以及观察字符解码器在实际翻译时的表现。

注意：  

- 这次作业的**代码提示更少**，需要你自己做更多自测。
- 最后完整模型（Part 2 结束后）在 Azure GPU 上要训练 8～12 小时左右。
- 提交时一定要保留 `outputs/test_outputs.txt` 等文件，不要篡改它们；因为提交后自动评分会用到这些文件来验证你跑出的 BLEU 分数。

---

## 目录结构

从官方提供的代码看，主要涉及以下文件/模块：  

1. **`highway.py`**：你需要自己创建 `Highway` 模块（nn.Module）。
2. **`cnn.py`**：你需要自己创建 `CNN` 模块（nn.Module），完成字符嵌入经卷积和 max-pooling 的过程。
3. **`model_embeddings.py`**：你要在 `ModelEmbeddings` 中整合 CNN + Highway + Dropout 等操作，并在 forward() 中对一个 batch 的字符级输入完成处理。
4. **`char_decoder.py`**：字符级 LSTM 解码器的核心文件，包括 forward()、train_forward()、decode_greedy() 等函数。
5. **`nmt_model.py`**：最后将字级 encoder 和字级 decoder 集成到 NMT 模型结构里，一部分地方需要你改动（例如 Part 1(i)）
6. 其他：`vocab.py`、`utils.py`、`run.sh`、若干脚本和数据文件夹等。

---

## Part 1: Character-based Convolutional Encoder for NMT (36 points)

### 1(a)～1(d)：理论与书面问题 (8 分)

1. **(a)** 1D 卷积网络对于可变长度输入的适用性？为什么 RNN 与 CNN 在这方面有何不同？（1 分）
2. **(b)** 在给定 kernel size = 5 时，决定 1D convolution 的 padding 大小，并简要说明理由。（2 分）
3. **(c)** Highway 网络的作用及初始 bias 的设定。（3 分）
4. **(d)** Transformer encoder 相对于 LSTM encoder 的两个优势。（2 分）

这些都是**简答题**，写在 Written 提交文件里即可。

### 1(e)～1(j)：编码实现 (28 分)

1. **(e) (4 分)** 实现 `to_input_tensor_char()`（在 `vocab.py` 中）的功能：
   - 把一个 batch 的句子转换成字符级索引，并对每个单词 pad 到同样长度（batch 内最長）。
   - 输出的张量尺寸要是 `(max_sentence_length, batch_size, max_word_length)`.  
   - 利用 `words2charindices()` 和 `pad_sents_char()` 来实现。
   - `python sanity_check.py 1e` 做初步检查。

2. **(f) (4 分) + (g) (4 分)** 分别实现 `Highway` 和 `CNN` 两个模块（都是 `nn.Module`）：
   - **`highway.py`**：需要写 `__init__()` 和 `forward()`，用 2 个线性层（投影 + gate），然后作 \(x_{highway} = x_{gate} \circ x_{proj} + (1 - x_{gate}) \circ x_{conv\_out} \)。
   - **`cnn.py`**：需要写 `__init__()` 和 `forward()`，使用 PyTorch 的 `nn.Conv1d`（kernel_size=5，padding=1）并后接 max-pooling。
   - 题目要求你自己写一些**测试**来验证功能，然后在报告中**说明你测试了哪些情况**及**为什么足够证明**模块正确（每个模块 4 分主要看你写的测试思路描述）。

3. **(h) (10 分)** 在 `model_embeddings.py` 中完成 `ModelEmbeddings` 类：
   - 整合上述 CNN、Highway 等到一起，把输入的 `(sentence_length, batch_size, max_word_length)` 转成 `(sentence_length, batch_size, e_word)` 的词向量。
   - 记得加 dropout(0.3)。
   - 你可以用 `python sanity_check.py 1h` 做基本检查。

4. **(i) (4 分)** 在 `nmt_model.py` 中，把原来的词表 lookup embedding 替换成**字符级** `ModelEmbeddings`，使 forward() 使用你的新编码方式。
5. **(j) (2 分)** 本地上用脚本 `sh run.sh train_local_q1` + `sh run.sh test_local_q1` 确认小规模数据可 100% 拟合（loss→0，BLEU→99+）。若达不到则需要 debug。

---

## Part 2: Character-based LSTM Decoder for NMT (26 points)

这里，你会在**解码器**层面，如果产生 `<UNK>` 时，就调用一个字符级 LSTM 解码器来"逐字母"生成目标语言的单词：

1. **(a)** (4 分) 在 `char_decoder.py` 里写 `forward()`：
   - 输入字符索引序列 `[x1,..., x_n]`，和初始 (h0, c0)（来自主模型的 combined output），输出各时刻的 logits `[s1,..., s_n]`（对应 `V_char` 大小），以及最终 (h_n, c_n)。
   - `python sanity_check.py 2a` 测试。

2. **(b)** (5 分) 在 `char_decoder.py` 里写 `train_forward()`：
   - 计算字符级解码器的 cross-entropy loss，对应公式 \(\sum_{t=1}^n -\log p_t(x_{t+1})\)（对一个 batch 作和）。
   - 用 `nn.CrossEntropyLoss`。
   - `python sanity_check.py 2b` 测试。

3. **(c)** (8 分) 在 `char_decoder.py` 里写 `decode_greedy()`：
   - 实现算法 1(伪代码) 的贪心生成，但要支持 batch；可以一次循环到 max_length，再在事后截断 `<END>` 之后的字符。
   - `python sanity_check.py 2c` 测试。

4. **(d)** (3 分) 同样先本地做小规模测试 `sh run.sh train_local_q2` + `sh run.sh test_local_q2`，期望看到训练集和 dev 上的 loss→0、perplexity→1，测试集 BLEU>99。
5. **(e)** (6 分) 在 Azure VM 上完整训练 + 测试：
   - `sh run.sh train` 大概需要 8～12 小时。
   - `sh run.sh test` 输出最终的 BLEU 分数，你需要在报告里写下该数值。
   - BLEU < 35 则给 0 分，≥35 给至少 2 分，≥36 给满分 6 分。
   - 训练完后，记得保留 `outputs/test_outputs.txt` 交作业时要一并上传。

---

## Part 3: Analyzing NMT Systems (8 points)

### (a) (2 分)

观察西语动词 "traducir" (意为 "to translate") 的多种变形（比如 `traduzco`, `traduces`, `traduce`, `traduzca`, `traduzcas`等），查找哪些出现在 5 万词表（vocab.json）里。  

- 哪些子形态被包含？哪些没有？
- 为什么对纯词级 NMT 不利？
- 字符级 NMT 如何更好处理这个问题？

### (b) (4 分)

与 Assignment1/2 类似，这里让你对比**Word2Vec 词向量**与**CharCNN 的词向量**在词语最近邻检索时有何区别。

1. **(b)(i)** 在 TF Embedding Projector 上查看 Word2Vec All 预训练向量，对给定词(`financial`, `neuron`, `Francisco`, `naturally`, `expectation`)分别找到最近邻并截图。
2. **(b)(ii)** 用提供的 `character-embeddings.txt` + `metadata.txt` 上传到 TF Embedding Projector，并同样检索相同单词的最近邻。
3. **(b)(iii)**（3 分）对比两者：为什么 Word2Vec 更偏"语义相似"，CharCNN 可能更偏"形态 / 拼写 / 字符相似"？简要解释算法原理导致的区别。

### (c) (2 分)

对比你**Assignment4**里的纯词级模型翻译 `outputs/test_outputs_a4.txt` 与这次**字符级解码**模型翻译 `outputs/test_outputs.txt`：  

- 找到一处 `<UNK>` 被 CharDecoder 成功替换为合适的单词，以及一处 `<UNK>` 被不正确地替换。
- 写下原句（西语）、参考译文、Assignment4 的翻译（标出 `<UNK>`），Assignment5 的翻译（标出生成的单词），并简要分析该现象的可能原因。

---

## 最终提交

你需要**两次**提交到 Gradescope：

1. **Assignment 5[coding]**  
   1. 确保你目录里有这几个输出文件：
      - `outputs/test_outputs.txt`  
      - `outputs/test_outputs_local_q1.txt`  
      - `outputs/test_outputs_local_q2.txt`  
   2. 运行 `collect_submission.sh` 得到 `assignment5.zip`，上传到 Gradescope。
   3. 查看公共测试结果（BLEU 等），确认无问题。
2. **Assignment 5[written]**  
   - 把 Part 1(a)～(d), Part 2(e) 的 BLEU、Part 3(a)～(c) 等所有书面问题回答整理成一个 PDF 并上传。
   - 记得在 Gradescope 上给每个题目正确关联到 PDF 页码。

如此，你就完成了 **CS224n: Assignment #5** 的全部要求。**祝你一切顺利**，在字符级 NMT 的探索中获得理想 BLEU 分数和更多收获！



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Assignment 5.md]
====================================================================

---
draw:
tags: []
title: Assignment 5
date created: 2025-01-26
date modified: 2025-01-26
---

下面给你一个「**任务概览**」，帮助你理解 **CS224n: Assignment #5[updated]** 的整体要求，包括代码部分（coding）需要实现的功能，以及书面部分（written）需要回答的分析问题。和之前的作业类似，你将分别在 Gradescope 上提交 **Assignment 5[coding]** 和 **Assignment 5[written]**。注意，这个作业编码量相对大，并且我们提供的辅助检查（sanity-check）非常有限，你需要投入更多时间自测和调试。

---

## 作业整体简介

在这次作业中，你会在 **Assignment #4** 的基础上，改进并扩展 NMT（神经机器翻译）系统，实现**字符级（subword-level）**处理。具体来说，你会让模型在以下两处使用字符级信息：

1. **Character-based CNN Encoder**（Part 1）
   取代之前对词的简单 lookup embedding，改用一个卷积网络（Conv1D）来结合字符嵌入，从而得到每个单词的向量表示。
   - 模型结构包含：字符嵌入、卷积 + max-pooling、highway network、最后 dropout，得到单词级向量。
   - 理想情况下，这可以让模型在遇到罕见词、未登录词（OOV）时表现更好，因为可以学习到字符层面的拼写信息。

2. **Character-based LSTM Decoder**（Part 2）
   当主力的**词级解码器**输出 `<UNK>` 时，转而调用一个**字符级 LSTM** 来"拼出"目标词。
   - 训练时，字符解码器会对所有目标词进行训练（而非只对 `<UNK>` 词）。
   - 推断时，如果词解码器产生 `<UNK>`，就用字符解码器根据之前解码器的隐藏状态，逐字母生成目标词。

最后在 **Part 3** 里，你会对自己训练好的模型做一些分析，包括比较字符级嵌入和传统 Word2Vec 的差异，以及观察字符解码器在实际翻译时的表现。

注意：  

- 这次作业的**代码提示更少**，需要你自己做更多自测。
- 最后完整模型（Part 2 结束后）在 Azure GPU 上要训练 8～12 小时左右。
- 提交时一定要保留 `outputs/test_outputs.txt` 等文件，不要篡改它们；因为提交后自动评分会用到这些文件来验证你跑出的 BLEU 分数。

---

## 目录结构

从官方提供的代码看，主要涉及以下文件/模块：  

1. **`highway.py`**：你需要自己创建 `Highway` 模块（nn.Module）。
2. **`cnn.py`**：你需要自己创建 `CNN` 模块（nn.Module），完成字符嵌入经卷积和 max-pooling 的过程。
3. **`model_embeddings.py`**：你要在 `ModelEmbeddings` 中整合 CNN + Highway + Dropout 等操作，并在 forward() 中对一个 batch 的字符级输入完成处理。
4. **`char_decoder.py`**：字符级 LSTM 解码器的核心文件，包括 forward()、train_forward()、decode_greedy() 等函数。
5. **`nmt_model.py`**：最后将字级 encoder 和字级 decoder 集成到 NMT 模型结构里，一部分地方需要你改动（例如 Part 1(i)）
6. 其他：`vocab.py`、`utils.py`、`run.sh`、若干脚本和数据文件夹等。

---

## Part 1: Character-based Convolutional Encoder for NMT (36 points)

### 1(a)～1(d)：理论与书面问题 (8 分)

1. **(a)** 1D 卷积网络对于可变长度输入的适用性？为什么 RNN 与 CNN 在这方面有何不同？（1 分）
2. **(b)** 在给定 kernel size = 5 时，决定 1D convolution 的 padding 大小，并简要说明理由。（2 分）
3. **(c)** Highway 网络的作用及初始 bias 的设定。（3 分）
4. **(d)** Transformer encoder 相对于 LSTM encoder 的两个优势。（2 分）

这些都是**简答题**，写在 Written 提交文件里即可。

### 1(e)～1(j)：编码实现 (28 分)

1. **(e) (4 分)** 实现 `to_input_tensor_char()`（在 `vocab.py` 中）的功能：
   - 把一个 batch 的句子转换成字符级索引，并对每个单词 pad 到同样长度（batch 内最長）。
   - 输出的张量尺寸要是 `(max_sentence_length, batch_size, max_word_length)`.  
   - 利用 `words2charindices()` 和 `pad_sents_char()` 来实现。
   - `python sanity_check.py 1e` 做初步检查。

2. **(f) (4 分) + (g) (4 分)** 分别实现 `Highway` 和 `CNN` 两个模块（都是 `nn.Module`）：
   - **`highway.py`**：需要写 `__init__()` 和 `forward()`，用 2 个线性层（投影 + gate），然后作 \(x_{highway} = x_{gate} \circ x_{proj} + (1 - x_{gate}) \circ x_{conv\_out} \)。
   - **`cnn.py`**：需要写 `__init__()` 和 `forward()`，使用 PyTorch 的 `nn.Conv1d`（kernel_size=5，padding=1）并后接 max-pooling。
   - 题目要求你自己写一些**测试**来验证功能，然后在报告中**说明你测试了哪些情况**及**为什么足够证明**模块正确（每个模块 4 分主要看你写的测试思路描述）。

3. **(h) (10 分)** 在 `model_embeddings.py` 中完成 `ModelEmbeddings` 类：
   - 整合上述 CNN、Highway 等到一起，把输入的 `(sentence_length, batch_size, max_word_length)` 转成 `(sentence_length, batch_size, e_word)` 的词向量。
   - 记得加 dropout(0.3)。
   - 你可以用 `python sanity_check.py 1h` 做基本检查。

4. **(i) (4 分)** 在 `nmt_model.py` 中，把原来的词表 lookup embedding 替换成**字符级** `ModelEmbeddings`，使 forward() 使用你的新编码方式。
5. **(j) (2 分)** 本地上用脚本 `sh run.sh train_local_q1` + `sh run.sh test_local_q1` 确认小规模数据可 100% 拟合（loss→0，BLEU→99+）。若达不到则需要 debug。

---

## Part 2: Character-based LSTM Decoder for NMT (26 points)

这里，你会在**解码器**层面，如果产生 `<UNK>` 时，就调用一个字符级 LSTM 解码器来"逐字母"生成目标语言的单词：

1. **(a)** (4 分) 在 `char_decoder.py` 里写 `forward()`：
   - 输入字符索引序列 `[x1,..., x_n]`，和初始 (h0, c0)（来自主模型的 combined output），输出各时刻的 logits `[s1,..., s_n]`（对应 `V_char` 大小），以及最终 (h_n, c_n)。
   - `python sanity_check.py 2a` 测试。

2. **(b)** (5 分) 在 `char_decoder.py` 里写 `train_forward()`：
   - 计算字符级解码器的 cross-entropy loss，对应公式 \(\sum_{t=1}^n -\log p_t(x_{t+1})\)（对一个 batch 作和）。
   - 用 `nn.CrossEntropyLoss`。
   - `python sanity_check.py 2b` 测试。

3. **(c)** (8 分) 在 `char_decoder.py` 里写 `decode_greedy()`：
   - 实现算法 1(伪代码) 的贪心生成，但要支持 batch；可以一次循环到 max_length，再在事后截断 `<END>` 之后的字符。
   - `python sanity_check.py 2c` 测试。

4. **(d)** (3 分) 同样先本地做小规模测试 `sh run.sh train_local_q2` + `sh run.sh test_local_q2`，期望看到训练集和 dev 上的 loss→0、perplexity→1，测试集 BLEU>99。
5. **(e)** (6 分) 在 Azure VM 上完整训练 + 测试：
   - `sh run.sh train` 大概需要 8～12 小时。
   - `sh run.sh test` 输出最终的 BLEU 分数，你需要在报告里写下该数值。
   - BLEU < 35 则给 0 分，≥35 给至少 2 分，≥36 给满分 6 分。
   - 训练完后，记得保留 `outputs/test_outputs.txt` 交作业时要一并上传。

---

## Part 3: Analyzing NMT Systems (8 points)

### (a) (2 分)

观察西语动词 "traducir" (意为 "to translate") 的多种变形（比如 `traduzco`, `traduces`, `traduce`, `traduzca`, `traduzcas`等），查找哪些出现在 5 万词表（vocab.json）里。  

- 哪些子形态被包含？哪些没有？
- 为什么对纯词级 NMT 不利？
- 字符级 NMT 如何更好处理这个问题？

### (b) (4 分)

与 Assignment1/2 类似，这里让你对比**Word2Vec 词向量**与**CharCNN 的词向量**在词语最近邻检索时有何区别。

1. **(b)(i)** 在 TF Embedding Projector 上查看 Word2Vec All 预训练向量，对给定词(`financial`, `neuron`, `Francisco`, `naturally`, `expectation`)分别找到最近邻并截图。
2. **(b)(ii)** 用提供的 `character-embeddings.txt` + `metadata.txt` 上传到 TF Embedding Projector，并同样检索相同单词的最近邻。
3. **(b)(iii)**（3 分）对比两者：为什么 Word2Vec 更偏"语义相似"，CharCNN 可能更偏"形态 / 拼写 / 字符相似"？简要解释算法原理导致的区别。

### (c) (2 分)

对比你**Assignment4**里的纯词级模型翻译 `outputs/test_outputs_a4.txt` 与这次**字符级解码**模型翻译 `outputs/test_outputs.txt`：  

- 找到一处 `<UNK>` 被 CharDecoder 成功替换为合适的单词，以及一处 `<UNK>` 被不正确地替换。
- 写下原句（西语）、参考译文、Assignment4 的翻译（标出 `<UNK>`），Assignment5 的翻译（标出生成的单词），并简要分析该现象的可能原因。

---

## 最终提交

你需要**两次**提交到 Gradescope：

1. **Assignment 5[coding]**  
   1. 确保你目录里有这几个输出文件：
      - `outputs/test_outputs.txt`  
      - `outputs/test_outputs_local_q1.txt`  
      - `outputs/test_outputs_local_q2.txt`  
   2. 运行 `collect_submission.sh` 得到 `assignment5.zip`，上传到 Gradescope。
   3. 查看公共测试结果（BLEU 等），确认无问题。
2. **Assignment 5[written]**  
   - 把 Part 1(a)～(d), Part 2(e) 的 BLEU、Part 3(a)～(c) 等所有书面问题回答整理成一个 PDF 并上传。
   - 记得在 Gradescope 上给每个题目正确关联到 PDF 页码。

如此，你就完成了 **CS224n: Assignment #5** 的全部要求。**祝你一切顺利**，在字符级 NMT 的探索中获得理想 BLEU 分数和更多收获！



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/CS224n Natural Language Processing.md]
==========================================================================================

---
draw:
title: CS224n Natural Language Processing
date created: 2025-01-23
date modified: 2025-02-14
---

| 课程编号  | 主题                                                  | 主要知识点                                                                                                              |                   |
| ----- | --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ | ----------------- |
| 1 & 2 | [[词向量]] (Word Vectors)                              | - 词义表示：one-hot 向量、词义相似度<br>- Word2vec (skip-gram, CBOW)：目标函数、梯度推导、负采样<br>- GloVe：共现矩阵、目标函数<br>- 词向量的评估方法：内在评估、外在评估 | 输入是句子，输出是词向量      |
| 3     | [[神经网络]] (Neural Networks)                          | - 神经网络基础：神经元、激活函数、前向传播、反向传播<br>- 梯度下降：随机梯度下降、小批量梯度下降<br>- 神经网络的训练技巧：正则化、dropout<br>- 神经网络的应用：情感分析                  | 用线性函数+非线性函数拟合一切规律 |
| 4     | [[依存句法分析]] (Dependency Parsing)                     | - 依存句法：依存树、依存关系<br>- 基于转移的依存句法分析：arc-standard 转移系统<br>- 神经网络在依存句法分析中的应用                                            |                   |
| 5     | 循环神经网络[RNN](RNN.md) (Recurrent Neural Networks)     | - RNN 的基本结构：隐藏状态、时间步<br>- RNN 的变体：LSTM、GRU<br>- 语言模型：RNN 语言模型、困惑度<br>- RNN 的应用：命名实体识别                              |                   |
| 6     | 神经机器翻译 ([[Neural Machine Translation]])             | - 序列到序列模型 (Seq2Seq)：[编码器、解码器](编码器、解码器.md)<br>- [[注意力机制]] (Attention)：点积注意力、缩放点积注意力<br>- Beam search：提高翻译质量的方法      |                   |
| 7     | [[问答系统]] (Question Answering)                       | - 阅读理解式问答：给定文章和问题，找到答案<br>- BiDAF 模型：双向注意力流<br>- 其他问答模型：Match-LSTM、QANet                                           |                   |
| 8     | 卷积神经网络[CNN](CNN.md) (Convolutional Neural Networks) | - CNN 的基本结构：卷积层、池化层<br>- CNN 在 NLP 中的应用：文本分类、句子建模                                                                  |                   |

## 整体思路

这门课的整体设计思路，可以理解为**一步步搭建从语言表示到复杂语言任务的系统化能力**。换句话说，课程是从最基本的"词语如何表示"开始，一路到如何"让模型在复杂任务中理解并生成语言"。最终目标是**帮助学生构建出对自然语言有深层次理解和处理能力的模型**，能够在各种下游 NLP 场景中取得好表现，比如问答、机器翻译、文本分类等。

下面从整体脉络上说明课程为什么要这么安排、以及它想要解决的核心问题：

1. **词向量 (课程 1 & 2)**
    
    - **为什么要学**：语言中最小的可学习单元就是"词"。我们需要一种方法，把"词"映射到向量空间中，让后续神经网络可以处理。
    - **要解决的核心问题**：如何让机器知道单词之间"相似"或"不同"？如何让机器具备最基础的"词义"感知？
    - **关键概念**：one-hot 向量、word2vec、GloVe、评估方法。
    - **作用**：这是 NLP 的**最底层**表示环节，几乎所有后续任务都依赖高质量的词向量。
2. **神经网络基础 (课程 3)**
    
    - **为什么要学**：有了词向量，下一步就要用模型去"学习、拟合"更复杂的语言规律。神经网络是目前最常用、最有效的处理方法。
    - **要解决的核心问题**：怎么让神经网络学会从数据中更新权重（梯度下降、反向传播），如何防止过拟合（正则化、dropout）？
    - **关键概念**：前向传播、反向传播、训练技巧、激活函数。
    - **作用**：这是现代 NLP 的"通用计算引擎"，为后续任何深度模型打下基础。
3. **依存句法分析 (课程 4)**
    
    - **为什么要学**：句法结构是语言理解的重要一环。只知道单词含义往往不够，还需要了解它们之间的"句法关系"。
    - **要解决的核心问题**：如何快速、准确地从一个句子里分析出依存树？进而帮助机器理解"谁是主语、谁是宾语、修饰关系如何"等。
    - **关键概念**：转移系统 (arc-standard)、神经网络在句法分析中的使用。
    - **作用**：让模型不仅能"读懂词"，也能"读懂句子结构"。
4. **循环神经网络 RNN (课程 5)**
    
    - **为什么要学**：语言本质上是序列数据，句子中的词是按时间步一个个出现的。RNN 能捕捉序列信息，尤其在长文本、上下文关联里非常常用。
    - **要解决的核心问题**：如何在一段变长的序列中"保留"上下文信息？如何解决梯度消失和梯度爆炸（LSTM、GRU）？
    - **关键概念**：RNN 结构、LSTM、GRU、语言模型、困惑度。
    - **作用**：为**序列建模**任务奠定基础，包括文本生成、序列标注（如命名实体识别）、语言模型等。
5. **神经机器翻译 (课程 6)**
    
    - **为什么要学**：机器翻译是 NLP 中最经典、复杂度高的核心任务之一。它需要**序列到序列 (Seq2Seq)** 思想，并引入**注意力机制**来更好地处理长句。
    - **要解决的核心问题**：如何把一句话（源语言）准确地映射为另一种语言的句子？怎样避免信息丢失？如何提升翻译的流畅度和准确度？
    - **关键概念**：编码器-解码器框架、注意力机制 (Attention)、Beam Search。
    - **作用**：通过翻译任务，学生能更全面地理解序列到序列模型和注意力机制，这些思想也可推广到其他生成式或序列映射任务。
6. **问答系统 (课程 7)**
    
    - **为什么要学**：问答系统是让模型真正"理解文本并作出回答"的典型任务；比翻译更贴近"内容理解"本身。
    - **要解决的核心问题**：给定一段文章和一个问题，怎么准确找到或生成答案？
    - **关键概念**：阅读理解式问答 (machine reading comprehension)、BiDAF、Match-LSTM、QANet 等。
    - **作用**：体现神经网络如何利用注意力、如何多次交互信息来回答问题，是 NLP 实际应用中的关键场景之一。
7. **卷积神经网络 CNN (课程 8)**
    
    - **为什么要学**：CNN 在图像处理里非常流行，但在文本上同样有用。它可以捕捉**局部 n-gram 特征**，在文本分类、情感分析等场景表现优异。
    - **要解决的核心问题**：如何在 NLP 场景中将卷积操作运用于变长句子？如何通过池化固定输出大小？
    - **关键概念**：卷积核、池化 (max-pooling, k-max)、多通道词向量。
    - **作用**：与 RNN 形成互补。CNN 更适合并行，能快速处理文本，也能学习到局部组合特征，是构建 NLP 模型的另一大主流方案。

---

## 为什么说这些模块共同解决的是"语言理解与生成"这个大问题？

- **多角度的语言表示**：从词向量、依存树到序列模型，课程提供了多种刻画语言的方法。
- **多任务驱动**：依存句法、机器翻译、问答系统等都是 NLP 里最常见、也最能体现"理解与生成"能力的任务。
- **多模型与技巧**：课程围绕神经网络（包括 CNN、RNN、Seq2Seq、注意力）一步步升级，让学生掌握如何从基础结构到复杂应用做端到端的模型设计与实现。

最终，这些**知识点与方法**的组合，正是当下构建**通用 NLP 系统**所需的核心模块。从词向量到深层神经网络，从句法到语义，从理解到生成，为的是让机器不仅能"背单词"，更能"看懂、翻译、回答问题"，即实现对文本的**深度理解和处理**。这正是这门课整体设计的根本目的。

## 相关资料

https://csdiy.wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CS224n/#_1  
官网  
https://web.stanford.edu/class/cs224n/index.html#schedule  
代码  
https://github.com/PKUFlyingPig/CS144-Computer-Network  
[3Blue1Brown](3Blue1Brown.md)的视频  
https://www.youtube.com/watch?v=LPZh9BOjkQs&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5  
课堂笔记  
https://cenleiding.github.io/cs224n%E7%AC%94%E8%AE%B0.html

## Assignment

[[Assignment 1 Exploring Word Vectors]]  
[[Assignment 2 word2vec]]  
[[Assignment 3]]  
[[Assignment 4]]  
[[Assignment 5]]

## 知识点

[[Word2vec]]

### 损失函数

让我来解释这段内容的核心意思，这里在讲神经网络的训练目标：

1. 首先明确两种输入：
- 真实样本："Museums in Paris are amazing"（标记为s）
- 错误样本："Not all museums in Paris"（标记为sc）

1. 初始的训练目标：
- 最大化(s - sc)或最小化(sc - s)
- 也就是希望真实样本的分数比错误样本的分数高

1. 第一次改进：

```python
minimize J = max(sc - s, 0)
```

这样改进的原因是：

- 只有当sc > s时（即错误样本分数高于真实样本）才计算损失
- 当s > sc时（即真实样本分数已经更高）就不需要再训练了
- 这符合我们的直觉：只要真实样本分数更高就行，不需要高太多

1. 第二次改进：

```python
minimize J = max(Δ + sc - s, 0)
```

这次改进增加了一个边界Δ：

- 不只是要求s > sc
- 而是要求s > sc + Δ
- Δ通常取1
- 这就是所谓的"安全边界"（margin of safety）

这种设计类似于SVM（支持向量机）中的间隔概念：

- 不仅要能正确分类
- 还要在正确和错误之间保持一定的"安全距离"
- 这样的模型通常泛化能力更强，更稳定

这就是为什么后面提到这个概念在SVM（支持向量机）研究中经常出现。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/GloVe.md]
=============================================================

---
draw:
tags: []
title: GloVe
date created: 2025-01-24
date modified: 2025-01-24
---

> _先用世界上所有的文本得到共现矩阵，
> 然后确定好参数也就是想做成几维，之后给每个词一个初始 embedding，
> 再通过随机梯度下降移动这些 embedding，直到两个 embedding 的点积等于最初共生矩阵里的数的对数

大体上可以这么理解，**GloVe** 确实是基于全局共现信息来训练词向量的。

1. **GloVe 与传统"共现矩阵 + SVD"思路的关系**
    
    - GloVe 的核心思想是：词和词在全球范围内（整个语料）的共现频次 (co-occurrence frequency)，能提供一个比较稳定的全局统计信息。
    - 它会构建一个反映词之间"共现关系" (co-occurrence) 的目标函数，然后进行迭代式的优化训练，得到每个词的向量表示。
    - 表面上看，GloVe 也和"共现矩阵 + 降维 (SVD)"的思路接近，但是 **GloVe 并不是简单地先显式构造一个完整 `N×N` 的共现矩阵再去做 SVD**，而是利用了一个针对共现频次的损失函数（以及权重函数）来直接训练出词向量。对比简单的 "共现矩阵 + SVD"，GloVe 的训练目标会更灵活，也能避免储存超大矩阵的问题。
2. **预训练好的 GloVe 词向量**
    
    - 你在实验中用的 "GloVe 数据库" 或者官方提供的 "GloVe 预训练词向量"（如 `glove.6B`, `glove.42B`, `glove.840B` 等），确实是作者在**大规模语料**（比如维基百科、Common Crawl、Gigaword 等）上，按照 GloVe 的算法训练好后发布的。
    - 这就好比他们在非常大的文本上做了全局的"共现统计 + 训练"，得到了一套"通用" (general) 的词向量空间。我们可以直接拿过来用，不需要自己在本地再做"从头训练"。
3. **为什么成为"General 的向量空间"**
    
    - 因为他们的语料库覆盖面很广，包含新闻、百科等各种领域，学到的词向量能够在较多一般场景下表现得不错。
    - 如果你的任务或领域非常特殊，也可以考虑自己在专业语料上重新训练（或者在通用 GloVe 上做微调），但最常见的方式是直接用这些预训练好的向量做初始化或做特征，帮助下游 NLP 任务。

因此，用你的话来说，**"GloVe数据库"可以理解为：他们（GloVe 的作者 / 维护者）在非常大规模的文本上做了'共现统计'以及对应的目标优化，最终得到一个通用的词向量空间**。我们下载后就可以直接使用它来进行相似度计算、类比、下游模型的 embedding 初始化等。

1. 首先构建共现矩阵X:

```Java
假设窗口大小=2,统计得到的共现次数是:
      cat  dog  pet
cat    2    3    5
dog    3    1    4
pet    5    4    2
```

2. 对共现矩阵取对数log(X):

```Java
      cat   dog   pet
cat   0.69  1.10  1.61
dog   1.10  0     1.39
pet   1.61  1.39  0.69

(注:这里是自然对数ln,保留两位小数)
```

3. 开始训练GloVe模型:  
假设我们想要学习2维的词向量(实际应用中通常是50-300维)。

初始随机化词向量和偏置项:

```Java
# 随机初始化的词向量(wi):
cat = [0.2, -0.5]
dog = [0.1,  0.3]
pet = [-0.3, 0.4]

# 随机初始化的偏置项(bi):
cat_bias = 0.1
dog_bias = -0.2
pet_bias = 0.3
```

1. 计算一个具体的预测值:  
以cat和dog为例:

```Java
# 计算 cat·dog + b_cat + b_dog:
cat·dog = (0.2 × 0.1) + (-0.5 × 0.3) = -0.13
预测的值 = 词向量相似度(点积) + 第一个词的偏好 + 第二个词的偏好
预测值 = -0.13 + 0.1 + (-0.2) = -0.23

# 与实际的log值比较:
真实值 = 1.10
误差 = -0.23 - 1.10 = -1.33
```

2. 更新向量(使用梯度下降):

```Java
learning_rate = 0.1

# 更新cat的向量:
new_value = old_value - learning_rate × gradient × error
# 原值 = 0.2 
# learning_rate = 0.1 
# error = -1.33 
# gradient = dog[0] = 0.1

cat[0] = 0.2 + 0.1 × (-1.33) × dog[0] = 0.187
cat[1] = -0.5 + 0.1 × (-1.33) × dog[1] = -0.54

# 更新dog的向量:
dog[0] = 0.1 + 0.1 × (-1.33) × cat[0] = 0.075
dog[1] = 0.3 + 0.1 × (-1.33) × cat[1] = 0.37

# 更新偏置项:
cat_bias = 0.1 + 0.1 × (-1.33) = -0.033
dog_bias = -0.2 + 0.1 × (-1.33) = -0.333
```

3. 重复这个过程:
- 对所有词对重复这个计算过程
- 多次迭代整个数据集
- 直到词向量收敛

最终得到的词向量会使得:

```Java
cat·dog + b_cat + b_dog ≈ log(3)
cat·pet + b_cat + b_pet ≈ log(5)
dog·pet + b_dog + b_pet ≈ log(4)
```

这样,通过最小化这些预测值与实际log共现值之间的差异,我们就得到了能够捕捉词语语义关系的词向量。这些词向量的几何关系会反映词语之间的语义关系。

所以GloVe：

- ✓ 确实使用了全局的共现统计
- ✗ 但不是简单地分析共现矩阵
- ✓ 而是训练了一个模型来学习词向量
- ✓ 使词向量能够捕捉词之间的语义关系

这就是为什么它叫"Global Vectors"：

- Global：使用全局统计信息
- Vectors：通过模型学习获得的向量表示

这种方法的优势是：

1. 保留了全局统计信息
2. 同时通过模型学习得到更好的语义表示
3. 词向量具有良好的线性代数性质



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Neural Machine Translation.md]
==================================================================================

---
draw:
tags: []
title: Neural Machine Translation
date created: 2025-01-29
date modified: 2025-01-29
---

这些任务需要"从一个序列映射到另一个序列"，因此就有了 Seq2Seq 模型。-

- **机器翻译**：输入是某种语言的句子，输出是另一种语言的句子。
- **对话系统**：根据上一句话，输出一个回答或回应。
- **文本摘要**：输入可能是长篇文章，输出是简短摘要。

## 1. 编码器（Encoder）

输入**英文句子** "编码" 成**一个固定长度的向量**

Seq2Seq 的编码器主要将输入序列（例如英文句子）"编码"成一个固定长度的向量（也叫"上下文向量" context vector）。这个向量可以被视为对整个输入序列含义的压缩表达。

让我们以翻译 "What is your name?" 为例:

1. **词向量转换**  
假设我们用 100 维的词向量,每个词会被转换成对应的向量:
- "What" →[0.2, -0.5, 0.1,..., 0.3]
- "is" →[-0.1, 0.3, 0.4,..., -0.2]
- "your" →[0.5, 0.1, -0.3,..., 0.1]
- "name" →[-0.3, 0.2, 0.1,..., 0.4]
- "?" →[0.1, -0.1, 0.2,..., -0.1]

1. **双向 LSTM 处理**  
我们用双向 LSTM 处理这个序列:

```Java
正向: What → is → your → name → ?
反向: ? ← name ← your ← is ← What
```

每个时间步,LSTM 会:

- 读入当前词的词向量
- 更新内部状态(记忆单元和隐藏状态)
- 输出一个隐藏状态向量

比如对于 "your" 这个位置:

- 正向 LSTM 已经看过 "What is"
- 反向 LSTM 已经看过 "? name"
- 两个方向的隐藏状态拼接后能同时利用前后文信息

1. **得到上下文向量**  
最后我们得到一个固定维度(比如 256 维)的上下文向量 C,它压缩了整个句子的信息,包含:
- 句子是疑问句
- 询问对方名字
- 语气比较礼貌正式
- 等等...

这个上下文向量 C 就会被传递给解码器,作为生成目标语言(比如中文)翻译的基础。

## 2. 解码器（Decoder）

让我用英译中的例子来说明解码器的工作过程,就拿 "What is your name?" → "你叫什么名字？" 为例:

1. **初始化阶段**
- 用编码器的上下文向量 C 初始化解码器的隐藏状态
- 输入起始符 ⟨GO⟩

1. **第一步生成**  
输入: ⟨GO⟩
- 解码器读入 ⟨GO⟩ 的向量表示
- 结合隐藏状态,计算所有中文字/词的概率分布:
  - "你": 0.82
  - "请问": 0.10
  - "告诉": 0.05
  - 其他: 0.03
- 选择概率最高的 "你" 作为第一个输出

1. **第二步生成**  
输入: "你"
- 将 "你" 的词向量输入 LSTM
- 更新隐藏状态
- 预测下一个字的概率:
  - "叫": 0.78
  - "是": 0.12
  - "好": 0.06
  - 其他: 0.04
- 输出 "叫"

1. **后续步骤**
- 输入: "叫" → 输出: "什"
- 输入: "什" → 输出: "么"
- 输入: "么" → 输出: "名"
- 输入: "名" → 输出: "字"
- 输入: "字" → 输出: "？"
- 输入: "？" → 输出: ⟨EOS⟩

值得注意的是,中文解码相比法语有一些特点:

1. 输出单位可能是字或词(取决于分词方式)
2. 语序差异更大(比如英语的疑问词在句首,中文在句中)
3. 需要处理标点符号的转换(如 "?" → "？")

解码器要在每一步都权衡:

- 已经生成的部分("你叫")是否通顺
- 未来要生成的部分("什么名字？")是否符合中文表达习惯
- 整体是否准确传达了原句的意思

这个中文的例子是否让解码过程更容易理解了?

## 注意力机制

让我用这个翻译例子具体解释为什么需要注意力机制。

假设我们要翻译一个较长的句子：
"The old man who lives next door to my family has a beautiful garden that blooms with colorful flowers every spring."  
→  
"住在我家隔壁的那位老人有一个漂亮的花园，每到春天都会开满彩色的花。"

**没有注意力机制时的问题**：
1. **信息瓶颈**
- 编码器必须把这整句28个英文单词的所有信息(人物特征、位置关系、花园特点、时间等)都压缩到一个固定维度(比如256维)的向量中
- 当解码器要生成"老人"时，相关信息("old man")可能已经在压缩过程中变得模糊
- 当要生成"春天"时，"spring"的信息可能已经被覆盖或削弱

1. **解码困境**
- 解码器只能依赖这一个全局向量来生成所有中文词
- 比如生成"花园"时，需要从这个向量中提取关于"garden"的信息
- 但这个信息可能已经和其他信息(如"老人"、"春天"等)混在一起，难以准确提取

**加入注意力机制后**：
- 生成"老人"时，可以主动关注输入中的"old man"部分的隐藏状态
- 生成"花园"时，可以聚焦于"beautiful garden"相关的隐藏状态
- 生成"春天"时，可以重点关注"every spring"对应的表示

这就像一个人在翻译时：

- 不用一次性记住整句话的所有细节
- 而是可以反复看原文的不同部分
- 需要翻译哪部分就重点看哪部分
- 从而保证翻译的准确性

这样解释是否帮助你更好地理解为什么需要注意力机制？



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Sigmoid函数.md]
=================================================================

---
draw:
tags: []
title: Sigmoid函数
date created: 2025-01-24
date modified: 2025-01-24
---

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2025%2F01%2F24%2F02-56-21-609d4f608183271e52fbb582766137b6-202501240256599-5424c3.png)  

$$σ(x) =1/(1+\exp(-x))$$



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Untitled.md]
================================================================

---
draw:
tags: []
title: Untitled
date created: 2025-01-24
date modified: 2025-01-24
---



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/Word2vec.md]
================================================================

---
draw:
tags: []
title: Word2vec
date created: 2025-01-25
date modified: 2025-01-25
---

非常有效的概率模型：Word2vec。Word2vec 是一个软件包实际上包含：

- **两个算法**：continuous bag-of-words（CBOW）和 skip-gram。CBOW 是根据中心词周围的上下文单词来预测该词的词向量。skip-gram 则相反，是根据中心词预测周围上下文的词的概率分布。
- **两个训练方法**：negative sampling 和 hierarchical softmax。Negative sampling 通过抽取负样本来定义目标，hierarchical softmax 通过使用一个有效的树结构来计算所有词的概率来定义目标。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/softmax.md]
===============================================================

---
draw:
tags: []
title: softmax
date created: 2025-01-24
date modified: 2025-01-24
---

```mermaid
flowchart LR
    A[输入向量] --> B[计算指数]
    B --> C[求和]
    C --> D[归一化]
    D --> E[概率分布]

    subgraph 示例
    F["[2.0, 1.0, 0.1]"] --> G["[7.4, 2.7, 1.1]"]
    G --> H["sum = 11.2"]
    H --> I["[0.66, 0.24, 0.10]"]
    end
```

## 目的

- 输出总和为1（归一化）
- 输出范围在[0,1]之间
- 保持相对大小关系
- 放大差异（exp函数的性质）

## 为什么叫"soft"max

- 传统max只选最大值（硬选择）
- softmax给每个选项一个概率（软选择）
- 最大值获得最大概率，但其他值也有概率



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/交叉熵.md]
===========================================================

---
draw:
title: 交叉熵
date created: 2025-01-24
date modified: 2025-02-14
---

交叉熵就像是一个评分系统：

- 当你的预测完全正确时，得到最低分（0）
- 预测越偏离真实情况，得分越高
- 这个分数帮助模型知道自己预测得有多准确，从而不断改进

可以参考下这个：为什么交叉熵（cross-entropy）可以用于计算代价？- 灵剑的回答 - 知乎  
https://www.zhihu.com/question/65288314/answer/849294209

让我用一个简单的可视化来解释交叉熵：

让我重新解释一下交叉熵的概念：

1. 基本定义：
交叉熵 = -Σ (真实概率 × log(预测概率))

2. 直观理解：
- 它衡量两个概率分布之间的"距离"或"差异"
- 值越小表示预测越准确
- 值越大表示预测越偏离真实情况

1. 举个例子：
假设我们有一个三分类问题：

```Java
真实分布：[1, 0, 0]（第一类）

情况1 - 好的预测：
预测分布：[0.9, 0.05, 0.05]
交叉熵较小

情况2 - 差的预测：
预测分布：[0.3, 0.3, 0.4]
交叉熵较大
```

1. 为什么使用交叉熵：
- 能很好地反映预测的"惩罚程度"
- 预测越错误，损失越大
- 数学性质好，便于优化
- 适合概率分布的比较

1. 在 word2vec 中的应用：
- 用于衡量模型预测的词概率分布与真实分布的差异
- 帮助模型学习更好的词向量表示
- 作为损失函数指导模型训练



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/依存句法分析.md]
==============================================================

---
draw:
tags: []
title: 依存句法分析
date created: 2025-01-29
date modified: 2025-01-29
---

下面用一个**具体的例句**来演示"**转移系统 (Transition-Based) 依存解析**"时，栈 (stack) 与缓冲区 (buffer) 的状态是如何一步步变化的。我们选一个简单句子：

> **句子：** He read a book

并在句首加一个虚拟根 (ROOT) 记作 *w0*，方便最终依存树只有一个唯一根。
因此我们的词序列是：

$$
w_0 = \text{ROOT},\quad w_1 = \text{He},\quad w_2 = \text{read},\quad w_3 = \text{a},\quad w_4 = \text{book}.
$$

在"Arc-Standard" 体系下，常见的三种动作是：

1. **SHIFT**  
   - 将缓冲区 (buffer) 最前面的单词移到栈 (stack) 顶部。
2. **LEFT-ARC(r)**  
   - 在栈顶记作 $(w_j)$，其下方记作 $(w_i)$。建立"$(w_j)$ ←(r)—$(w_i)$"的依存关系（即 $(w_j)$ 是父节点，$(w_i)$ 是子节点），然后**弹出 $(w_i)$**。
3. **RIGHT-ARC(r)**  
   - 在栈顶记作 $(w_j)$，其下方记作 $(w_i)$。建立"$(w_i)$ ←(r)—$(w_j)$"的依存关系（即 $(w_i)$ 是父节点，$(w_j)$ 是子节点），然后**弹出 $(w_j)$**。

> **注意：** 有些资料中对 LEFT-ARC/RIGHT-ARC 的方向描述可能略有差异，核心是"谁是 head，谁是 dependent"以及"移除哪个元素"。这里我们沿用与本示例相一致的版本：**Left-Arc** 去掉下方元素，**Right-Arc** 去掉顶部元素，并在弧 (arc) 中标明谁是父、谁是子。

## 场景设置

- **初始状态**：
    - 栈：只含 `[ROOT]`
    - 缓冲区：`[He, read, a, book]`
    - 弧集 A={}A = \{\}（空）

我们一步步演示 8 次动作（Transition），最终得到完整依存树。

### Step 0：初始状态

- **动作**：无（刚开始）
- **Stack**：ROOTROOT
- **Buffer**：He,read,a,bookHe, read, a, book
- **Arcs**：∅\varnothing

**还没有任何依存关系**，因此如果勉强用"树"来画，就是所有词都彼此独立，还没有边 (arc)：

```Java
Step 0: 

+---------+
| Stack   |   (top在最右)
+---------+
| ROOT    |
+---------+

+------------------+
|   Buffer         |
+------------------+
| He, read, a, book  (front在左)
+------------------+

arcs = {}

(所有词彼此独立, 并无连接)
ROOT    He    read    a    book
```

---

### Step 1：SHIFT

- **动作**：SHIFT
- **效果**：把 "He" 从缓冲区前端移到栈顶
- **Arcs**：仍然是 ∅\varnothing

ASCII "树"依然空空如也，因为 SHIFT 并不会产生依存关系：

```ascii
Step 1: SHIFT

	Stack: [ROOT, He]
	       ^^^^^  ^^^
	       bottom top
	
	Buffer: [read, a, book]
	
	arcs = {}
	
	ROOT    He    read    a    book
	(依然没有任何弧)
```

---

### Step 2：SHIFT

- **动作**：SHIFT
- **效果**：把 "read" 从缓冲区前端移到栈顶
- **Arcs**：仍然是 ∅\varnothing

依然没有新弧：

```java
Step 2: SHIFT

Stack: [ROOT, He, read]
                     top

Buffer: [a, book]

arcs = {}

ROOT    He    read    a    book
```

---

### Step 3：LEFT-ARC(nsubj)

- **动作**：LEFT-ARC(nsubj)
- **栈顶**是 "read"，下方是 "He"  
    => 建立 `(read ←(nsubj)— He)` 并弹出 "He"
- **Arcs**：{(read,nsubj,He)}\{(read, nsubj, He)\}

现在有了第一条弧：**"read" 是父节点，"He" 是它的主语**。
用 ASCII 画"部分树"如下：

```Java
Step 3: LEFT-ARC(nsubj)

Stack: [ROOT, read]
               top

Buffer: [a, book]

arcs = {
    (read, nsubj, He)
}

目前可视化为：

      read
     /
He (nsubj)

(还没与 ROOT、a、book 等连接)
```

---

### Step 4：SHIFT

- **动作**：SHIFT
- **效果**：把 "a" 从缓冲区前端移到栈顶
- **Arcs**：仍是 { (read, nsubj, He) }

部分树不变：

```Java
Step 4: SHIFT

Stack:  [ROOT, read, a]

Buffer: [book]

arcs = {
    (read, nsubj, He)
}

      read
     /
He (nsubj)

剩余词: ROOT, a, book (无连接)
```

---

### Step 5：SHIFT

- **动作**：SHIFT
- **效果**：把 "book" 从缓冲区前端移到栈顶
- **Arcs**：仍是 { (read, nsubj, He) }

依然没有新的弧：

```Java
Step 5: SHIFT

Stack:  [ROOT, read, book]

Buffer: []

arcs = {
    (read, nsubj, He)
}

      read
     /
He (nsubj)

剩余词: ROOT, a, book (无连接)
```

---

### Step 6：LEFT-ARC(det)

- **动作**：LEFT-ARC(det)
- **栈顶**是 "book"，下方是 "a" => 建立 `(book ←(det)— a)` 并弹出 "a"
- **Arcs**：{ (read, nsubj, He), (book, det, a) }

现在产生第二条弧：**"book" 是父节点，"a" 是修饰它的限定词 (det)**。
所以我们有两棵小树：

```Java
Step 6: LEFT-ARC(det)

Stack: [ROOT, read]

Buffer: []

arcs = {
    (read, nsubj, He),
    (book, det, a)
}

部分树1：
      read
     /
He (nsubj)

部分树2：
    book
   /
a (det)

( 还未与 ROOT 相连 )
```

---

### Step 7：RIGHT-ARC(obj)

- **动作**：RIGHT-ARC(obj)
- **栈顶**是 "book"，下方是 "read" => 建立 `(read ←(obj)— book)` 并弹出 "book"
- **Arcs**：{ (read, nsubj, He), (book, det, a), (read, obj, book) }

现在我们把"read" 和"book"连起来，意味着**"read" 是父节点，"book" 是它的宾语 (obj)**。于是两棵小树合二为一：

```Java
Step 7: RIGHT-ARC(obj)

Stack: [ROOT, read]

Buffer: []

arcs = {
    (read, nsubj, He),
    (book, det, a),
    (read, obj, book)
}

合并树：
        read
       /    \
(nsubj)/      \(obj)
     He       book
               /
             a (det)

尚未和 ROOT 相连
```

---

### Step 8：RIGHT-ARC(root)

- **动作**：RIGHT-ARC(root)
- **栈顶**是 "read"，下方是 "ROOT" => 建立 `(ROOT ←(root)— read)` 并弹出 "read"
- **Arcs**：{ (ROOT, root, read),  
    (read, nsubj, He),  
    (book, det, a),  
    (read, obj, book) }

这一步把"ROOT"与"read"连起来，表示"read"是全句的根 (root)。
**最终**树形结构：

```Java
Step 8: RIGHT-ARC(root)

Stack: [ROOT]

Buffer: []

arcs = {
    (ROOT, root, read),
    (read, nsubj, He),
    (read, obj, book),
    (book, det, a)
}

完整依存树 (包含 ROOT) 如下：

   ROOT
     \
     (root)
      read
     /    \
(nsubj)    (obj)
   He       book
            /
         a (det)
```

很多时候我们在可视化时会把 `ROOT` 省略，直接把 "read" 当成可视化的**最高节点**：

```Java
        read (root)
       /       \
(nsubj)He     (obj)book
               /
             a (det)
```

这样就是一个标准的依存树：

- 谓语动词 "read" 在根位置；
- "He" 是主语 (nsubj)；
- "book" 是宾语 (obj)；
- "a" 依存于 "book" (det)。

---

## 总结

通过上面**每一步都给出 arcs 和对应的"小树"**，可以清晰地看到：

1. **初始 (Arcs = {}) 时**：所有单词都"各自独立"。
2. **每个 SHIFT** 只是在栈与缓冲区之间移动单词，并不产生弧。
3. **每个 LEFT-ARC/RIGHT-ARC** 会在 `Arcs` 里多加入一个依存关系，并从栈中弹出相应单词。
4. **最终** 当缓冲区为空、栈只剩 `ROOT`，依存关系 `Arcs` 中就包含整句依存树。

这样就实现了**从无到有**、**一步步**添加依存弧的"转移式依存解析"过程。

## 完整依存关系

最终得到的依存关系集合为：

```Java
{
  (ROOT, root, read),    # read 是整句核心动词
  (read, nsubj, He),     # read 的主语是 He
  (read, obj, book),     # read 的宾语是 book
  (book, det, a)         # book 的修饰语是冠词 a
}
```

如果把这些关系画成依存树（简写省略 w0、w1 等编号），可以这样理解：

```Java
    ROOT
     ↓ (root)
    read ------------> He
      (nsubj)         
      \
       \ (obj)
       book ---> a
         (det)
```

或更常见的写法是把 `ROOT` 省去，只说 `read` 是根节点：

```Java
         read (root)
         /   \
(nsubj) /     \ (obj)
       He     book
              |
             (det)
              a
```

> 由此可见，我们**一步步**地用 SHIFT / LEFT-ARC / RIGHT-ARC 这些动作，配合栈与缓冲区的状态变化，最终构建出该句子的依存树。

---

### 小结

- **初始**：栈只有 `ROOT`，缓冲区里是句子所有单词，弧集为空。
- **每步**：根据当前栈顶、次顶、缓冲区等上下文信息，选择一个动作（如 SHIFT 或 LEFT-ARC/RIGHT-ARC + 关系标签）。
- **结束**：当缓冲区清空、栈只剩 `ROOT` 时，弧集里就包含了完整的依存关系，解析完成。

在神经网络实现中，系统会对当前状态 (σ,β,A)(\sigma, \beta, A) 提取特征（单词向量、POS、已建关系等），用模型来**预测**最优动作（transition）和依存关系标签，从而实现自动、贪心地构建依存树。上述 ASCII 步骤演示的正是"转移式依存解析"的核心逻辑。

### 神经网络

- **输入特征改用嵌入向量** (embedding)。
    - 例如，把当前栈顶 2-3 个单词、缓冲区前面 2-3 个单词等，都变成对应的词向量。
    - 同时可能还会把它们的词性、已经建立好的依存标签等也映射成 embedding。
    - 把所有这些向量**拼接 (concatenate)** 在一起，作为神经网络的输入。
- **神经网络计算**
    - 常见做法：输入层 → 若干隐藏层 (多层感知器 MLP) → 输出层 (softmax)。
    - 输出层会给出对三个动作 (Shift / Left-Arc / Right-Arc) 以及其对应依存标签的概率分布。
    - 训练时，用交叉熵损失 (cross entropy) + 反向传播 (backpropagation) 来更新网络参数，包括嵌入矩阵和全连接层。
- **贪心解码**
    - 在真实解析时，我们从初始状态开始，**每一步都选置信度最高的动作**，直到到达终止状态，从而得到依存树。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/偏导数.md]
===========================================================

---
draw:
tags: []
title: 偏导数
date created: 2025-01-24
date modified: 2025-01-24
---

让我用一个山地地形的例子来解释偏导数：

想象你站在一座山上的某个位置（点P），你想知道这个位置的陡峭程度。你可以从两个方向来测量：

1. X方向的偏导数（红色路径）：
- 这就像你沿着等高线水平移动
- 只考虑向东或向西走时地形的变化
- 相当于固定y坐标，只让x变化时的斜率

1. Y方向的偏导数（蓝色路径）：
- 这就像你直接向山上或山下走
- 只考虑向南或向北走时地形的变化
- 相当于固定x坐标，只让y变化时的斜率

关键理解：

1. 偏导数就是在某一点，当你只沿着一个方向移动时地形的陡峭程度
2. 在同一个点P，不同方向的偏导数可能完全不同
3. 每个点都有两个偏导数（对于二元函数），分别描述两个方向上的变化率

生活中的例子：

- 就像在游乐场的滑梯上，你可以：
  - 顺着滑梯方向滑下去（一个方向的偏导数很大）
  - 横着走（另一个方向的偏导数接近于零）

这就是为什么我们说偏导数是"偏的"——因为它只关注一个方向上的变化率，而忽略其他方向的变化。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/共现矩阵.md]
============================================================

---
draw:
tags: []
title: 共现矩阵
date created: 2025-01-23
date modified: 2025-01-24
---

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2025%2F01%2F23%2F23-51-26-fa9ce9e797c46268061f112c551e302e-202501232351103-bd79d0.png)  
这个值为1是因为在上方显示的英汉混合句子中 all 和 that 这两个词是相邻的。

在这个位置标记为1表示这两个词在文本中作为相邻词出现了一次

## 窗口大小

不是的,让我来澄清一下窗口大小的定义:

当我们说窗口大小(window size) n=1 时:

- 只看当前词左边1个词和右边1个词
- 所以整个窗口大小实际是2(不包含中心词本身)

例如对于句子 "all that glitters is not gold":

当 n=1 时,对中心词 "glitters" 的扫描是:

```Java
... [that] glitters [is] ...
    ↑     ↑中心词    ↑
    左1          右1
```

当 n=2 时,对中心词 "glitters" 的扫描是:

```Java
[all] [that] glitters [is] [not] ...
  ↑     ↑      ↑中心词   ↑     ↑
  左2    左1         右1    右2
```

所以:

- 窗口大小 n 表示单侧看的词数
- 总的上下文窗口长度 = 2n (两侧各n个词)
- 不计算中心词本身

这就是为什么在共现矩阵的例子中,当 n=1 时:

- that 只和 all、glitters 有共现关系
- is 只和 glitters、not 有共现关系
- 更远的词不计入共现统计

希望这个解释能帮助你更好地理解窗口大小的概念。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/函数图像.md]
============================================================

---
draw:
tags: []
title: 函数图像
date created: 2025-01-24
date modified: 2025-01-24
---

https://www.desmos.com/calculator



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/复合函数求导.md]
==============================================================

---
aliases: [3. 为什么在神经网络中很重要？]
draw:
tags: []
title: 3. 为什么在神经网络中很重要？
date created: 2025-01-25
date modified: 2025-01-25
linter-yaml-title-alias: 3. 为什么在神经网络中很重要？
---

## 1. 单变量情形

### 1.1 公式形式

最常见、最基础的**链式法则**公式是针对**单变量**函数的复合：
$y = f\bigl(g(x)\bigr)$  
要对 $y$ 关于 $x$ 求导数，即 $\frac{dy}{dx}$，**链式法则**告诉我们：
$\frac{dy}{dx} = f'\bigl(g(x)\bigr) \times g'(x)$

含义是：

> 当你把$x$ 先输入到 $g$ 函数里，再把 $g(x)$ 的结果输入到 $f$ 函数里，如果你想知道 **$x$ 的微小变化对输出 $y$ 有什么影响**，可以把它拆分为"$g$ 对 $x$ 的影响" 和 "$f$ 对 $g$ 的影响" 两部分，再把它们**相乘**。

### 1.2 示例 1：$y=(x^2+1)^3$

这里可以把  
$\underbrace{g(x)}_{\text{内函数}} = x^2 + 1, \quad \underbrace{f(u)}_{\text{外函数}} = u^3 \quad (\text{其中 }u=g(x))$

于是  
$y = f\bigl(g(x)\bigr) = (x^2 + 1)^3$

- **先求** $g'(x)$:  
  $g'(x) = \frac{d}{dx}(x^2 + 1) = 2x$
- **再求** $f'(u)$:  
  $f'(u) = \frac{d}{du}(u^3) = 3u^2$  
  在具体求导时，要把 $u$ 替换成 $g(x)$，也就是 $x^2+1$。

**最后用链式法则**：
$\frac{dy}{dx} = f'(g(x)) \cdot g'(x) = 3 \,\bigl(x^2 + 1\bigr)^2 \,\times\, 2x = 6x\,\bigl(x^2 + 1\bigr)^2$

### 1.3 示例 2：$y=\sqrt{1 + x^2}$

可以写成  
$y = (1 + x^2)^{\tfrac12}$  
看作  
$g(x) = 1 + x^2,\quad f(u) = u^{\tfrac12}$

- $g'(x) = 2x$  
- $f'(u) = \frac12 u^{-\tfrac12} = \frac{1}{2\sqrt{u}}$

所以  
$\frac{dy}{dx} = \frac{1}{2\sqrt{g(x)}} \,\times\, 2x = \frac{2x}{2\sqrt{1 + x^2}} = \frac{x}{\sqrt{1 + x^2}}$

这个例子也常见于导数的练习题。

## 2. 多变量情形

在神经网络里（以及更普遍的场景），我们往往遇到**多变量**的复合函数。比如：
$z = f\bigl(u,v\bigr)$ 而  $u = g(x,y), \quad v = h(x,y)$  

这个时候，如果要对 $z$ 关于 $x$ 求偏导数，就要用到**多元链式法则**：
$\frac{\partial z}{\partial x} = \frac{\partial f}{\partial u}\cdot\frac{\partial u}{\partial x} + \frac{\partial f}{\partial v}\cdot\frac{\partial v}{\partial x}$

> 这是因为 $z$ 既会随 $u$ 的变化而变化，也会随 $v$ 的变化而变化；而 $u$ 和 $v$ 又都依赖于 $x$。

## 2.1 示例：$z=f(g(x,y), h(x,y))$

让我们构造一个稍微具体一点的例子：
$z = (u + v)^2, \quad u = xy, \quad v = x + y$

你可以认为  
$f(u,v) = (u+v)^2$  
$g(x,y)=x y,\quad h(x,y)=x + y$

要对 $z$ 关于 $x$ 和 $y$ 分别求偏导，就得同时考虑 $u$ 和 $v$ 对 $x$ 的影响。

1. **$\frac{\partial z}{\partial x}$**:
   - 先算 $\frac{\partial f}{\partial u} = \frac{\partial}{\partial u} (u+v)^2 = 2(u+v)$
   - 再算 $\frac{\partial u}{\partial x} = \frac{\partial}{\partial x}(xy) = y$
   - 还有 $\frac{\partial f}{\partial v} = 2(u+v)$（同理，因为对 $v$ 的偏导也一样）
   - 再算 $\frac{\partial v}{\partial x} = \frac{\partial}{\partial x}(x + y) = 1$

   由多元链式法则：
   $\frac{\partial z}{\partial x} = \underbrace{2(u+v)}_{\frac{\partial f}{\partial u}}\times \underbrace{y}_{\frac{\partial u}{\partial x}} + \underbrace{2(u+v)}_{\frac{\partial f}{\partial v}}\times \underbrace{1}_{\frac{\partial v}{\partial x}} = 2(u+v)\cdot y + 2(u+v)\cdot 1 = 2(u+v)(y + 1)$  
   最后再把 $u=xy, v=x+y$ 带回去：
   $\frac{\partial z}{\partial x} = 2\bigl(xy + x + y\bigr)\,(y + 1)$

2. **$\frac{\partial z}{\partial y}$**:  
   可以做类似步骤：
   - $\frac{\partial u}{\partial y} = x$
   - $\frac{\partial v}{\partial y} = 1$  
   $\frac{\partial z}{\partial y} = 2(u+v)\cdot x + 2(u+v)\cdot 1 = 2(u+v)(x + 1)$  
   再替换 $u,v$:  
   $\frac{\partial z}{\partial y} = 2(xy + x + y)\,(x + 1)$

这就是"多元链式法则"在一个小示例中的运用。

## 3. 为什么在神经网络中很重要？

在神经网络里，常见情形是：  
$\hat{y} = a^{(L)}$ 而 $a^{(\ell)} = f^{(\ell)}\bigl(W^{(\ell)}\,a^{(\ell-1)} + b^{(\ell)}\bigr)$  
其中 $\ell = 1,2,\dots,L$ 表示不同层级；$a^{(\ell-1)}$ 表示上一层的输出等。

如果再加上一个损失函数 $J(y, \hat{y})$，想要求 $\frac{\partial J}{\partial W^{(\ell)}}$ 或 $\frac{\partial J}{\partial b^{(\ell)}}$，就需要**不断应用链式法则**，才能把损失 $J$ 一直"传回"到某个权重/偏置上。具体展开时，你会看到我们一层层反推，所以叫**"反向传播"**(backpropagation)。但**底层逻辑**就是：对多层复合函数求导时，一次次用到链式法则**。

## 4. 总结与建议

1. **单变量链式法则**是最基础的：$\frac{d}{dx}f(g(x))=f'(g(x))g'(x)$。先从这些经典例子（$(x^2+1)^3$、$\sqrt{1+x^2}$等）入手，把它记熟并理解透。
2. **多元复合函数**时，要先搞清楚哪个变量依赖于谁，再用"对每个中间量的偏导相乘，然后相加"的方式来求最终对 $x$ 或者 $y$ 的偏导。这就是**多元链式法则**。
3. **在神经网络中**，它会体现为：
   - 输出层对损失的偏导 $\times$（输出层激活函数的导数）
   - $\times$（输出层对上一层输出的依赖）
   - $\times$（上一层激活函数的导数）
   - … 一直乘到你所关心的某个权重或偏置的那一层。

> 所以，**掌握链式法则的本质**（"复合函数求导时，**分段求导再相乘**"）就足以支撑你理解反向传播中那看似复杂的一连串乘积运算。

### 参考学习顺序

1. **复习高中或大学微积分**里关于复合函数求导、偏导数、梯度的章节。
2. 做一些简单单变量、多变量例题，确保对**链式法则**和**偏导**都熟练。
3. 再回到**神经网络**：把每层输出当成一个中间变量，在纸上清晰地写出"这一层对上一层的依赖关系"，然后用链式法则"连乘"求导。

这样一来，你就能慢慢理解：反向传播其实就是在做一大串链式法则的乘法运算。尤其是看到公式里"$\sigma'(z)$"、"$(a^{(2)} - y)$" 等等，都只是一小块一小块的局部导数。每一层计算完之后，再往前一层乘，这就是**"反向"**思想的由来，也是为什么它能高效地计算深层网络的梯度。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/差分.md]
==========================================================

---
draw:
tags: []
title: 差分
date created: 2025-01-26
date modified: 2025-01-26
---

让我为您介绍大学微积分中关于差分的重要知识：

差分是微积分中的一个重要概念,主要研究函数在离散点上的变化。以下是关键知识点：

1. 差分的基本概念
- 设函数y=f(x)，当自变量x有一个增量Δx时，函数的相应增量Δy=f(x+Δx)-f(x)称为函数的差分，记作Δy或Δf(x)
- 差分可以看作是连续变化的离散化表示

1. 差分的基本运算法则
- 两函数和的差分等于两个函数差分的和：Δ(u±v) = Δu±Δv
- 常数与函数的积的差分：Δ(cu) = cΔu (c为常数)
- 两函数积的差分：Δ(uv) = uΔv + vΔu + ΔuΔv

1. 差分与导数的关系
- 当Δx很小时，Δy/Δx近似等于导数dy/dx
- 差分可以看作是导数的离散形式
- 差商的极限就是导数：lim(Δx→0)Δy/Δx = dy/dx

1. 高阶差分
- 一阶差分的差分称为二阶差分，记作Δ²y
- 二阶差分的差分称为三阶差分，记作Δ³y
- 以此类推可得到n阶差分Δⁿy

1. 差分在实际应用中的意义
- 在数值分析中用于近似计算导数
- 在数值解方程中用于离散化微分方程
- 在数据分析中用于研究数据的变化趋势

1. 差分方程
- 是描述离散系统的重要工具
- 类似于微分方程描述连续系统
- 在经济学、人口统计等领域有广泛应用

希望这些内容对您理解差分概念有所帮助。如果您对某个具体部分想深入了解，我很乐意为您详细解释。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/强独立性假设.md]
==============================================================

---
draw:
tags: []
title: 强独立性假设
date created: 2025-01-25
date modified: 2025-01-25
---

强独立性假设在统计机器翻译中指的是："句子中的每个中文词 xj 只依赖它所对齐的英文词 yaj"。让我用具体例子解释这个概念：

假设有一个中文句子和它的英文翻译：

```Java
中文：我 喜欢 吃 苹果
英文：I  like  to eat apples
```

不做强独立性假设时：

- 每个中文词的翻译可能依赖于整个英文句子的上下文
- 翻译概率需要考虑所有词之间的相互关系
- 计算复杂度非常高

做了强独立性假设后：

- "我" 只依赖于它对齐的英文词 "I"
- "喜欢" 只依赖于它对齐的英文词 "like"
- "吃" 只依赖于它对齐的英文词 "eat"
- "苹果" 只依赖于它对齐的英文词 "apples"

这样的好处是：

1. 计算变得简单，可以把句子的翻译概率分解成各个词对的概率乘积
2. 可以使用动态规划等算法高效求解
3. 大大降低了模型的复杂度

缺点是：

1. 忽略了更广泛的上下文信息
2. 可能错过一些需要考虑整体语境的翻译
3. 对于一些需要整句理解的习语翻译效果不好

这也是为什么后来的神经机器翻译(NMT)模型更受欢迎，因为它不需要这种强假设，可以自然地考虑整个句子的上下文。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/损失函数.md]
============================================================

---
draw:
tags: []
title: 损失函数
date created: 2025-01-24
date modified: 2025-01-24
---



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/最大似然估计.md]
==============================================================

---
draw:
tags: []
title: 最大似然估计
date created: 2025-01-24
date modified: 2025-01-24
---

好的，我来用形象的方式解释一下什么是最大似然估计（Maximum Likelihood Estimation，MLE）。

**打个比方：**

想象一下，你和朋友在玩一个猜硬币的游戏。你朋友拿出一个硬币，你不知道这个硬币是不是均匀的（也就是正反面出现的概率是不是都是50%）。你们抛了几次硬币，结果是：

- 抛了10次
- 其中7次是正面朝上
- 3次是反面朝上

现在，你需要根据这个结果来猜测：这个硬币是正面朝上的概率（我们用 θ 来表示）。

**最大似然估计的思想就是：**

既然我们观测到了"7次正面，3次反面"这个结果，那么我们就应该认为，使得这个结果最有可能发生的 θ 值，就是最合理的猜测。
也就是说，我们要找到一个 θ 值，使得"在 θ 的情况下，抛10次硬币出现7次正面"的概率最大。

**具体怎么做呢？**

我们可以用一个公式来表示"在 θ 的情况下，抛10次硬币出现7次正面"的概率，这个公式叫做**似然函数**：
L(θ) = C(10, 7) * θ^7 * (1-θ)^3

其中：

- C(10, 7) 是组合数，表示从10次中选7次的组合方式。
- θ^7 表示正面朝上的概率的7次方。
- (1-θ)^3 表示反面朝上的概率的3次方。

我们的目标是找到一个 θ 值，使得 L(θ) 最大。

**怎么找到最大值呢？**

通常我们会对似然函数取对数（因为对数函数是单调递增的，所以最大值的位置不变），然后求导，令导数为0，解出 θ。
在这个例子中，通过计算可以得出，当 θ = 0.7 时，L(θ) 最大。

**结论：**

根据最大似然估计，我们猜测这个硬币正面朝上的概率是 0.7，也就是70%。

**总结一下最大似然估计的步骤：**

1. **确定模型：** 也就是知道数据服从什么分布（比如上面的例子中，我们假设每次抛硬币是独立的伯努利分布）。
2. **写出似然函数：** 用公式表示在给定参数下，观测数据出现的概率。
3. **求解最大值：** 通常通过取对数、求导等方法找到使似然函数最大的参数值。

**最大似然估计的用途：**

最大似然估计是统计学中一种常用的[[参数估计]]方法，广泛应用于机器学习、信号处理、生物统计等领域。比如：

- 在机器学习中，可以用来估计模型中的参数，比如线性回归、逻辑回归等。
- 在信号处理中，可以用来估计信号的频率、幅度等。

希望通过这个例子，你能够对最大似然估计有一个更形象的理解。

**补充说明：**

- 最大似然估计是一种**频率学派**的估计方法，它认为参数是固定的，只是我们不知道它的值。
- 与最大似然估计相对的是**贝叶斯估计**，它认为参数也是随机变量，服从一定的分布。

希望以上解释能够帮助你理解最大似然估计。如果还有其他问题，欢迎继续提问。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/条件概率.md]
============================================================

---
draw:
tags: []
title: 条件概率
date created: 2025-01-24
date modified: 2025-01-24
---

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2025%2F01%2F24%2F01-38-24-0d7233418f5d873a54d3369dd2057608-202501240138792-0a950b.png)

"在给定中心词是c的条件下，它周围的上下文词是o的概率"



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/注意力机制.md]
=============================================================

---
draw:
tags: []
title: 注意力机制
date created: 2025-01-29
date modified: 2025-01-29
---

Seq2Seq and Attention Models

Thought for 3m 35s

下面是一份分步骤、通俗易懂的讲解，帮助你理解 CS224n 第六部分（"神经机器翻译、Seq2Seq 与注意力机制"）的主要内容。我会结合一些例子，尽量让概念更加清晰。

---

## 一、背景与动机

### 1. 为什么要研究"序列到序列"（Seq2Seq）模型？

在之前的 NLP 任务中（如命名实体识别、语言模型预测下一个词等），我们大多只需要输出一个单独的标签或词。然而在某些任务中，我们的输出本身也是一个长度不固定的序列，比如：

- **机器翻译**：输入是某种语言的句子，输出是另一种语言的句子。
- **对话系统**：根据上一句话，输出一个回答或回应。
- **文本摘要**：输入可能是长篇文章，输出是简短摘要。

这些任务需要"从一个序列映射到另一个序列"，因此就有了 Seq2Seq 模型。

### 2. 传统机器翻译的历史方法

在深度学习方法普及之前，机器翻译多用**统计机器翻译（SMT）**，比如基于 n-gram 语言模型、词/短语对齐、翻译规则等。然而它们常常无法很好地捕捉长距离依赖或复杂的句子结构，开发和维护也比较复杂。2014 年，基于神经网络的**Seq2Seq** 翻译模型崛起，迅速成为主流。

---

## 二、Seq2Seq 基本原理

### 1. 编码器（Encoder）

Seq2Seq 的编码器主要将输入序列（例如英文句子）"编码"成一个固定长度的向量（也叫"上下文向量" context vector）。这个向量可以被视为对整个输入序列含义的压缩表达。

- **操作方式**：通常使用 RNN（如 LSTM/GRU）把输入序列 {x1,x2,…,xn}\{x_1, x_2, \dots, x_n\}{x1​,x2​,…,xn​} 依次读进去，得到一系列隐藏状态 {h1,h2,…,hn}\{h_1, h_2, \dots, h_n\}{h1​,h2​,…,hn​}，最后一个隐藏状态（或最后一层的隐藏状态）就是我们所说的上下文向量 CCC。
- **前后双向（Bidirectional）**：为了更好地捕捉上下文，通常会用双向 LSTM，让编码器能同时读"正向"和"反向"的句子，再把它们的隐藏状态拼接起来，以获得更丰富的表征。
- **反转输入**：有些论文（如最初的 Seq2Seq）会把输入序列反转后再喂给编码器，目的是让编码器最后一个看到的词，大致对应目标句子开头需要翻译的地方，从而帮助解码器"更方便地开始翻译"（但在实际实现中，也可用双向 RNN，不一定非要反转输入）。

简单示例：

- 英文句子 "What is your name?"
    - 先把每个词转换为词向量（Embedding），再依次输入 LSTM。
    - 最后得到一个隐藏状态向量 CCC，可被视为"整个句子的大意"。

### 2. 解码器（Decoder）

解码器的任务就是根据这个上下文向量 CCC，一步步生成目标序列（比如翻译成法语句子）。

- **初始化**：用编码器最终的隐藏状态来初始化解码器的第一个隐藏状态。这样"输入句子的语义信息"就能传到解码器里。
- **生成序列**：解码器通常也是一个 RNN（LSTM/GRU）。它会把上一步的输出词（或者在第一步时用特殊的 ⟨GO⟩\langle GO\rangle⟨GO⟩ 起始符）作为输入，再结合当前隐藏状态，输出下一个词的概率分布。选择概率最高的那个词（或者在训练时用教师强制），作为输出。
- **迭代**：将刚输出的词再次送入解码器，持续进行，直到生成结束符 ⟨EOS⟩\langle EOS\rangle⟨EOS⟩ 或达到设定的最大长度。

例如：

- 已得到的译文前半句是 "Comment t'". 下一步解码器会把 "t'" 作为输入，加上隐状态来预测下一个法语单词是 "appelles"，然后继续下一步，直到生成完整句子 "Comment t'appelles tu?"

---

## 三、注意力机制（Attention）

### 1. 为什么需要注意力？

让编码器把整个输入句子"全部信息"都压缩到一个定长向量，其实很难，尤其当句子很长时，信息可能会丢失或难以利用。**注意力机制（Attention）** 的核心思想就是：当解码器要生成目标序列的第 iii 个词时，它可以"动态地去看"输入句子的所有隐藏状态，根据需要"关注"不同部分，而不是只依赖一个全局向量。

### 2. Bahdanau 注意力（举例说明原理）

- **计算对齐（alignment）分数**：对于解码器当前时刻 iii 的隐藏状态 si−1s_{i-1}si−1​，以及输入句子中每个位置 jjj 的编码器隐藏状态 hjh_jhj​，我们定义一个打分函数 ei,j=a(si−1,hj)e_{i,j} = a(s_{i-1}, h_j)ei,j​=a(si−1​,hj​) 它可以是一个小的前馈神经网络输出一个标量分数。
- **归一化得到注意力分布**：对每个 jjj 做 Softmax，得到 αi,j\alpha_{i,j}αi,j​，就像"对齐概率"或"注意力权重"。
- **计算上下文向量**：把输入序列所有隐藏状态加权求和：ci=∑jαi,jhjc_i = \sum_j \alpha_{i,j} h_jci​=j∑​αi,j​hj​
- **将上下文和解码器隐藏状态结合**：最后得到新的解码器隐藏向量，用来预测第 iii 个输出词。

这样一来，解码器在生成每个词时，都能对源句子的不同位置进行"聚焦"，比如在翻译英语"your name"时，注意力更集中在输入的"your name"相关的隐藏状态上。

### 3. Attention 带来的好处

- **更好地处理长句子**：不必把所有信息塞到一个向量里，模型在解码时可以灵活地"查看"输入句子的各个部分。
- **翻译质量更高**：网络能学到类似"对齐"关系。例如英语单词"name"往往对应法语单词"nom"等。
- **可视化可解释性**：我们可以把 αi,j\alpha_{i,j}αi,j​ 的矩阵用热力图画出来，就能看出翻译过程中的对齐关系（对研究很有帮助）。

3blue的transformer视频

Transformer 模型 Q 由 Vaswani 等人在 2017 年提出，最初用于机器翻译任务。与传统的 RNN（循环神经网络）和 LSTM（长短期记忆网络）不同，Transformer 完全基于自注意力机制（Self-Attention Mechanism）实现，并行处理能力更强，训练速度更快。Transformer 的出现彻底改变了 NLP 领域，使得任务的性能显著提升。

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F22%2F03-55-58-dd276138c27da251f8022778663c33e9-202409220355685-852643.png)

## 自注意力机制

以 d_model=1 为例

```Java
## Q K V
Q 是问题，K 是 key，V 是 value
三者相乘得到 Q 的回答 A
自注意力就是 Q、K、V 是相同的

## 输入
我: [1]
爱: [2]
你: [3]

## 注意力分数(Q × K^T)

[我对我的关注度, 我对爱的关注度, 我对你的关注度]
[爱对我的关注度, 爱对爱的关注度, 爱对你的关注度]
[你对我的关注度, 你对爱的关注度, 你对你的关注度]
    softmax处理
[0.6, 0.2, 0.2]
[0.2, 0.6, 0.2]
[0.2, 0.2, 0.6]


## 与V相乘得到输出

[0.6, 0.2, 0.2]   [1]     [1.4]  我视角里的我爱你
[0.2, 0.6, 0.2] × [2] =   [1.8]  爱视角里的我爱你
[0.2, 0.2, 0.6]   [3]     [2.2]  你视角里的我爱你

通过这种方式映射以后，每个词从自我为中心的基础上稍微关注窗口内其他词，用每个词的视角去解读了一遍整个句子，输入是n个词，那么输出就是n个视角里的这句话

```

上面的部分『我爱你』只是变成了序列无关的embedding，但是现实中「我爱你」和「你爱我」其实是不同的，所以在 embedding 的时候还要加入位置编码以反应单词出现顺序的信息

知名大学教授

[@ProfTomYeh](https://x.com/ProfTomYeh)  
 并重新计算查看变化！

千万不要搁那研究 k 是建值，q 是查询，v 是值，如果你看到这种讲解，基本就别看了，那作者自己也没搞明白。

信我一句，把 transformer 和 [GNN](GNN.md)，[GCN](GCN) 放在一起学，你会看到更加本质的东西。

这样你就能理解位置嵌入，不管是正弦还是可学习的嵌入，不管是时间嵌入还是其他先验嵌入。
进而理解什么 [autoformer](https://www.zhihu.com/search?q=autoformer&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，ltransformer，[itransformer](https://www.zhihu.com/search?q=itransformer&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，graphformer，这样你就会看到 [transformer](https://www.zhihu.com/search?q=transformer&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 在多元时序和图上的应用（二者本就一样）

然后你就能明白只要改动注意力计算的方式就能造一个新的 transformer，至于 [多头](https://www.zhihu.com/search?q=%E5%A4%9A%E5%A4%B4&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 和单头，就非常容易理解。而至于什么 [多模态cross attention](https://www.zhihu.com/search?q=%E5%A4%9A%E6%A8%A1%E6%80%81cross%20attention&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，那也就更加显而易见了。
而 [残差](https://www.zhihu.com/search?q=%E6%AE%8B%E5%B7%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 和 norm 只是模型的一种技巧，虽然是小技巧，但实际很有用。
那个 [ffn](https://www.zhihu.com/search?q=ffn&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，则更是不值一提。你就算用 [CNN](https://www.zhihu.com/search?q=CNN&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 去平替，在小问题上也毫无压力。
而至于在 [cv](https://www.zhihu.com/search?q=cv&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 上的使用，其实就是变着法把图像信息变成 [token序列](https://www.zhihu.com/search?q=token%E5%BA%8F%E5%88%97&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)。

后续的自监督方式，则是另一个内容了。
从可解释性上可以试着考虑一下流形，信息论，
encoder 和 decoder 看着玄学，实则简单，建议多往 [流形](https://www.zhihu.com/search?q=%E6%B5%81%E5%BD%A2&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 和信息论上靠一靠。
如果发顶会了记得带我一个名字。

区别于 [Transformers](Transformers.md)



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/激活函数.md]
============================================================

---
draw:
title: 激活函数
date created: 2025-02-03
date modified: 2025-02-06
---

[Sigmoid函数](Sigmoid函数.md)

## ReLU

ReLU(Rectified Linear Unit)的数学表达式为:

$\text{ReLU}(x) = \max(0, x)$

这意味着:

- 当 $x < 0$ 时,$\text{ReLU}(x) = 0$;
- 当 $x \ge 0$ 时,$\text{ReLU}(x) = x$。

图像上看,ReLU 函数的形状是这样的:

- 对于负数部分($x < 0$),输出一直为 0,即图像在 $y = 0$ 处是一条水平线。
- 当 $x$ 到达 0 后,函数开始沿着直线 $y = x$ 向上延伸,也就是说对于正数部分,图像是一条斜率为 1 的直线。

简单示意图如下:

```Java
         |
       4 |           /
         |          /
       2 |         /
         |        /
       0 |--------•-------->
         |       
      -2 |       
         |
```

- 横轴表示 $x$;
- 纵轴表示 $\text{ReLU}(x)$;
- 当 $x<0$ 时,输出为 0(图中水平线部分);当 $x \ge 0$ 时,输出与 $x$ 相等(图中斜线部分)。

这就是 ReLU 函数的图像形状。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/编码器、解码器.md]
===============================================================

---
draw:
tags: []
title: 编码器、解码器
date created: 2025-01-29
date modified: 2025-01-29
---

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2025%2F01%2F29%2F15-36-04-6cc5db4e52ade5d3677fa7ef3f9a28a6-202501291536533-0e467d.png)

从**计算本质**上看，几乎所有的神经网络确实都可以被视作「线性变换 + 非线性激活」的反复堆叠——也就是你所说的

$h^{(l)} = \sigma\bigl(W^{(l)}h^{(l-1)} + b^{(l)}\bigr)$

**但为何还要谈论"编码器-解码器"这种更高层的结构名称？**它并不仅仅是给几层网络换了个"花哨的名字"，更多是出于**任务需求**和**网络拓扑**的考量。

下面从几个角度说明，为什么"编码器-解码器"的结构并非只是简单的线性+非线性，而是对网络**功能与信息流**的"特殊组织"：

---

## 1. 模块的功能与信息流方向不同

在很多序列到序列 (Seq2Seq) 或复杂预测任务里，我们常常将网络的整体流程划分成"编码 (Encoder)"和"解码 (Decoder)"两大模块：

1. **编码器 (Encoder)**
    
    - 主要负责"读取"输入数据（可能是句子、图像、音频等），逐步将外部信息压缩/编码成内部的表示（隐藏向量）。
    - 例如在 NLP 任务里，编码器通常接收源语言序列 {x1,x2,… }\{x_1, x_2, \dots\}，输出一系列隐藏状态或一个总结向量，表示对整句的理解。
2. **解码器 (Decoder)**
    
    - 接收编码器的输出或上下文向量，再"解码"出目标信息（可能是翻译后的序列、预测标签等）。
    - 在神经机器翻译中，解码器每一步需要基于**当前解码状态**以及**编码器输出**，逐词地生成目标序列 {y1,y2,… }\{y_1, y_2, \dots\}。

如果单纯只看"数学运算"，的确都是矩阵乘法加激活函数；但是在**信息如何流动**、**哪些参数在什么阶段共享/独立**、**如何组织多步生成**等方面，编码器与解码器往往有各自不同的结构和接口。

- 举例：在机器翻译中，编码器要一次性处理整句并得到一个上下文向量 cc，解码器则在每个目标词生成时，都会用到这个 cc（或者结合注意力机制 focus on 源句不同位置）以及上一步预测的词。
- 这种"循环式地生成输出"的流程，和"统一式地将整句编码成向量"的流程，就能形成鲜明的**功能区分**。

---

## 2. 模块化设计带来更灵活的组合与扩展

虽然核心仍是「线性+非线性」，但把网络划分为编码器、解码器这类**功能块**能让我们：

1. **更灵活地组合**不同类型的模块。
    
    - 比如图像到文本描述 (Image Captioning) 里，可将 CNN 作为编码器（处理图像，输出视觉特征），RNN/Transformer 作为解码器（生成描述文本）。
    - 如果我们只把它们看作"第 ll 层、第 l+1l+1 层"，就难以直观地表达不同模块各自的输入输出形式。
2. **易于针对特定功能进行改进**。
    
    - 我们可以对编码器部分做双向 RNN、多层卷积、注意力机制等各种增强，也可以单独改进解码器部分（例如增加 Beam Search 的生成策略）。
    - 这种"**模块化思路**"能让研发与实验更具可控性。
3. **反映在训练和推理过程中的区别**。
    
    - 训练时，编码器和解码器可能会有不同的数据喂法（例如，解码器常常需要训练策略，如 Teacher Forcing，或者结合已生成的词来继续解码）。
    - 推理时，解码器会一边生成输出、一边更新自己的隐藏状态，这和普通全连接层一次性前向计算有显著区别。

---

## 3. "门控结构"或"注意力机制"超越简单的 σ(Wx+b)\sigma(Wx + b)

在 RNN（或 LSTM、GRU）中，每个时间步不再只是

$h_t = \sigma(W h_{t-1} + U x_t + b)$

- $h_t$ 是当前时刻t的隐藏状态
- $h_{t-1}$ 是上一时刻的隐藏状态
- $x_t$ 是当前时刻的输入
- $W$、$U$ 和 $b$ 是可学习的参数
- $\sigma$ 是非线性激活函数

而是引入了**门控机制** (gates) 或**记忆单元** (memory cell)。比如 LSTM 中有遗忘门、输入门、输出门，GRU 中有更新门、重置门，Transformer 中也会有自注意力等结构。

这些结构的出现让"**单纯的一层线性映射 + 激活函数**"变得更复杂，已经不是直接套用下面公式就能涵盖的。

$h^{(l)} = \sigma\bigl(W^{(l)}h^{(l-1)} + b^{(l)}\bigr)$

**门控**让网络可以"有选择地保留或遗忘"过往信息；**注意力**让网络可以"动态地为输入序列不同位置分配权重"。这些都是在公式层面额外引入了许多**参数化操作**和**非线性控制**，不再是一次性地把上一层的输出扔进某个函数这么简单。

因此，把它们笼统地写作"线性+激活"虽然**本质运算**依旧是矩阵乘法加上元素级操作，但**结构/连接模式**有了显著差异，也对应了不同的学习功能。

---

## 4. 任务驱动下的架构与普通层堆叠的差别

- 在实际大型系统（如机器翻译、对话系统、自动驾驶感知-决策）中，往往需要把多个神经网络模块串起来，每个模块完成不同"子任务"，其输入输出的形态、时间步循环方式、损失函数都可能不同。
- 如果我们只是一味地把所有操作都写成某些层的前向计算，固然在低层次上是"同样的公式"，却难以**清晰地标识出**：哪些层是感知、哪些是推理、哪些是生成、哪些是辅助监督等等。
- 「编码器-解码器」是非常典型且被实践验证了的**结构化模式**：它帮助我们更好地组织训练过程、理解中间的表示，以及在推理时拆分任务流程。

---

## 5. 小结

- **从最底层的数学运算上看**，神经网络确实绕不开「矩阵乘法 + 非线性激活」这条主线；你所说的"所有网络本质相同"可以理解为"它们都遵循类似的核操作"。
- **但在更高层的架构设计**中，什么时候读入数据、如何将信息编码、何时对数据再加工、如何一步步生成序列、各个门或注意力如何影响信息流动，这些"结构层面"的差异，决定了网络在不同任务上的表现和可解释性。
- 因此，"编码器-解码器"并不是简单的给几层网络取个名字，而是一种**面向任务的网络拓扑与信息流设计**；它能把**简单的线性+激活运算**"排布"得更有针对性，并且利用门控、注意力等机制达成更强的学习与推理能力。

换言之，你完全可以把一整个 Seq2Seq 模型都写成一个巨大的 h=σ(Wh+b)h = \sigma(W h + b) 的展开，但那样做会让我们在工程实现、可读性、可解释性，以及后续的改进中付出极大代价。真正让模型在特定场景发挥作用的，往往是**"在这些基础运算之上，怎样规划信息流、怎样让网络学会在某些子任务上专门负责某件事"**。这就是"编码器-解码器"这样高层结构所扮演的角色。

## 编码器

好的,我来帮你用一个具体的例子解释编码器的工作原理。

让我们以翻译 "What is your name?" 为例:

1. **词向量转换**  
假设我们用 100 维的词向量,每个词会被转换成对应的向量:
- "What" →[0.2, -0.5, 0.1,..., 0.3]
- "is" →[-0.1, 0.3, 0.4,..., -0.2]
- "your" →[0.5, 0.1, -0.3,..., 0.1]
- "name" →[-0.3, 0.2, 0.1,..., 0.4]
- "?" →[0.1, -0.1, 0.2,..., -0.1]

1. **双向 LSTM 处理**  
我们用双向 LSTM 处理这个序列:

```Java
正向: What → is → your → name → ?
反向: ? ← name ← your ← is ← What
```

每个时间步,LSTM 会:

- 读入当前词的词向量
- 更新内部状态(记忆单元和隐藏状态)
- 输出一个隐藏状态向量

比如对于 "your" 这个位置:

- 正向 LSTM 已经看过 "What is"
- 反向 LSTM 已经看过 "? name"
- 两个方向的隐藏状态拼接后能同时利用前后文信息

1. **得到上下文向量**  
最后我们得到一个固定维度(比如 256 维)的上下文向量 C,它压缩了整个句子的信息,包含:
- 句子是疑问句
- 询问对方名字
- 语气比较礼貌正式
- 等等...

这个上下文向量 C 就会被传递给解码器,作为生成目标语言(比如中文)翻译的基础。

你觉得这个例子有助于理解编码器的工作原理吗?要是有任何不清楚的地方,我很乐意进一步解释。

## 解码器

好的,让我继续用同样的例子("What is your name?" → "Comment t'appelles tu?")来详细解释解码器的工作过程:

1. **初始化阶段**
- 将编码器得到的上下文向量 C 用来初始化解码器的隐藏状态
- 输入特殊的起始符 ⟨GO⟩

1. **第一步生成**  
输入: ⟨GO⟩
- 解码器 LSTM 读入 ⟨GO⟩ 的词向量
- 结合隐藏状态,通过全连接层+softmax 计算所有法语词的概率
- 假设概率分布为:
  - "Comment": 0.85
  - "Bonjour": 0.10
  - 其他词: 0.05
- 选择概率最高的 "Comment" 作为输出

1. **第二步生成**  
输入: "Comment"
- 将 "Comment" 的词向量输入 LSTM
- 得到新的隐藏状态
- 预测下一个词的概率分布:
  - "t'": 0.75
  - "tu": 0.15
  - 其他词: 0.10
- 输出 "t'"

1. **继续迭代**
- 输入: "t'" → 输出: "appelles"
- 输入: "appelles" → 输出: "tu"
- 输入: "tu" → 输出: "?"
- 输入: "?" → 输出: ⟨EOS⟩(结束符)

每一步解码器都在:

1. 利用上一个输出词的信息
2. 利用隐藏状态中保存的句子整体语义
3. 预测最合适的下一个词

这就像一个"翻译官"在:

- 记住了英文句子的意思(通过上下文向量)
- 一边参考已翻译的部分
- 一边想接下来应该用什么词
- 直到翻译完整个句子

这个例子是否帮助你更好地理解解码器的工作机制?

## 序列解码策略

当我们用 Seq2Seq 预测一个句子时，需要在每一步挑出最可能的下一个词。整体来说，这就是一个**搜索问题**。常见方法有：

1. **贪心搜索（Greedy Search）**：每一步都选出概率最高的词。优点是快，缺点是可能一时选错导致后续翻译质量下降。
2. **束搜索（Beam Search）**：维持一个大小为 KKK 的候选序列队列，在每一步扩展所有候选，然后留下分数最高的前 KKK 条。
    - KKK 越大，理论上搜索范围越广，质量越好，但计算也越慢。
    - 实际中常用束宽 5 或 10（Beam size = 5 or 10），即可取得比较好的折中效果。
3. **随机采样（Ancestral Sampling）**：根据概率分布随机采样。可能生成多样性高，但有时翻译不稳定。

Beam Search 在机器翻译中用得最广泛。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/自注意力.md]
============================================================

---
draw:
title: 自注意力
date created: 2025-02-03
date modified: 2025-02-06
---

要想更形象、深刻地理解 Transformer，可以先从它的核心思想和整体结构入手，然后再去体会它为什么要用这种结构、它是如何将之前的 RNN/CNN 等结构加以改进的。下面提供一些帮助理解的思路和类比，供你参考：

  

一、核心思想：自注意力（Self-Attention）

Transformer 最突出的创新点就是基于自注意力机制（Self-Attention），它可以让网络在处理序列时，根据内容本身来决定"谁应该更加关注谁"。

你可以把自注意力想象成一个"**同学互相评分**"的场景：在一段文本中，每个"词"就像一位同学，每位同学需要给班里其他同学打分，打分越高表示自己越应该关注那位同学的信息。最后每位同学会结合别人的信息来丰富自己。

- 每个词会计算对所有词的注意力权重，并将这些权重加权地融合进来。
- 注意力权重越大，就表示这个词在当前词的理解里越重要。
- 对比 RNN/CNN：
	- 传统 RNN 是一种顺序阅读文本的方式，当前时刻会依赖前面时刻的隐藏状态，不能很好地捕捉远距离依赖。
	- CNN 也要通过卷积核在局部滑动并多次叠加卷积层，才能逐渐把远距离信息捕捉起来。
- Transformer 直接通过自注意力机制，让每个位置"一次就能看到"序列中所有其他位置的信息（可以并行处理），从而让捕捉全局依赖关系变得高效灵活。

  

二、整体结构：编码器-解码器（Encoder-Decoder）框架

  

最初的 Transformer（如论文《Attention is All You Need》）是为了机器翻译而设计的，使用了Encoder-Decoder的结构。但在很多下游任务中，我们也常常只用 Encoder（例如 BERT）或只用 Decoder（例如 GPT）部分。

- Encoder：把输入序列编码成一个更高级的表示，即把输入序列中的各个位置（词）"翻译"成更能被模型理解的向量。
- Decoder：基于 Encoder 的输出和部分目标序列信息，逐步生成目标序列（翻译、文本生成等）。

  

Encoder 内部的结构：

1. 输入嵌入 + 位置编码：
	- 把每个词或 token 映射到一个向量（Embedding），这是你常见的 " ->[向量]" 的操作。
	- 因为自注意力不再显式依赖 "顺序结构"，所以还要加入"位置编码" 来告诉模型输入序列中每个词的相对或绝对位置。

2. 自注意力层（Multi-Head Self-Attention）：
	- 每个词对整个序列进行"打分"，从而获取上下文信息。
	- 这里的 "Multi-Head" 表示模型会并行使用多个注意力头，每个注意力头都可以"关注"文本里的不同方面（比如某些头专注于语法关系，另一些头专注于语义关联），然后将它们的结果拼接或加和在一起，形成更丰富的表示。

3. 前向反馈网络（Feed-Forward Network, FFN）：
	- 在每个注意力层后面，还会有一个简单的多层感知机（MLP），对每个位置的向量单独地进行非线性变换。这就像是 "对每个词做进一步处理"。

4. 残差连接 + Layer Norm：
	- 跳跃连接（residual）用来缓解深层网络训练困难；Layer Normalization 用来稳定和加快训练收敛。

  

Decoder 内部的结构：

Decoder 的每个层也包含自注意力机制，但分成两个部分：

1. Masked Self-Attention：保证解码时只能看到已生成的部分，防止"剧透"；
2. Encoder-Decoder Attention：向 Encoder 的输出"查询"信息，以便生成当前词时可以参考原文或条件信息；
3. 前向反馈网络：同样用于对解码中的向量做进一步的特征变换。

  

三、分步骤的形象理解

可以把 Transformer 分解为几个"动作"来理解：

1. 输入词向量化：
	- 每个词先变成一个向量，你可以把它理解成描述这个词"语义特征"的一个数字向量。再加上位置编码，让模型知道这是第 1 个词、还是第 2 个词、还是第 10 个词。

2. 自注意力：谁该关注谁？
	- 对于序列中的每个词，生成三个向量：
	- Query（查询），Key（键），Value（值）。
	- 用 Query 和 Key 计算相似度（点积），相似度越大说明词 A 更应该关注词 B。
	- 将相似度（再经过 Softmax）当成加权系数，去对 Value 做加权求和，得到融合了全局信息的新表示。
	- 多个注意力头（Multi-Head）同时进行，这样每个头就像"不同的观察角度"，最后把它们的输出综合起来。

3. 前向网络：深挖信息
	- 把注意力层输出的结果，再经过一个全连接/非线性的网络，就像对每个词的位置做"进一步处理"，萃取更抽象的特征。

4. 层与层之间的残差和归一化
	- 保持上下文梯度流动通畅，避免训练中梯度消失或爆炸，提升稳定性。

5. 不断堆叠：
	- 将若干个这种 "自注意力 + 前向网络 + 残差 + 归一化" 的模块堆叠，就构成了编码器或解码器的大部分主体。

6. （如果是 Decoder）再和 Encoder 交互
	- Decoder 里面专门有一个"Encoder-Decoder Attention"，把 Encoder 的输出当做 Key/Value，把 Decoder 当前层的中间表示当做 Query，让生成端能参考输入信息。

  

四、结合数学公式与直觉

你提到已经理解 MLP 里的  并接激活函数来拟合，这里 Transformer 的 Feed Forward 部分和常规的 MLP 很相似。而它的神来之笔在于自注意力：

1. 自注意力的公式（以单头为例）：

  

其中：

- 是输入序列的向量表示；是可学习的参数矩阵；
- 是缩放因子，防止分母过小或过大导致梯度不稳定；
- 让所有的注意力权重加和为 1。

1. Multi-Head：

把上面这套操作同时做 8 次或更多次，然后再把这些结果拼接起来，通过一个线性变换。这样可以保留多方面的注意力信息。

  

从直觉上来讲，可以理解为"每个位置"在做信息整合之前，先用不同的方式去"看别的位置"、"决定关注度"，然后再把不同视角下的关注结果都合并起来。

  

五、为什么这样做有效？

- 并行性：自注意力让整个序列的处理可以在一个层级并行进行，而不像 RNN 那样顺序依赖，从而极大加快了训练速度和效率。
- 全局关联：每个位置随时都能查看到其他位置的信息，远距离依赖也能够被轻松捕捉。
- 多头机制：让模型可以在不同子空间中学习到不同类型的特征或依赖模式，呈现出比单一注意力头更强的表达能力。

  

六、在脑海里建模 Transformer

  

如果想让这个概念更形象，可以尝试以下几种想象方式：

1. "讨论组"比喻：

- 序列中的每个词是一位组员，每个人都有自己携带的信息（Value），也都有自己能发问（Query）和被询问（Key）的方式。
- 当"讨论"开始时，每个人先根据自己的问题（Query）去看所有其他人的特征（Key），算出相似度来决定"要听谁说话"。
- 最后通过这种"注意力加权"把他人带来的信息"听"进自己这里，更新自己的状态。

1. "看世界"比喻：

- 每个注意力头都像一副特殊的眼镜，它能捕捉到句子/序列中的某些关系（比如语法结构或核心语义）。每个词都会戴上这些眼镜，看看整句话谁最重要，然后加权吸收信息。多头注意力就像戴上了很多副不同风格的眼镜。

1. "全局扫描"对比 "局部扫描"：

- RNN/CNN 更多是"一步步或者局部"去提取信息；
- Transformer 在一个层就可以做"一次全局扫描"。想象你手里有一束光，可以一次就照在所有词上，然后根据他们与当前词之间的重要程度来融合信息。

  

七、学习建议

- 循序渐进地实现小示例：

可以先尝试用一个非常小的 Transformer，甚至手写一个 Self-Attention 层，来看看计算过程。

- 可视化工具：

有一些可视化 Demo（比如 The Illustrated Transformer），能让你直观地看到注意力分布。

- 调试与观察：

在调试过程里打印出注意力矩阵，看看模型到底"关注"了哪些位置。

- 常见的改进版：

比如 BERT、GPT、T5 以及它们的各种变体，都在 Transformer 的基础上进行一些改动或针对性增强，理解它们也有助于更好地加深对 Transformer 原理的感受。

  

总结

  

要形象深刻地理解 Transformer，最关键的是抓住"自注意力机制"这条主线，然后再结合多头注意力 + 前向网络 + 残差连接这几个重要模块去形成完整的心智模型。自注意力让每个位置都能够"看到"整个序列，并且在不同的头中以不同方式关注别的词，从而一次就能得到全局的关联信息。

  

希望这些类比和思路能让你在脑中更轻松地描绘出 Transformer 的工作过程。如果能配合简单的实现、可视化注意力权重、再结合一些小任务来实验，理解会更加深刻。祝学习顺利！



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/词向量 embedding.md]
=====================================================================

---
draw:
tags: []
title: 词向量 embedding
date created: 2025-01-24
date modified: 2025-01-24
---

> _a word is known by the company it keeps_



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/词向量.md]
===========================================================

---
draw:
tags: []
title: 词向量
date created: 2025-01-29
date modified: 2025-01-29
---

```shell
jupyter notebook exploring_word_vectors.ipynb
```

## 词如何变成向量？

### 方法一：[[共现矩阵]]

1. 收集所有的文本成为 `List<word>`
2. 然后去重成 `Set<word>`并排序
3. 然后把 `Set<word>` 作为共生矩阵的横轴和纵轴，设定窗口大小后生成共生矩阵的内容，
4. 然后做降维，然后画图

### 方法二：预测式词向量

[[GloVe]]

## 多义词与同音词

多义词 Polysemes 的不同含义之间有逻辑关联，可以追溯到相同的语源

  1. "head" (头)
     - 身体的头部："He nodded his head"
     - 领导者："She is the head of the department"
     - 物体的顶部："the head of the mountain"
  2. "paper"（纸）
     - 材料："a sheet of paper"
     - 报纸："the morning paper"
     - 学术论文："published a paper"
     

同音词 Homonyms 的不同含义之间没有关联，通常来自不同的语源，只是碰巧拼写或发音相同

- 举例：
  1. "bank"
     - 银行："I went to the bank to deposit money"
     - 河岸："sitting on the river bank"
  2. "rose"
     - 玫瑰花："a red rose"
     - rise的过去式："The sun rose in the east"

## 类比

这段代码是在演示如何使用词向量（word vectors）来解决类比问题。具体来说：

1. 问题设置：
- 给定类比 "man: king:: woman: x"
- 意思是："男人之于国王，如同女人之于 x"
- 目标是找出 x 应该是什么词

1. 解决方法：
- 使用 `most_similar` 函数
- 设置 positive=['woman', 'king']和 negative=['man']
- 这相当于在计算：king - man + woman = x
- 函数返回与计算结果最相似的词及其相似度分数

1. 结果解读：
- 'queen'（女王）得分最高（0.697）
- 这符合直觉：男人对应国王，女人对应女王
- 其他相关词如 'princess'（公主）、'monarch'（君主）等也都在列表中

这个例子展示了词向量能够捕捉词语之间的语义关系，不仅仅是简单的相似性，还包括更复杂的类比关系。这是词向量一个很有趣的特性，说明它们在某种程度上编码了语言中的语义结构。

这种技术在自然语言处理中很有用，可以用于：

- 自动问答系统
- 语义搜索
- 机器翻译
- 文本理解等任务

## 词向量中的 bias

**（1）数据本身带有偏见**  
词向量（无论是 GloVe、word2vec、还是更先进的模型）都需要在大规模语料上进行训练。如果我们所收集的海量文本（如维基百科、新闻、社交网络等）本身就反映了人类社会的刻板印象或不平衡现象，比如"nurse（护士）和女性"同时出现频率高，而"nurse（护士）和男性"一起出现频率较低，这些"共现频率"的差异最终会在词向量里留下烙印。因此，即使训练算法是中立的，一旦语料本身偏向某种群体、或呈现不平等的语言分布，就会将偏见嵌入模型。

**（2）模型优化目标强化了数据趋势**  
训练词向量的目标通常是让模型能"预测上下文"或"重构共现概率"等。换句话说，只要某些词之间在语料里经常一起出现，模型就会把它们向量拉近；如果不常一起出现，就会拉远。这样一来，社会惯用说法、媒体报道习惯等，会不断被放大并固化在词向量空间里。比如，如果语料中"boss（老板）"多半出现于"he" 的上下文，而较少出现在 "she" 的上下文，那么最终的向量空间也会将 boss 与男性词之间的距离拉近。

在下游任务里，如果直接拿这些词向量来做例如"招聘推荐""自动生成文本""情感分析"等时，模型可能会继承甚至放大这些偏见，从而导致歧视性结果。如：

- 自动翻译可能将中性的"医生"翻译成"He is a doctor"，而"护士"翻译成"She is a nurse"；
- 职位推荐系统更容易把管理岗职位推荐给男性用户，而不是女性。

因此，不仅仅是学术研究，也有越来越多的工业应用场景里需要意识到并处理这种偏见。

1. **对它的存在保持警觉**
    
    - 首先要承认：词向量学到的并不是"纯粹的语言知识"，而是人类社会使用语言过程中所存在的方方面面，包括历史和现实的刻板印象。
    - 一旦要在实际项目中用到这些向量，就需要仔细审视是否会引发潜在的歧视或不公平问题。
2. **数据采样与多样性**
    
    - 如果我们的语料过于单一，或过分偏向某个特定群体，那么偏见就更容易被放大。
    - 尝试从更多元化的文本来源获取数据，可以在一定程度上减少偏见（但并不能完全消除）。
3. **偏见检测与去偏（Debiasing）算法**
    
    - 对于"gender bias"，学术界已有一些方法来探测并"移除"向量中最显著的性别维度，从而减少对性别二分的过度关联（如 [Bolukbasi 等人在 2016 年提出的去偏方法](https://arxiv.org/abs/1607.06520)）。
    - 除了传统的词向量，现在对 BERT、GPT 等大型预训练语言模型，社区里也有一系列研究在探讨如何评测和缓解模型中的歧视与偏见。
4. **透明度与责任追究**
    
    - 在上线应用之前，需要让使用方或决策者清楚了解模型可能带来的歧视风险，并提供必要的审计与反馈渠道。
    - 在研究层面，也需要在论文或报告中披露训练数据来源、可能存在的偏见，从而让同行与公众更好地评估和应对。

在 CS224N 或相关 NLP 课程里，看到词向量的偏见并不是一个"偶然的 bug"，而是提醒我们：**自然语言处理算法背后的数据与社会现状息息相关**。通过这些实验与可视化，可以帮助学生意识到语言模型绝不是中性的。对这种 bias 要**积极地洞察、测试、审查和改进**，而不是盲目将模型当作"纯粹客观"的工具。这是当前 NLP 和 AI 领域重要的研究方向之一，也关乎社会公正与技术伦理。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/语法分析、依存句法分析.md]
===================================================================

---
draw:
tags: []
title: 语法分析、依存句法分析
date created: 2025-01-24
date modified: 2025-01-24
---

好的！为了让你更直观地理解 **依存句法分析 (Dependency Parsing)** 是什么，以及它在 Neural Transition-Based Dependency Parsing 中扮演的角色，我们来看一个具体的例子。

**例子句子：** "The cat sat on the mat." (猫坐在垫子上。)

**依存句法分析的目标：** 就是要分析出这句话中，每个词 **依赖** 于哪个词，以及它们之间是什么 **依存关系**。

**分析结果 (依存树)：**

```mermaid
graph LR
    sat --> cat("cat")
    cat --> The("The")
    sat --> on("on")
    on --> mat("mat")
    mat --> the("the")
    style sat fill:#f9f,stroke:#333,stroke-width:2px
```

**关系解释 (用大白话描述):**

- **sat --> cat("cat")**: 表示 "sat" (坐) 支配 "cat" (猫)，箭头方向从支配词 "sat" 指向依存词 "cat"。 ("cat") 是在节点中显示的文本，这里用的是词本身。
- **cat --> The("The")**: 表示 "cat" (猫) 支配 "The" (The)。
- **sat --> on("on")**: 表示 "sat" (坐) 支配 "on" (在...上)。
- **on --> mat("mat")**: 表示 "on" (在...上) 支配 "mat" (垫子)。
- **mat --> the("the")**: 表示 "mat" (垫子) 支配 "the" (the)。
- **"cat" (猫) -> "sat" (坐)**
    
    - 关系：**主语 (subject)**
        
    - 解释：谁在坐？是 "猫" 在坐。"猫" 是 "坐" 这个动作的执行者，所以 "cat" 依赖于 "sat"。
        
- **"The" -> "cat" (猫)**
    
    - 关系：**限定词/冠词 (determiner)**
        
    - 解释："The" 用来限定 "cat"，说明是 "这只" 猫，而不是 "那只" 或其他猫。所以 "The" 依赖于 "cat"。
        
- **"on" (在...上) -> "sat" (坐)**
    
    - 关系：**介词 (preposition)**
        
    - 解释：在哪里坐？在 "垫子上"。"on" 引导的介词短语 "on the mat" 用来修饰 "坐" 这个动作的地点。所以 "on" 依赖于 "sat"。
        
- **"mat" (垫子) -> "on" (在...上)**
    
    - 关系：**介词宾语 (object of preposition)**
        
    - 解释：在 "什么" 上？在 "垫子上"。"mat" 是介词 "on" 的宾语，表示介词作用的对象。所以 "mat" 依赖于 "on"。
        
- **"the" -> "mat" (垫子)**
    
    - 关系：**限定词/冠词 (determiner)**
        
    - 解释：和 "The" -> "cat" 类似，"the" 限定了 "mat"，说明是 "这个" 垫子。所以 "the" 依赖于 "mat"。
        

**总结:**

依存句法分析就是找出句子中词语之间的 **依存关系**，用 **箭头** 表示 **从属关系的方向**，用 **词语** 表示 **核心词** 和 **从属词**。分析的结果可以画成 **依存树**，清晰地展示句子的结构。

**在 Neural Transition-Based Dependency Parsing 中：**

我们训练神经网络的目的，就是让它学会 **自动** 分析出这种依存关系。 在 Transition-Based 的方法中，我们通过一步步的 **转移操作** (SHIFT, LEFT-ARC, RIGHT-ARC) 来逐步构建这个依存树，而神经网络的作用就是 **预测每一步应该执行哪个转移操作**，最终完成整个句子的依存分析。

希望这个例子能让你更明白 "依存句法分析" 是在做什么了！它其实就是分析句子中词语之间的 **语法关系**，并用一种结构化的方式 (依存树) 来表示出来。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/负对数.md]
===========================================================

---
draw:
tags: []
title: 负对数
date created: 2025-01-24
date modified: 2025-01-24
---

负对数可以理解为一种**衡量不确定性或信息量**的方式。

- **概率越小，负对数越大：** 如果一个事件发生的概率非常小，那么我们对该事件发生的不确定性就越高，或者说该事件包含的信息量就越大。例如，如果我说"明天太阳从西边升起"，这是一个概率非常小的事件，因此它的负对数就非常大，表示我们对这个说法的惊讶程度很高，或者说这个说法包含的信息量很大。
- **概率越大，负对数越小：** 如果一个事件发生的概率非常大，那么我们对该事件发生的不确定性就越低，或者说该事件包含的信息量就越小。例如，如果我说"明天太阳从东边升起"，这是一个概率非常大的事件，因此它的负对数就非常小，表示我们对这个说法的惊讶程度很低，或者说这个说法包含的信息量很小。

**生活中的例子**

1. **地震预测：** 地震发生的概率通常很小，因此地震预测的负对数就很大。如果一个地震预报说"明天某个地方发生 8 级地震的概率是 0.001"，那么这个预报的负对数就是 -log(0.001) ≈ 6.91。这个数值越大，表示这个预报包含的信息量越大，也越值得我们关注。
2. **彩票中奖：** 彩票中奖的概率非常小，因此中奖的负对数非常大。这说明中奖是一个非常不容易发生的事件，包含的信息量非常大，因此中奖才会让人感到如此兴奋。
3. **天气预报：** 如果天气预报说"明天晴天的概率是 90%"，那么这个预报的负对数就比较小。这说明我们对明天是晴天这件事的确定性比较高，信息量比较小。而如果天气预报说"明天晴天的概率是 10%"，那么这个预报的负对数就比较大，说明我们对明天是晴天这件事的不确定性比较高，信息量比较大。
4. **信息论中的熵：** 在信息论中，熵（entropy）就是用负对数来定义的，用于衡量一个随机变量的不确定性。一个随机变量的熵越大，表示它的不确定性越高，包含的信息量越大。
    

**总结**

负对数在机器学习中扮演着重要的角色，它不仅方便了计算和优化，还提供了一种衡量不确定性和信息量的有效方式。通过理解负对数的含义，我们可以更好地理解机器学习模型的训练过程和目标。希望以上解释能够帮助你更好地理解负对数。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/链式法则.md]
============================================================

---
draw:
tags: []
title: 链式法则
date created: 2025-01-25
date modified: 2025-01-25
---

从数学的角度来说，**反向传播用到的"链式法则"主要来源于微积分中的多变量求导**，是对**复合函数求导**的一种系统运用。如果你回想一下传统的单变量微积分，可能在学到**"如果 $y = f(g(x))$，那么 $\frac{dy}{dx} = f'(g(x)) \cdot g'(x)$"**这类公式时，就已经接触到了最基础的**链式法则**。但在神经网络中，**我们有多个函数嵌套、且往往是多元函数**，所以会用到多重"链式"乘法。

下面做个简要梳理，帮你回忆和理解：

---

## 1. 单变量链式法则（高中或大学初级微积分）

最简单的形式：如果

$y = f\bigl(u(x)\bigr)$,

那么

$\frac{dy}{dx} = f'(u) \cdot \frac{du}{dx}$

这就是**单变量**的"链式法则"。在高等数学或者 AP Calculus 里，这一部分通常比较早就会学到。

---

## 2. 多变量版本（多元微积分）

神经网络的"前向传播"可以看作一个多层复合函数。例如，一个两层网络可以写成：

$\hat{y} = a^{(2)} = f^{(2)} \Bigl(W^{(2)} \underbrace{f^{(1)}(W^{(1)}x + b^{(1)})}_{\text{隐藏层输出}} + b^{(2)}\Bigr)$

如果损失函数是

$J = \frac{1}{2} \bigl(y - \hat{y}\bigr)^2$,

那么你可以将 $\hat{y}$ 又替换回上面那串复合函数，就得到

$J = \frac{1}{2} \Bigl(y - f^{(2)}\bigl(\dots f^{(1)}(\dots) + b^{(2)}\bigr)\Bigr)^2$

**要对这么一个"大复合函数"求各个参数的导数（梯度），就必须反复运用链式法则**。每一层都可以看成某种"内函数"的组合。所以我们往往写成分步：

$\frac{\partial J}{\partial \theta} = \frac{\partial J}{\partial a^{(2)}} \cdot \frac{\partial a^{(2)}}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial a^{(1)}} \cdot \frac{\partial a^{(1)}}{\partial z^{(1)}} \cdot \dots \cdot \frac{\partial z^{(1)}}{\partial \theta}$

这就是链式法则在多元、多层场景下的**扩展**。

---

## 3. 在神经网络中是怎么体现的？

在神经网络训练时，我们有很多参数（权重和偏置），每个参数对最终损失 $J$ 都有"间接影响"——它并不是直接出现在损失函数里，而是**先影响某一层的激活值，再逐层传递**，最终影响输出 $\hat{y}$，进而影响损失 $J$。

要知道某个权重 $w$ 的微调对损失 $J$ 的影响，就要**一路"倒推"**：

- $w$ 怎么影响本层输入 $z$；
- $z$ 怎么影响本层激活 $a$；
- 本层激活怎么影响下一层输入；
- 直至输出层，再从输出层怎么影响损失。

把这些局部影响（导数）**乘起来**，就得到了 $\frac{\partial J}{\partial w}$。这是典型的"链式法则"思路。

---

## 4. 建议的学习路径

1. **先把单变量微积分的链式法则复习好**：
    - 如果 $y=f(g(x))$，那么 $\frac{dy}{dx}=f'(g(x))\cdot g'(x)$。
    - 这是最简单的情形，先确保你对这条公式背后的推导和直觉非常熟悉。

2. **看看多元函数的导数和梯度概念**：
    - 比如 $z = f(x_1, x_2, \dots)$ 时，$\frac{\partial z}{\partial x_i}$ 是如何定义的。
    - 感受一下向量形式和矩阵形式的写法。其实只要你知道"对每个变量分别求偏导，再用链式法则"，就足够应付神经网络的推导了。

3. **结合神经网络的层结构去体会"复合函数"的层级"**：
    - 任何一层都是前一层输出的函数；
    - 激活函数、线性层(矩阵乘法)、偏置加法……都可以拆分成一小段、一小段，你只要对每一小段的导数公式熟悉，再把它们乘起来，就可以得到最终梯度。

---

## 5. 小结

- 你在神经网络中看到的**误差信号** $\delta$ 以及**梯度**的计算，**本质**就是在做多次"链式法则"的乘积。
- 所以如果你觉得不太理解，可以先回到**大学（或高中）微积分"[[复合函数求导]]"那一块，把**链式法则**重新捋一遍。
- 然后再看神经网络的例子时，就能把每一步都看作："先求对这一步输出的偏导，再乘以对上一步输入的偏导……"。于是，**一点点地把整个传播路线上的微小影响都链起来**，就知道一个参数变化对最终损失的影响了。

简单总结：**反向传播算法 = 多元微积分 + 链式法则** (在多层结构上的系统应用)。这就是它的数学来源与本质所在。



[CS/Q2：做CS的经典lab/CS224n Natural Language Processing/问答系统.md]
============================================================

---
draw:
tags: []
title: 问答系统
date created: 2025-01-29
date modified: 2025-01-29
---

这篇文章主要介绍了动态记忆网络(Dynamic Memory Network, DMN)在问答系统中的应用。让我用中文总结一下主要内容：

1. 文章开头介绍了问答系统(QA)的基本概念：
- QA系统的目标是从文档、对话等中直接提取信息来满足用户需求
- 最常见的是事实型问答(factoid QA)，比如"什么是氧化汞的符号？"这类问题
- 实际上很多NLP问题都可以看作是问答问题

1. DMN的四个主要模块：

- 输入模块(Input Module)：
  - 使用GRU或双向GRU来处理输入序列
  - 将词序列转换为事实表示序列
- 问题模块(Question Module)：
  - 使用GRU来处理问题
  - 输出问题的编码表示
- 情节记忆模块(Episodic Memory Module)：
  - DMN的特色模块
  - 可以多次处理输入序列，每次关注不同的事实子集
  - 使用注意力机制来更新记忆表示
- 回答模块(Answer Module)：
  - 使用GRU解码器
  - 结合问题模块和情节记忆模块的输出来生成答案

1. DMN的优势：
- 在多个任务上表现优秀，包括问答、情感分析和词性标注
- 可以通过修改输入模块来处理不同类型的输入，比如图像问答
- 通过多次传递可以更好地理解句子，只关注与最终任务相关的部分

总的来说，这篇文章介绍了一个模块化的通用型神经网络架构，它试图解决各种问答任务，并且具有良好的扩展性和适应性。



[CS/Q2：做CS的经典lab/CS229/CS229 Machine Learning.md]
=================================================

---
draw:
title: CS229 Machine Learning
date created: 2025-01-29
date modified: 2025-02-14
---

我也在自学CS229，我就做一点微小的工作，来给题主一点思考吧。CS229的材料分为notes，四个ps，还有ng的视频。我是先看notes，预习一下。然后再看视频。个人觉得最重要的是你看完这两个东西之后能够独立的说出你看的那个话题的逻辑思路，然后最好是能够通过严格的数学推导来论证你这个逻辑思路。我是每看完一个话题之后，盖上书，自己用纸笔推导一遍。强烈建议当进行到一定程度的时候把提供的problem set 自己独立做一遍，然后再看答案。你提到的project的东西，个人觉得可以去kaggle上认认真真刷一个比赛，就可以把你的学到的东西实战一遍。最后强烈推荐同时看一看同样在SEE里的convex optimization！的内容。学一点优化的人生经验。毕竟机器学习的问题到最后都转化为优化的问题。［SEE］（https：//http：//see。stanford。edu），个人觉得这个网站真是良心。

同在自学cs229。我是看完ng在coursera上的机器学习视频来的。一楼的老兄说的没错，听课之前最好还是先浏览一下材料，然后不懂的地方去结合李航的《统计学习方法》上面找答案。实战的话，《机器学习实战》对初学者很友好。Ng讲完哪个算法，去《统计》上巩固一下，然后找到《实战》上对应的章节用python敲一下代码，美滋滋。

让我用 Markdown 创建一个 CS229 编程作业的详细表格：

| Problem Set | 主要内容 | 算法/模型 | 应用案例 |
|------------|---------|-----------|----------|
| PS1 | 监督学习基础 | • 线性回归<br>• Logistic 回归<br>• 梯度下降 | • 房价预测<br>• 分类问题 |
| PS2 | 支持向量机与生成模型 | • SVM<br>• 核方法<br>• EM算法 | • 垃圾邮件分类<br>• 手写数字识别 |
| PS3 | 神经网络与降维 | • 神经网络<br>• PCA<br>• 偏差/方差分析 | • 图像处理<br>• 数据可视化 |
| PS4 | 高级主题 | • 强化学习<br>• ICA<br>• 马尔可夫决策过程 | • 智能体控制<br>• 信号分离 |
| Final Project | 实际应用 | • 综合运用课程所学算法<br>• 自选主题 | • 开放性项目<br>• 真实数据集应用 |

这些作业通常要求学生：

- 实现算法核心部分
- 数据预处理和分析
- 参数调优
- 结果可视化和分析



[CS/Q2：做CS的经典lab/MIT 6.828/C/C语言.md]
====================================

---
draw:
tags: []
title: C语言
date created: 2024-12-16
date modified: 2024-12-27
---

## 学习 C 语言的建议

1. **理解与 Java 的差异**  
    你已具备 Java 基础，因此首先需要意识到 C 与 Java 的本质区别：
    - **内存管理**：C 没有垃圾回收，需要手动使用 `malloc`/`free` 或 `realloc` 管理内存。
    - **指针与数组**：C 中指针是核心概念，可直接操作内存地址。数组与指针关系紧密。
    - **无运行时检查**：C 是更底层的语言，没有默认的越界检查，也不提供异常机制，程序员需要自行确保安全性。
    - **结构与共用体**：C 中的 `struct` 用于聚合数据，不像 Java 的类那样自带方法。`union` 是特别的类型用来共享同一块内存区域。
2. **核心语法和基本用法**  
    学习重点可以参考 K&R 的《C程序设计语言》或其它经典教材。熟悉以下内容：
    - 基本数据类型 (`int`, `char`, `float`, `double`)、控制结构 (`if`, `while`, `for`, `switch`)
    - 指针、函数指针、指针与数组的使用方式
    - 字符串处理（C 字符串以 `\0` 结尾）
    - 结构体与共用体 (`struct`, `union`)
    - 预处理器指令（`#include`, `#define`, `#ifdef`等）
    - 内存分配函数（`malloc`, `calloc`, `free`）及内存管理
3. **通过小项目练习**
    - 用 C 实现简单数据结构（如链表、栈、队列）。
    - 用 C 编写简单的工具程序（如读取文件内容、文本处理）。
    - 尝试移植你在 Java 中写过的简单算法到 C 中实现。
4. **调试与工具**
    - 学会使用 `gcc` 或 `clang` 编译器以及基本编译参数。
    - 熟悉 `gdb` 调试器，能在 C 程序中断点调试、查看变量、堆栈回溯。
    - 学习使用 `valgrind` 等工具检查内存泄漏和内存错误。



[CS/Q2：做CS的经典lab/MIT 6.828/C/goto.md]
=====================================

---
draw:
tags: []
title: goto
date created: 2024-12-25
date modified: 2024-12-27
---

这是一个很好的问题！这里使用 `goto found` 而不是直接写代码主要有两个原因：

1. **代码结构清晰**：

```c
static struct proc* allocproc(void) {
  struct proc *p;

  for(p = proc; p < &proc[NPROC]; p++) {
    acquire(&p->lock);
    if(p->state == UNUSED) {
      // 如果在这里写初始化代码：
      // p->pid = allocpid();
      // p->state = USED;
      // ... 20多行初始化代码 ...
      // 会让循环结构不清晰
    } else {
      release(&p->lock);
    }
  }
  return 0;

found:
  // 所有初始化代码集中在这里
  p->pid = allocpid();
  p->state = USED;
  // ... 其他初始化 ...
}
```

1. **避免重复的锁处理**：

```c
for(p = proc; p < &proc[NPROC]; p++) {
    acquire(&p->lock);
    if(p->state == UNUSED) {
      // 如果在这里写代码，需要考虑：
      // 1. 初始化失败时的锁释放
      // 2. 成功时的返回前锁处理
      // 会使锁的处理逻辑分散
    } else {
      release(&p->lock);
    }
}
```

使用 goto 的好处：

1. 搜索逻辑和初始化逻辑分离
2. 错误处理更集中
3. 锁的管理更清晰

就像：

```java
// 不好的方式
for(Parking spot : parkingLots) {
    spot.lock();
    if(spot.isEmpty()) {
        spot.setOwner(car);
        spot.markOccupied();
        spot.paintLines();
        spot.installSensor();
        // ... 很多初始化代码
        // 容易让循环逻辑变得混乱
    } else {
        spot.unlock();
    }
}

// 更好的方式
Parking foundSpot = null;
for(Parking spot : parkingLots) {
    spot.lock();
    if(spot.isEmpty()) {
        foundSpot = spot;
        break;  // 相当于 goto
    }
    spot.unlock();
}

if(foundSpot != null) {
    // 所有初始化代码集中在这里
    foundSpot.setOwner(car);
    foundSpot.markOccupied();
    foundSpot.paintLines();
    foundSpot.installSensor();
}
```

虽然通常不推荐使用 goto，但在这种特定场景下，它能让代码更清晰和易维护。

`found` 不是一个独立的方法，它是一个标签（label），是 goto 语句的跳转目标。

在 C 语言中：

```c
static struct proc* allocproc(void) {
  struct proc *p;

  for(p = proc; p < &proc[NPROC]; p++) {
    acquire(&p->lock);
    if(p->state == UNUSED) {
      goto found;  // 跳转到 found 标签
    } else {
      release(&p->lock);
    }
  }
  return 0;

found:   // 这是一个标签，不是方法
  p->pid = allocpid();
  // ... 更多初始化代码 ...
  return p;
}
```

Java 中没有 goto 语句，但有类似的替代方案：

1. **使用标签 break**：

```java
public Process allocProc() {
    searchBlock: {  // 定义标签
        for(Process p : processes) {
            p.lock();
            if(p.isUnused()) {
                break searchBlock;  // 跳出到标签
            }
            p.unlock();
        }
        return null;
    }
    // 相当于 C 中的 found 标签后的代码
    p.setPid(allocPid());
    // ... 更多初始化代码 ...
    return p;
}
```

2. **使用方法提取**：

```java
public Process allocProc() {
    Process p = findUnusedProcess();
    if (p == null) return null;
    
    // 初始化代码
    initializeProcess(p);
    return p;
}

private Process findUnusedProcess() {
    for(Process p : processes) {
        p.lock();
        if(p.isUnused()) {
            return p;
        }
        p.unlock();
    }
    return null;
}
```

3. **使用异常控制流**（不推荐）：

```java
public Process allocProc() {
    try {
        for(Process p : processes) {
            p.lock();
            if(p.isUnused()) {
                throw new FoundException(p);  // 跳转到 catch
            }
            p.unlock();
        }
        return null;
    } catch (FoundException e) {
        Process p = e.getProcess();
        // 初始化代码
        return p;
    }
}
```

总结：

- C 中的 goto + label 是一种低级的控制流机制
- Java 不支持 goto，但提供了其他更结构化的替代方案
- 在大多数情况下，重构为独立方法是更好的选择



[CS/Q2：做CS的经典lab/MIT 6.828/C/头文件.md]
====================================

---
draw:
tags: []
title: 头文件
date created: 2024-12-25
date modified: 2024-12-27
---

在 C 语言里，我们通常把某个"模块"或"类"拆成 `.h`（头文件）和 `.c`（源文件）两个部分：

- **`.h`** 里放数据结构声明、函数原型、宏常量等"对外可见的接口"。
- **`.c`** 里放具体的函数实现、内部逻辑、私有变量等。

而在 Java 里，并没有单独的"头文件"概念，通常是**一个类写在一个 `.java` 文件里**，既包含类名、方法声明，又包含具体实现。可以粗略类比为：

- **`.h`** 对应 Java 代码中"类的签名和对外公开的方法、字段等接口"。
- **`.c`** 对应 Java 类中"方法体和实现细节"。

只是 Java 在一个 `.java` 文件中就把"声明"和"实现"都写完了，不需要像 C/C++ 一样拆成两个文件。

如果硬要找种"更像.h 和.c 分开的形式"，则可以把 Java **`interface`**（接口）类比为 C 语言里的头文件（只声明了要实现的方法），再把实现该 interface 的 **`class`** 类比为对应的 `.c` 文件（提供了这些方法的实际代码）。不过依旧不完全相同，因为 Java 的接口和类是两个不同的概念，而 C 里头文件和源文件本质上还是同一个"模块"的两部分。



[CS/Q2：做CS的经典lab/MIT 6.828/C/导包机制.md]
=====================================

---
draw:
tags: []
title: 导包机制
date created: 2024-12-25
date modified: 2024-12-27
---

是的！这是一个很好的观察。让我对比一下 Java 和 C：

## Java 的导入机制

```java
import java.util.List;
import com.example.MyClass;
```

1. 只有 `import` 关键字
2. 面向对象，以类为单位
3. 编译器自动处理依赖
4. 包（package）管理系统
5. 不需要关心实现和声明的分离

## C 的机制

```c
#include <stdio.h>        // 头文件导入
extern int some_var;      // 外部变量声明
```

1. 有两种机制：
   - `#include`：预处理阶段的文件包含
   - `extern`：编译阶段的符号声明
2. 以文件为单位
3. 需要手动管理实现和声明
4. 没有包管理系统

## 为什么 C 需要两种机制？

这与 C 语言的编译模型有关：

1. **C 的编译过程**：

```Java
[源文件] → 预处理 → 编译 → 链接 → [可执行文件]
```

2. **Java 的编译过程**：

```Java
[源文件] → 编译 → [字节码] → JVM运行
```

## 实际例子对比

**Java 方式**：

```java
// MyClass.java
public class MyClass {
    public static int count = 0;
}

// OtherClass.java
import com.example.MyClass;

public class OtherClass {
    void someMethod() {
        MyClass.count++;  // 直接使用
    }
}
```

**C 方式**：

```c
// globals.h
extern int count;  // 声明

// globals.c
int count = 0;    // 定义

// other.c
#include "globals.h"  // 获取声明
void someFunction() {
    count++;      // 使用
}
```

## 总结

1. Java 的 `import` 更高级，自动处理了很多底层细节
2. C 需要手动管理实现和声明的分离，所以需要 `extern` 和 `include` 两种机制
3. 这反映了两种语言的设计理念：
   - Java：高级、自动化、面向对象
   - C：底层、手动控制、面向过程



[CS/Q2：做CS的经典lab/MIT 6.828/C/格式说明符.md]
======================================

---
draw:
tags: []
title: 格式说明符
date created: 2024-12-25
date modified: 2024-12-27
---

%p 是用来打印指针（pointer）的格式说明符。它会以十六进制形式打印内存地址。

例如：

void *ptr = (void*)0x1234abcd;

printf("%p\n", ptr);  // 输出类似：0x1234abcd

常见的格式说明符：

- %d - 十进制整数
- %x - 十六进制整数（小写）
- %X - 十六进制整数（大写）
- %p - 指针地址（通常是十六进制）
- %s - 字符串
- %c - 单个字符
- %f - 浮点数

在你提供的代码中：

printf("page table %p\n", pagetable);

这行代码会打印页表的内存地址，例如可能输出：

page table 0x80023000

这对于调试和理解内存布局非常有用，因为它让我们能看到具体的内存地址。



[CS/Q2：做CS的经典lab/MIT 6.828/MIT 6.828 Operating System Engineering.md]
=====================================================================

---
draw:
tags: []
title: MIT 6.828 Operating System Engineering
date created: 2024-12-11
date modified: 2025-01-24
---

## 参考资料

参考思路：
https://blog.miigon.net/categories/mit6-s081/

MIT 6.S081 2020 操作系统 实验环境搭配详解及问题处理 - 北海草鱼的文章 - 知乎  
https://zhuanlan.zhihu.com/p/331492444

MIT6.828- 神级 OS 课程 - 要是早遇到，我还会是这种 five 系列 - 一丁点儿的文章 - 知乎  
https://zhuanlan.zhihu.com/p/74028717

notebook：
https://notebooklm.google.com/notebook/0d26bbfc-c5d1-4ff3-9bb0-3823aa2d956c?_gl=1*1j7d54k*_ga*MTk1MTE0NzAxMC4xNzI3NTc3NTUw*_ga_W0LDH41ZCB*MTczMzkyNDYzMC4yLjEuMTczMzkyNDYzMC42MC4wLjA.  
参考书籍：
https://pdos.csail.mit.edu/6.828/2021/xv6/book-riscv-rev2.pdf  
参考代码：
https://github.com/PKUFlyingPig/MIT6.S081-2020fall  
课程原视频：
https://www.youtube.com/watch?v=J3LCzufEYt0&list=PLTsf9UeqkReZHXWY9yJvTwLJWYYPcKEqK  
官方地址：
https://pdos.csail.mit.edu/6.S081/2020/

## Lab

| Lab                      | Tasks                                                                    |
| ------------------------ | ------------------------------------------------------------------------ |
| [[Lab1 Unix utilities]]  | • 实现基本的Unix工具（如sleep, find, ping-pong等）<br>• 熟悉xv6系统调用<br>• 初步了解Unix系统编程 |
| [[Lab2 System calls]]    | • 添加系统调用到xv6<br>• 实现trace和sysinfo系统调用<br>• 理解系统调用机制                      |
| [[Lab3 Page tables]]     | • 实现页表相关功能<br>• 添加物理页面的分配追踪<br>• 实现用户页面访问权限控制                            |
| [[Lab4 traps]]           | • 实现RISC-V汇编陷阱处理<br>• 实现时钟中断<br>• RISC-V的陷阱机制                            |
| [[Lab5 Lazy allocation]] | • 实现延迟页面分配<br>• 处理页面错误<br>• 优化内存使用                                       |
| [[Lab6 Copy on Write]]   | • 实现写时复制fork<br>• 优化进程创建<br>• 实现页面引用计数                                   |
| [[Lab7 Multithreading]]  | • 实现用户级线程<br>• 实现线程切换<br>• 实现线程同步原语                                      |
| [[Lab8 Lock]]            | • 实现自旋锁和睡眠锁<br>• 提高并行性<br>• 减少锁竞争                                        |
| [[Lab9 File system]]     | • 实现大文件支持<br>• 实现符号链接<br>• 优化文件系统性能                                      |
| [[Lab10 mmap]]           | • 实现mmap系统调用<br>• 实现内存映射文件<br>• 处理页面错误                                   |
| [[Lab11 Network Driver]] | • 实现网络设备驱动<br>• 实现网络数据包收发<br>• 了解设备驱动编程                                  |

## 常用命令

```shell
cd /Users/liuyishou/usr/projects/MIT6.S081-2020fall

make qemu // 启动虚拟机

make grade // 打分

// 虚拟机里只有 python3
python3 ./grade-lab-util sleep


```

## 配环境

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F12%2F11%2F20-16-18-49f660603551ec40cb3d2de84564db4c-202412112016393-088440.png)

| 命令行                                                                                                                        | 作用                        | 说明                                                                                                                                                                                                                                                                                                |
|:------------------------------------------------------------------------------------------------------------------------- |:------------------------ |:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `sudo apt-get install git build-essential gdb-multiarch qemu-system-misc gcc-riscv64-linux-gnu binutils-riscv64-linux-gnu` | 安装基础编译与开发工具链              | 通过 APT 包管理器以管理员权限（sudo）安装：<br>- **git**：版本控制工具<br>- **build-essential**：编译所需的基础工具（如 gcc、g++、make）<br>- **gdb-multiarch**：支持多种架构的 GDB 调试器<br>- **qemu-system-misc**：QEMU 仿真器的一组工具，用于模拟多种硬件架构<br>- **gcc-riscv64-linux-gnu**：RISC-V 架构交叉编译的编译器<br>- **binutils-riscv64-linux-gnu**：RISC-V 架构的二进制工具集 |
| `sudo apt-get remove qemu-system-misc`                                                                                     | 移除已安装的 qemu-system-misc 包 | 在需要安装特定版本的 QEMU 之前，先移除已有的 qemu-system-misc 版本，以防冲突。|
| `sudo apt-get install qemu-system-misc=1:4.2-3ubuntu6`                                                                     | 安装指定版本的 qemu-system-misc  | 利用 apt-get 安装特定版本（`1:4.2-3ubuntu6`）的 qemu-system-misc，以满足特定的环境要求和依赖关系，确保后续实验顺利运行。|



[CS/Q2：做CS的经典lab/MIT 6.828/Makefile.md]
=======================================

---
draw:
tags: []
title: Makefile
date created: 2024-12-18
date modified: 2024-12-27
---

让我用简单的方式来解释 Makefile：

想象你在搭乐高玩具城堡：

1. 你知道要按顺序来：
   - 先搭底座
   - 再搭墙
   - 最后放屋顶

2. Makefile 就像是你写的搭乐高说明书：

```makefile
城堡: 底座 墙 屋顶
    把它们组合在一起

底座:
    先找出所有底座的零件
    按说明拼好底座

墙:
    拿出墙的零件
    把墙砌好

屋顶:
    准备屋顶零件
    把屋顶拼好
```

你已经会 Java，对吧？那这样理解：

- 如果你写了 3 个 Java 文件: A.java、B.java、C.java
- 你需要把它们编译成 A.class、B.class、C.class
- 最后组合成一个程序

Makefile 就是帮你：

1. 记住编译的顺序
2. 只重新编译改动过的文件（就像乐高城堡，如果只有墙倒了，你只要重新搭墙，不用整个拆了重来）
3. 告诉电脑怎么一步步把你的程序搭建起来



[CS/Q2：做CS的经典lab/MIT 6.828/QEMU.md]
===================================

---
draw:
tags: []
title: QEMU
date created: 2024-12-11
date modified: 2024-12-27
---



---

## 1. **虚拟化功能**

QEMU 可以虚拟化一整台计算机，包括 CPU、内存、硬盘、网络设备等。

- **加速虚拟化（KVM）**：当与 KVM（Kernel-based Virtual Machine）结合使用时，QEMU 能够通过硬件支持（如 Intel VT 或 AMD-V）显著提升性能。
- **支持多架构**：支持包括 x86、x86-64、ARM、MIPS、RISC-V、PowerPC 和 SPARC 等在内的多种 CPU 架构。

---

## 2. **设备仿真**

QEMU 可以模拟不同的硬件设备，帮助开发者在不需要真实硬件的情况下测试和运行程序。

- 适合嵌入式开发人员测试特定架构上的程序。
- 提供与实际硬件环境接近的仿真效果。

---

## 3. **软件开发和调试**

QEMU 是操作系统开发者的理想工具，可以用于以下场景：

- 测试不同架构上的操作系统。
- 调试未完成的系统内核。
- 在仿真环境中运行交叉编译的程序。

---

## 4. **快照与迁移**

QEMU 支持对虚拟机状态的快照和保存，方便用户恢复到之前的某个状态。同时，它还能将运行中的虚拟机迁移到另一台物理主机上。

---

## 5. **高可定制性**

- 支持自定义虚拟硬件配置，比如 CPU 核数、内存大小、网络拓扑。
- 提供多个命令行选项，满足高级用户的个性化需求。

---

## 常见用例

1. **虚拟机管理**：在桌面或服务器上运行虚拟机（结合 KVM）。
2. **嵌入式开发**：模拟 ARM 或 RISC-V 的硬件环境。
3. **操作系统学习**：测试运行多个系统（如 Linux、Windows、FreeBSD 等）。
4. **软件测试**：跨平台软件开发和调试。

---

## 示例：使用 QEMU 启动一个 Linux 镜像

```bash
qemu-system-x86_64 -hda linux.img -m 1024 -enable-kvm
```

- `qemu-system-x86_64`：指定模拟的 CPU 架构（x86-64）。
- `-hda linux.img`：指定虚拟机的硬盘镜像。
- `-m 1024`：为虚拟机分配 1024MB 内存。
- `-enable-kvm`：启用 KVM 加速。

---

**总结：** QEMU 是一个功能强大且灵活的工具，无论是虚拟化还是设备仿真，都能胜任多种场景。它在开源社区中备受欢迎，并在虚拟化和嵌入式开发领域有广泛的应用。



[CS/Q2：做CS的经典lab/MIT 6.828/RISC-V.md]
=====================================

---
draw:
tags: []
title: RISC-V
date created: 2024-12-16
date modified: 2024-12-27
---

## 学习 RISC-V 的建议

1. **RISC-V 基本概念**  
    RISC-V 是一种精简指令集架构（RISC）。学习时需理解：
    
    - 通用寄存器（如 `x0`-`x31` 的用途，`x0` 总是0）
    - 程序计数器（`pc`）与指令流水线基本概念
    - 基本指令分类（算术、逻辑、加载/存储、控制流等）
    - RISC-V 指令格式（R-type, I-type, S-type, B-type, U-type, J-type）
2. **官方资源与文档**
    
    - RISC-V 的官方文档中提供了基本指令集、扩展和参考手册（The RISC-V Reader 是很好的入门书籍）。
    - 确保掌握 RV32I 或 RV64I 基本指令集和其通用编码格式与执行逻辑。
3. **写简单的汇编程序**
    
    - 从最简单的"Hello World"汇编程序开始，在 RISC-V 模拟器或真实硬件（如 Spike 模拟器或 QEMU）上运行。
    - 学习汇编伪指令（如 `la`, `li`, `mv`）与真正的机器指令的区别。
    - 尝试实现基本算术计算、条件判断和循环的汇编实现。
4. **工具链使用**
    
    - 学习使用 RISC-V 工具链（`riscv64-unknown-elf-gcc`、`riscv64-unknown-elf-objdump`、`riscv64-unknown-elf-gdb`）。
    - 在模拟器（如 Spike）或 QEMU 中运行你的 RISC-V 程序。
    - 使用 `objdump` 查看编译后生成的二进制文件的反汇编结果，分析指令序列。
5. **与 C 结合**  
    一旦你掌握了 C 与 RISC-V，可以尝试用 RISC-V 工具链编译 C 程序为 RISC-V 机器码，然后通过查看反汇编理解编译器生成的指令，实现 C 到底层指令之间的理解与连贯。
    
6. **进一步拓展**
    
    - 如果你的目标与 xv6 或操作系统开发相关，可以将 RISC-V 的学习与 xv6 for RISC-V 的版本相结合，从中学习系统调用、上下文切换、中断处理这些底层机制在 RISC-V 上的实现。
    - 学习基本的汇编与链接脚本（`linker script`）知识，以更好地理解程序是如何加载与运行的。

## 整体学习策略

1. **快速过一遍语法与概念**：
    因为你有 Java 基础，在 C 中请重点放在指针、内存管理和编译工具链上，不需要在基本控制流上花太多时间。
    
2. **多实践**：
    理解是通过实践积累的。经常编译、运行、调试你的 C 代码，并通过观察程序行为加深理解。
    
3. **从简单到复杂**：
    对 RISC-V 来说，不要一开始就研究复杂的特性。先熟悉基本指令与寄存器，然后写小片段的汇编代码进行尝试。
    
4. **参考文档和样例代码**：
    当你遇到困惑，可以多查阅官方文档、手册和其他示例代码。RISC-V 有许多开源资源和示例工程可供参考。
    
5. **逐步结合实际场景**：
    当你对 C 与 RISC-V 有了一定掌握，可以尝试更低级的项目，比如编写简单的内核引导代码（bootloader）、或在 QEMU 中运行小的 RISC-V OS Demo（如 xv6 for RISC-V 版本）。
    

通过上述路径和策略，你可以利用已有的 Java 编程经验快速上手 C 语言，然后通过工具和简单示例代码理解 RISC-V 架构，从而为更深入的系统级编程（比如 xv6 开发或其他底层系统编程）奠定良好的基础。



[CS/Q2：做CS的经典lab/MIT 6.828/lab/Lab1 Unix utilities.md]
======================================================

---
draw:
tags: []
title: Lab1 Unix utilities
date created: 2024-12-11
date modified: 2025-01-25
---

## 任务

| 测试项目                       | 状态  | 分数    | 问题描述                      |
| -------------------------- | --- | ----- | ------------------------- |
| sleep, no arguments        | 通过  | 5/5   | 无                         |
| sleep, returns             | 通过  | 5/5   | 无                         |
| sleep, makes syscall       | 通过  | 10/10 | 无                         |
| pingpong                   | 通过  | 20/20 | 无                         |
| primes                     | 失败  | 0/20  | exec 失败，未输出素数             |
| find, in current directory | 通过  | 10/10 | 无                         |
| find, recursive            | 通过  | 10/10 | 无                         |
| xargs                      | 失败  | 0/19  | 'hello' 出现次数错误（期望3次，实际0次）|
| time                       | 失败  | 0/1   | 缺少 time.txt 文件            |

## 知识点

1. `exit(0)`:

- 表示程序正常退出
- 是约定俗成的成功状态码
- 通常意味着程序完成了它的任务且没有遇到错误

1. `exit(1)` (或任何非零值):

- 表示程序异常退出
- 是错误状态码
- 通常用于表示程序遇到了错误或异常情况

## 思想

[用户态、内核态](用户态、内核态.md)

## 在[Makefile](Makefile.md)添加条目

在 MIT 6.828 的 Lab1 中，你需要在 Makefile 里加入一些条目（targets、规则和变量）来编译和打包你要在 xv6 操作系统里运行的用户态程序。这么做的原因是：

1. **统一编译流程**：Makefile 是一个自动化编译工具。在 Lab1 里，你不光有内核代码（kernel code），也有用户态的测试程序（比如 pingpong、primes、xargs 等）。如果不在 Makefile 中指定这些程序的编译规则，`make` 就不会自动帮你编译、链接这些用户程序，也无法将它们打包进 xv6 的文件系统镜像中。换句话说，Makefile 里的改动让编译器和链接器知道该如何处理、产出对应的可执行文件。
2. **生成可执行文件和镜像文件系统**：xv6 的用户程序需要在实验给的 "用户态文件系统镜像" 中出现，这样系统在启动后才能从内核的 shell 中直接运行这些程序。Makefile 中增加的新内容会规定：
    - 如何从 `.c` 源文件生成可执行文件（如 `xargs`、`pingpong` 等）。
    - 如何将编译好的用户程序打包进 xv6 的文件系统镜像（`fs.img`）。
   没有这些信息，`make` 不会把用户程序放进系统镜像里，启动后你就无法在 xv6 中运行这些命令。

3. **实验要求与流程**：MIT 6.828 实验要求你不断丰富 xv6 的功能。早期的 Lab（如 Lab1）就会让你添加一些简单的用户态程序或工具，以测试、展示或检验你对内核和系统调用的理解。这些工具如果不通过 Makefile 进行适当配置，编译系统不清楚要处理哪些文件、生成什么目标文件，以及如何将其整合到可用的文件系统中。

总结来说，给 Makefile 增加相应条目是为了让你的用户态代码自动被编译、链接，并打包进 xv6 的镜像，从而在 xv6 环境中正常运行。这与 Java 项目中使用 `javac` 编译、`jar` 打包的流程是类似的，只不过这里是 C 程序和系统镜像的生成，而 Makefile 起到的作用有点类似于自动化构建脚本，让整个构建和打包过程自动化、可重复、可维护。

## 具体实现

### argv 和 argv 是什么的简称

argc 和 argv 是 C 语言中 main 函数的标准参数：

- argc = "argument count"
- 表示命令行参数的数量
- 至少为1（因为程序名本身就是第一个参数）
- argv = "argument vector"
- 是一个指向字符串数组的指针
- 存储所有的命令行参数
- argv[0] 是程序名
- argv[1] 到 argv[argc-1] 是实际的命令行参数

让我帮你分析一下 Lab1 Unix utilities 的实现思路。根据文档，这个 lab 包含几个主要任务:

1. sleep (easy)
2. pingpong (easy)
3. primes (moderate)
4. find (moderate)
5. xargs (moderate)

让我们一个一个来实现:

## 1. sleep (easy)

这是最简单的任务,需要实现一个类似 UNIX sleep 命令的程序。实现步骤:

1. 创建文件 `user/sleep.c`:

```c
#include "kernel/types.h"
#include "kernel/stat.h"
#include "user/user.h"

int main(int argc, char *argv[]) {
    if(argc != 2) {
        fprintf(2, "Usage: sleep ticks\n");
        exit(1);
    }
    
    int ticks = atoi(argv[1]);
    sleep(ticks);
    exit(0);
}
```

1. 在 Makefile 的 UPROGS 中添加 _sleep

### 1. 为什么 argc 要等于 2?

- argc 表示命令行参数的数量，包括程序名称
- 当我们运行 `sleep 10` 时:
  - argv[0]是 "sleep" (程序名)
  - argv[1]是 "10" (参数)
  - 所以总共应该有 2 个参数
- 如果 argc!= 2，说明用户输入格式不对，比如:
  - `sleep` (缺少参数，argc = 1)
  - `sleep 10 20` (参数过多，argc = 3)

### 2. 为什么要用 atoi 而不直接用 argv?

- argv 中的参数都是字符串类型 (char*)
- sleep 系统调用需要的是整数类型的 ticks
- atoi 函数将字符串转换为整数，比如:
  - "10" -> 10
  - "100" -> 100

### 3. 为什么两个分支都需要 exit?

- exit(1) 表示程序异常退出
  - 用于参数错误的情况
  - 返回非零值表示错误
- exit(0) 表示程序正常退出
  - 用于程序完成正常功能后退出
  - 返回 0 表示成功
- 如果不调用 exit:
  - 程序会继续执行到 main 函数结束
  - 可能导致未定义行为
  - 父进程无法知道子进程是否正确执行

### 4. 为什么需要 include 这三个

有了他们，下面的代码才拥有 1.使用基本数据类型 2.使用[[文件系统]] 3.调用[[system call]]的能力

```c
// 定义了基本数据类型，比如:uint (无符号整型)uint64 (64位无符号整型)uchar (无符号字符) 这些是系统编程常用的类型定义

#include "kernel/types.h"

// 定义了文件系统相关的结构体和常量, 包含文件状态信息(比如文件大小、创建时间等), 虽然在 sleep 程序中没有直接使用，但这是 xv6 用户程序的标准包含文件

#include "kernel/stat.h"

//定义了用户程序可以使用的系统调用接口, 包含了 sleep、fork、exit 等函数的声明

#include "user/user.h"
```

## 2. pingpong (easy)

需要实现一个使用管道在父子进程间传递字节的程序:

```c
#include "kernel/types.h"
#include "kernel/stat.h"
#include "user/user.h"

int main() {
    int p1[2], p2[2]; // Two pipes
    char buf[1];
    
    pipe(p1);
    pipe(p2);
    
    if(fork() == 0) {
        // Child process
        close(p1[1]);
        close(p2[0]);
        
        read(p1[0], buf, 1);
        printf("%d: received ping\n", getpid());
        
        write(p2[1], "x", 1);
        exit(0);
    } else {
        // Parent process  
        close(p1[0]);
        close(p2[1]);
        
        write(p1[1], "x", 1);
        read(p2[0], buf, 1);
        printf("%d: received pong\n", getpid());
        
        wait(0);
        exit(0);
    }
}
```

### 为啥 fork() == 0是子进程反之是父

fork() 的特点：调用一次，返回两次

- 在父进程中返回子进程的 PID（大于 0）
- 在子进程中返回 0

### read 和 write 方法的第三个参数啥意思，为啥都是 1

在 read 和 write 函数中，第三个参数表示要读取或写入的字节数。

```c
// 从文件描述符 fd 读取最多 n 个字节到 buf
ssize_t read(int fd, void *buf, size_t n);

// 向文件描述符 fd 写入 buf 中的 n 个字节
ssize_t write(int fd, const void *buf, size_t n);
```

## 3. find (moderate)

实现一个简单版本的 UNIX find 命令。根据文档([1](https://pdos.csail.mit.edu/6.S081/2020/labs/util.html))的提示:

1. 参考 `user/ls.c` 来了解如何读取目录
2. 使用递归来遍历子目录
3. 不要递归进入 "." 和 ".."
4. 使用 strcmp() 来比较字符串

```c
#include "kernel/types.h"
#include "kernel/stat.h"
#include "user/user.h"
#include "kernel/fs.h"

void find(char *path, char *name) {
    char buf[512], *p;
    int fd;
    struct dirent de;
    struct stat st;

    if((fd = open(path, 0)) < 0) {
        fprintf(2, "find: cannot open %s\n", path);
        return;
    }

    if(fstat(fd, &st) < 0) {
        fprintf(2, "find: cannot stat %s\n", path);
        close(fd);
        return;
    }

    switch(st.type) {
        case T_FILE:
            if(strcmp(path + strlen(path) - strlen(name), name) == 0) {
                printf("%s\n", path);
            }
            break;
        case T_DIR:
            if(strlen(path) + 1 + DIRSIZ + 1 > sizeof buf) {
                printf("find: path too long\n");
                break;
            }
            strcpy(buf, path);
            p = buf+strlen(buf);
            *p++ = '/';
            while(read(fd, &de, sizeof(de)) == sizeof(de)) {
                if(de.inum == 0)
                    continue;
                if(strcmp(de.name, ".") == 0 || strcmp(de.name, "..") == 0)
                    continue;
                memmove(p, de.name, DIRSIZ);
                p[DIRSIZ] = 0;
                find(buf, name);
            }
            break;
    }
    close(fd);
}

int main(int argc, char *argv[]) {
    if(argc != 3) {
        fprintf(2, "Usage: find path name\n");
        exit(1);
    }
    find(argv[1], argv[2]);
    exit(0);
}
```

我建议按照这个顺序逐个实现,每完成一个就测试一下。文档中提供了详细的测试用例([1](https://pdos.csail.mit.edu/6.S081/2020/labs/util.html))。

需要我继续说明 xargs 和 primes 的实现吗？



[CS/Q2：做CS的经典lab/MIT 6.828/lab/Lab2 System calls.md]
====================================================

---
draw:
tags: []
title: Lab2 System calls
date created: 2024-12-18
date modified: 2024-12-27
---

```Java
```

## 一、System call tracing 思路

### 1. 为什么要做系统调用追踪

在后续调试各种功能（尤其是内核相关功能）时，如果能在内核态"看见"当前进程调用了哪些系统调用、返回值是什么，会非常方便排查 Bug。因此，Lab2 的第一个子实验要求你为 xv6 添加一个 `trace` 系统调用，用于打开/关闭对应进程的"系统调用追踪"开关，并在追踪打开时打印出所有被追踪的系统调用的**名称**、**返回值**以及**发起调用的进程 PID**。

### 2. 实验大致做法

1. **添加系统调用 `trace`**
    
    - 在 `kernel/syscall.h` 中定义 `SYS_trace` 的编号。
    - 在 `kernel/syscall.c` 中把新的编号映射到你写好的内核处理函数 `sys_trace()`。
    - 在 `kernel/sysproc.c` 里实现 `sys_trace()` 函数，核心逻辑就是给当前进程（`myproc()`）的 `syscall_trace` 赋值。
    - 在用户态则需要在 `usys.pl`、`user.h` 等处为 `trace` 添加跳板函数和声明，如此才能从用户态方便地调用 `trace(mask)`。
2. **在 `proc.h` 中为进程添加一个 `syscall_trace` 字段**  
    这个字段用来存储"需要追踪哪些系统调用"的 bitmask，比如如果你传入的 mask 的某一位是 1，就代表需要追踪对应编号的系统调用。
    
    - 父进程在 fork 的时候会把这个 `syscall_trace` 传给子进程，这样父进程设置了 trace 之后，子进程也自动开启对应的追踪。
3. **在 `syscall()` 这个"统一处理入口"里去打印日志**  
    xv6 中所有系统调用都会在内核态走到 `syscall(void)` 函数，所以要做"哪几个系统调用被追踪并打印"的工作，自然就放在这里最好。
    
    - 先根据寄存器 `a7` 拿到系统调用编号 `num`。
    - 调用正确的内核处理函数，得到系统调用的返回值 (存入 `p->trapframe->a0`)。
    - 如果进程的 `syscall_trace` 对应 bit 置位，就通过 `printf` 打印出 `pid`, `syscall name`, `return value` 等信息。

## trace 的实现效果

在你完成 **trace** 系统调用并编译好 xv6 之后，你的 shell 里会多出一个用户态的可执行程序（通常叫 **`trace`**），用法大致是：

```Java
trace <mask> <command> [<args>...]
```

- **`<mask>`**：一个整数，里面的各个 bit 控制是否追踪对应编号（`SYS_xxx`）的系统调用。
- **`<command>`**：要执行的命令或程序。
- **`<args>...`**：该命令需要的参数。

**当 trace 启动 `<command>` 时，只要 `<mask>` 对某个系统调用的编号进行"位掩码"匹配，这个系统调用就会被跟踪，并在命令执行期间每次系统调用返回前打印一行**。输出格式类似：

```Java
<进程ID>: syscall <系统调用名> -> <返回值>
```

下面给你举一些在 xv6 shell 里可能看到的例子，方便了解 trace 实际如何工作。

---

### 1. 只追踪一个系统调用（比如 `read`）

假设在 xv6 内核的 `kernel/syscall.h` 中，`SYS_read` 的编号是 5，那么它的位掩码就是 `1 << 5 = 32`。于是：

```sh
$ trace 32 grep hello README
```

这会只追踪 `SYS_read`。示例输出可能是：

```Java
3: syscall read -> 1023
3: syscall read -> 966
3: syscall read -> 70
3: syscall read -> 0
3: syscall close -> 0
```

> 这里显示进程 PID 为 3，`read` 每次返回了多少字节，最后还有一次 `close`（如果你也把 `close` 的掩码打开，就会显示它的踪迹）。

由于这里只设置了掩码 `32`（即只跟踪 `read`），所以 `exec`、`open` 等其他系统调用并不会输出。

---

### 2. 跟踪所有系统调用

如果你想看某个程序的**所有系统调用**，可以把 31 位都置为 1，比如 `2147483647 = 0x7fffffff`，这是一个常见的"所有位都打开"的掩码：

```sh
$ trace 2147483647 grep hello README
```

输出示例（省略部分）：

```Java
4: syscall trace -> 0
4: syscall exec -> 3
4: syscall open -> 3
4: syscall read -> 1023
4: syscall read -> 966
4: syscall read -> 70
4: syscall read -> 0
4: syscall close -> 0
...
```

> 可以看到这时会打印 `trace`, `exec`, `open`, `read`, `close` 等所有调用及它们的返回值。

---

### 3. 不追踪任何系统调用

如果 `<mask>` 是 0，那么不会打印任何跟踪信息：

```sh
$ trace 0 grep hello README
```

此时 grep 照常执行，但不会输出任何 "syscall … -> …" 行。

---

### 4. 追踪 fork 及其子进程的 fork

假设 `SYS_fork` 的编号是 1，那么掩码就是 `1 << 1 = 2`。例如，有个测试程序叫 `forkforkfork`（会连续地 fork 好几次）：

```sh
$ trace 2 usertests forkforkfork
usertests starting
test forkforkfork: 
407: syscall fork -> 408
408: syscall fork -> 409
409: syscall fork -> 410
410: syscall fork -> 411
409: syscall fork -> 412
410: syscall fork -> 413
409: syscall fork -> 414
411: syscall fork -> 415
...
```

因为你在父进程中打开了 trace，同时在 `fork()` 的实现里会把 `trace mask` 复制到子进程，所以后面所有子进程也继续带着同样的跟踪掩码——只要它们调用 `fork`，就会打出类似的行。

---

### 5. 追踪多个系统调用

如果你想一次追踪多个系统调用，比如 `fork` (`SYS_fork`) 和 `read` (`SYS_read`)，可以把对应的位掩码"或"起来。举例：

- `SYS_fork` 编号是 1 → `1 << 1 = 2`
- `SYS_read` 编号是 5 → `1 << 5 = 32`

那么 "2 | 32 = 34" 就会同时追踪 `fork` 和 `read`。你可以在 shell 中：

```sh
$ trace 34 somecommand ...
```

---

### 总结

- **`trace <mask> <command>`** 会启动 `<command>` 并对 `<mask>` 指定的系统调用进行跟踪。
- **输出格式**：`PID: syscall xxx -> return_value`。
- **子进程继承**：一旦某进程调用了 `trace(mask)`, 该进程后续的子进程也会带着相同的 `mask`。

通过这些示例，你就能看到 **trace** 在 xv6 shell 里具体如何工作、以及它如何帮助你调试或观察系统调用的执行状况。祝你实验顺利!



[CS/Q2：做CS的经典lab/MIT 6.828/lab/Lab3 Page tables.md]
===================================================

---
draw:
tags: []
title: Lab3 Page tables
date created: 2024-12-25
date modified: 2024-12-27
---

## ## A kernel page table per process ([hard](https://pdos.csail.mit.edu/6.S081/2020/labs/guidance.html))

### 为什么要对进程的页表做四次映射

```c
  kvmmapkern(pagetable, UART0, UART0, PGSIZE, PTE_R | PTE_W);
  kvmmapkern(pagetable, VIRTIO0, VIRTIO0, PGSIZE, PTE_R | PTE_W);
  kvmmapkern(pagetable, CLINT, CLINT, 0x10000, PTE_R | PTE_W);
  kvmmapkern(pagetable, PLIC, PLIC, 0x400000, PTE_R | PTE_W);

  return pagetable;
```

这四次映射是为了让进程能访问不同的硬件设备，每个映射对应一个不同的设备：
UART0 (Universal Asynchronous Receiver/Transmitter)  
kvmmapkern(pagetable, UART0, UART0, PGSIZE, PTE_R | PTE_W);  
串口通信设备  
用于控制台输入输出  
比如 printf 就需要通过 UART 输出

VIRTIO0  
kvmmapkern(pagetable, VIRTIO0, VIRTIO0, PGSIZE, PTE_R | PTE_W);  
虚拟 I/O 设备接口  
用于磁盘操作  
比如文件读写需要通过这个接口

CLINT (Core Local Interruptor)  
kvmmapkern(pagetable, CLINT, CLINT, 0x10000, PTE_R | PTE_W);

- 处理器本地中断控制器  
用于定时器中断  
比如进程调度需要用到

PLIC (Platform-Level Interrupt Controller)  
kvmmapkern(pagetable, PLIC, PLIC, 0x400000, PTE_R | PTE_W);  
平台级中断控制器  
处理外部设备中断  
比如键盘、磁盘等设备的中断

这些映射是必需的，因为：

1. 每个设备都有其特定的物理地址
2. 内核需要通过这些地址访问设备
3. 没有这些映射，内核就无法与设备通信

就像一个城市需要：

- 港口（UART）用于对外通信
- 仓库（VIRTIO）用于存储
- 警报系统（CLINT）用于定时
- 交通控制（PLIC）用于协调  
每个设备都有其特定用途，缺一不可。



[CS/Q2：做CS的经典lab/MIT 6.828/lab/Lab4 traps.md]
=============================================

---
draw:
tags: []
title: Lab4 traps
date created: 2024-12-25
date modified: 2024-12-27
---

先来回顾一下这个 **backtrace** 函数做了什么，再解释它为什么这样写——主要和 **RISC-V 的函数栈帧布局**、**xv6 内核栈大小** 以及 **如何检测栈回溯的终点** 有关。

```c
void backtrace(void)
{
  uint64 cur_fp = r_fp();       // 1. 获取当前函数的帧指针 (s0)
  while(cur_fp != PGROUNDDOWN(cur_fp))  // 2. 检查是否超过当前栈页的边界
  {
    printf("%p\n", *(uint64 *)(cur_fp - 8));    // 3. 打印返回地址 (ra)
    cur_fp = *(uint64 *)(cur_fp - 16);          // 4. 更新 fp 为上层函数的帧指针
  }
}
```

让我们分步骤看其中的关键点。

---

## 1. RISC-V 函数栈帧布局

按照 GCC 在 RISC-V 平台上的调用约定，每个函数的大致栈帧 (frame) 结构如下（从高地址到低地址）：

```Java
                high addresses
                +-------------------+
                |   ...            |  (可能还有函数的局部变量)
 (frame base) ->+-------------------+
       s0(fp) ->|  old s0 (上一个函数的帧指针)  |  (存放在 fp-16)
                |  return address (ra)         |  (存放在 fp-8)
                +-------------------+
                |   ...            |
                low addresses
```

- **s0 (fp)**：当前函数的帧指针寄存器。
- **(fp - 8)**：存放函数的返回地址 (ra)，用来在函数结束时跳回调用者。
- **(fp - 16)**：存放上一层函数的帧指针 (old s0)，用来回溯调用栈。

在 xv6 中，为了做 backtrace，就需要：

1. 读出当前函数的帧指针。
2. 从 `(fp - 8)` 拿到当前函数保存的返回地址 (ra)，打印它。
3. 再从 `(fp - 16)` 拿到前一个函数的帧指针，继续往上层函数回溯。

---

## 2. 为什么用 `while (cur_fp!= PGROUNDDOWN(cur_fp))` 判断结束？

xv6 的内核线程栈大小是一页(4KB)，是 **页对齐** 的。也就是说，每个内核线程有一页专门做它的内核栈，起始地址是 `PGROUNDDOWN(stack_top)`，结束地址是 `PGROUNDUP(stack_top)`。

当我们回溯栈帧时，一旦 `cur_fp`（当前帧指针）**走到或跨过这页的边界**，就说明已经超出了这条内核栈的范围，再继续回溯就会访问到不属于该栈或者无效的内存区域，可能会崩溃或读到垃圾数据。

- `PGROUNDDOWN(x)`：把地址 `x` 下舍到所在的页边界。例如，如果 x 在 0xABCDEF，那么它对应页边界可能是 0xABCDE000(假设 4KB 一页)。
- 所以 `cur_fp!= PGROUNDDOWN(cur_fp)` 这个判断相当于 "**只要 `cur_fp` 还不是等于它所在页的起始地址，就继续回溯**"。若相等，说明我们到底了，必须停下来。

也有人会采用其他判断方式，比如"只要 `cur_fp` 落在[stack_bottom, stack_top]之间就继续"，原理类似，都是为了避免访问越界。

---

## 3. 打印返回地址并回溯到上一帧

在循环里：

1. `*(uint64 *)(cur_fp - 8)`：这正是栈帧中的返回地址 ra。
2. `cur_fp = *(uint64 *)(cur_fp - 16)`：将当前帧指针更新为"上一个函数"的帧指针 old s0。

周而复始，把每一层函数的返回地址都打印出来，就得到了「调用栈回溯」的地址列表。

在 GDB 里，你看到的 "backtrace" 命令做了类似的事情，只不过它还会把地址映射到具体的函数名、行号（通过符号表 / 调试信息）。

---

### 总结一下

- **帧指针 (fp = s0)**：RISC-V / GCC 的函数约定把老的 fp 和 ra 分别存在 `fp-16`、`fp-8`。
- **循环回溯**：从当前帧指针往回一个个找老的 fp，并打印 ra。
- **终止条件**：在 xv6 里，每个内核栈是一页大小，一旦 `cur_fp` 减小到等于该页的下边界(`PGROUNDDOWN`)，说明已经退到栈底了，必须停止。否则就越界了。

所以，这就是为什么 backtrace 会以这种形式实现：**因为栈帧布局固定**，并且 **xv6 每条内核栈只有一页**，故用这样的简单循环即可实现堆栈回溯。

## 为什么需要 alarm 打断

- **操作系统要"分配时间"给大家**  
    你的电脑可能同时要运行很多程序，比如游戏、浏览器、音乐播放器。要是其中一个程序一直霸占着 CPU，不让出时间，其他程序就没法顺利工作。
- **时钟"叮"一下，是给操作系统发个信号**  
    操作系统设置了一个"小闹钟"（硬件定时器），让它定时"叮"一下，提醒操作系统："诶，现在过了一个时间片了，你要不要换个程序来跑啊？"
- **被时钟"打断"，进入内核**  
    当这个小闹钟响了，CPU 立刻暂停正在运行的用户程序，跳到内核里运行一段"特殊程序"（时钟中断处理例程）。这个程序会决定是否要让当前进程再继续跑，还是切换到别的进程去跑，或者做一些别的周期性工作（比如维护系统时间等）。
- **然后再回去**  
    处理完以后，操作系统又把 CPU 切回给某个进程（可能是原来那个，也可能是别的）。于是，用户程序继续从刚才被打断的地方继续运行。
**防止重复 alarm**：处理闹钟函数时，不要再被闹钟打断，否则会乱套。

下面给出一个整体的思路指南，帮助理解和实现这个 Lab 的关键步骤和原理。各个部分互相关联，需要在阅读 xv6 相关源码 (trap.c、trampoline.S、syscall.c、proc.c 等) 和 Lab 文档的基础上逐步实现。

---

## 1. 理解 RISC-V 汇编与栈帧

Lab 的第一部分，先让你去阅读并理解一些 RISC-V 汇编代码，以及函数调用过程中的寄存器使用、压栈和回栈逻辑。这为后面做 backtrace（回溯调用栈）和手动保存/恢复寄存器（alarm 机制）做准备。

- **函数参数与返回值**  
    RISC-V 调用约定中，a0~a7 作为函数参数和返回值寄存器；ra (return address) 用来存储返回地址；s0 (fp/frame pointer) 用来存储当前函数的帧指针等。
    
- **函数调用的 inlining**  
    C 编译器有时会把一些小函数"内联"到调用处，导致在汇编里看不到对应的 `call` 指令。需要仔细查看生成的汇编(`.asm`)以确定哪些函数被真正调用、哪些被内联。
    
- **小端与大端**  
    RISC-V 是小端序 (little-endian)，这会影响到像 `0x00646c72` 这样的数在内存中的存储顺序，以及在 printf 时如何被解释成字符串。如果切换到大端序，需要调整字节顺序。
    

---

## 2. Backtrace（回溯调用栈）

在 xv6 中，如果发生错误或想要调试，就希望能打印出内核函数的调用栈。为了拿到调用栈上的各个返回地址，需要：

1. **了解栈帧结构**
    
    - 每个函数在进入时，会把前一个函数的帧指针 (s0) 和返回地址 (ra) 等压栈，然后更新 s0，构造新的栈帧。
    - 对于 RISC-V/GCC，`s0` (fp) 指向当前栈帧的底部。
        - `(fp - 8)` 处存返回地址 `ra`；
        - `(fp - 16)` 处存老的 fp；
        - 然后再往下是函数的局部变量区域等。
2. **在内核里实现 `backtrace()` 函数**
    
    - 可以在 `kernel/printf.c` 添加一个函数 `backtrace()`，利用内联汇编或者写一个 `r_fp()` 读取当前 `s0` (即当前帧指针)。
    - 循环地根据内存结构，依次找出保存的返回地址、前一个帧指针，再往上回溯，直到超过该内核栈所在的地址区间 (例如用 `PGROUNDDOWN(fp)` 和 `PGROUNDUP(fp)` 来确定栈的边界)。
    - 每找到一个返回地址，就用 `printf` 打印出来。
3. **调用 backtrace()**
    
    - 在 `sys_sleep()` 或者在 `panic()` 等函数中调用 `backtrace()`，便可以观察到函数调用栈。

---

## 3. Alarm (用户态定时"中断"/陷入处理)

这一部分让你在用户空间里模拟出一个"信号"或"定时器"处理机制：进程在用户态运行一段时间后，被时钟中断打断，进入内核，然后从内核"返回"到用户态时，跳转执行一个用户态的"handler"函数；用户态"handler"执行完后，再返回原来的代码继续执行。

### 3.1 新增系统调用

- **`sigalarm(int ticks, void (*handler)())`**  
    用来设置：
    
    1. `interval`：进程每运行 `ticks` 个时钟周期后，就触发一次"定时报警"。
    2. `handler`：报警时要执行的用户态函数。
    

    如果传入 `(0, 0)`，表示取消报警。

    
- **`sigreturn()`**  
    用户态 `handler` 执行完后，要通过 `sigreturn()` 返回原来的执行现场，继续执行被打断的用户程序。
    

要做的事：

1. 在 `user/user.h` 中声明函数原型。
2. 在 `kernel/syscall.c` 和 `kernel/syscall.h` 里添加对应的 syscall 编号和分发逻辑。
3. 在 `kernel/sysproc.c` 中实现 `sys_sigalarm()` 和 `sys_sigreturn()` 函数体。
4. 在 `proc.h` 的进程结构 `struct proc` 里加几个字段，用来记录：
    - `alarm_interval`：`sigalarm(ticks, handler)` 传入的间隔。
    - `alarm_handler`：指向用户传入的函数地址。
    - `alarm_ticksleft`：距离下次触发 alarm 还剩多少 tick。
    - 以及防止递归重入的标志位，如 `alarm_in_handler` 表明当前是否已经在执行 alarm handler。

### 3.2 在内核捕获时钟中断并切换到用户 handler

时钟中断发生在 `trap.c` 的 `usertrap()` 里，每次时钟中断 (`which_dev == 2`) 都会让当前正在运行的进程 `p->alarm_ticksleft` 自减。若减到 0，表示需要调用用户态 alarm handler 了：

1. **判断是否已经在 handler 里**
    
    - 如果已经在 handler 里了，就不要再进，以防止在 handler 里又被时钟打断，再次进 handler，陷入无限递归。
2. **保存现场**
    
    - 这里的"现场"包括所有通用寄存器（a0~~a7、t0~~t6、s0~s11、ra、sp 等）、原始的程序计数器 (PC) 等。
    - xv6 提供了 `struct trapframe` 记录用户态大部分寄存器，但你需要确保把"足以恢复"的信息都保存下来。尤其是 PC、ra、sp、s0 等对后面恢复很关键的寄存器。
    - 将这些寄存器保存到 `proc` 里专门为 alarm 准备的保存区，比如可以再额外加一个 `struct trapframe alarm_trapframe`，或者把需要保存的字段先缓存。
3. **修改 trapframe**
    
    - 把 `trapframe->epc`（即下一条将要执行的指令）改为 `alarm_handler` 的地址，这样在 `usertrap()` 返回用户态时，会从 `alarm_handler` 的入口执行。
    - 同时可以把 `alarm_ticksleft` 重置成 `alarm_interval`，这样过了 interval 之后又会触发下一次 alarm。

### 3.3 `sigreturn` 恢复现场

在用户态的 `handler` 函数执行完毕，会调用 `sigreturn()`。进入内核的 `sys_sigreturn()` 后，你需要：

1. **从 `proc` 中取出先前保存好的现场信息**
    - 将保存的所有寄存器值、PC 等还原到当前进程的 `trapframe`。
2. **将 `alarm_in_handler` 标志位清 0**，允许下次 alarm 发生。

这样在下一次 `usertrap()` 返回用户态时，就能回到原本被打断的指令处继续执行，好像什么都没发生过一样。

---

## 4. 关键注意点

- **防止重复进入 handler**  
    如果 handler 还没执行完就又发生时钟中断，要么忽略这次中断，要么累加计数但不调用新的 handler，以免出现严重错误或无限递归。
- **保存/恢复足够的寄存器**  
    如果漏保存某些寄存器值，handler 返回后用户程序可能会出现奇怪的 bug。
- **测试**
    - `alarmtest`：包括了 test0 / test1 / test2 等，逐渐会检查是不是能正确打印 `"alarm!"`、能否多次周期性地调用 handler、能否在 handler 中嵌套等。
    - `usertests`：检查 xv6 全部其他功能是否被破坏。

---

## 5. 提交与打包

- `make grade` 确保本 lab 所有测试都能通过。
- `make handin` 根据课程网站要求，上传你的提交。

---

### 总结

**实现思路简要概括**：

1. **前置准备**：理解 RISC-V 调用约定和栈帧结构。
2. **Backtrace**：利用 s0(fp) 链式回溯，每个函数的返回地址保存在 `fp-8`。循环向上直到越过当前栈的边界停止。
3. **Alarm**：
    - 新增 `sigalarm()`/`sigreturn()` 系统调用，扩展进程结构存储 alarm 的状态 (interval / handler / 是否在执行 handler / 剩余 ticks / 保存现场)。
    - 时钟中断时，在 `usertrap()` 判断是否到期执行用户态 handler；执行前保存寄存器现场，修改返回的 PC 指向 handler；handler 结束后用 `sigreturn()` 恢复原先被打断的上下文。
    - 重复此过程，实现周期性"闹钟"功能。

整体比较考验对系统调用流程、RISC-V trap 机制、栈帧布局和寄存器保存的理解。只要理清 trap→kernel→修改 trapframe→返回 user space 以及再从 user space→sigreturn→kernel→恢复 trapframe 这一来一回的过程，并 carefully 处理好寄存器保存/恢复，就能顺利完成。祝一切顺利!



[CS/Q2：做CS的经典lab/MIT 6.828/lab/信令风暴.md]
=======================================

---
draw:
tags: []
title: 信令风暴
date created: 2024-12-24
date modified: 2024-12-27
---

信令风暴是指大量设备同时向移动网络发送信令请求的现象,可能导致网络拥塞和服务中断。主要发生在:

1. 大量物联网设备频繁建立/断开连接
2. App频繁心跳包检测
3. 设备反复尝试重连
4. 恶意程序批量发送信令

常见影响:

- 网络信道拥堵
- 基站负载过高
- 用户无法正常通信
- 网络设备可能崩溃

防范措施包括限制连接频率、优化心跳机制、部署信令风暴检测系统等。



[CS/Q2：做CS的经典lab/MIT 6.828/lab/常用命令.md]
=======================================

---
draw:
tags: []
title: 常用命令
date created: 2024-12-18
date modified: 2024-12-27
---

```Java
0. 登录虚拟机，打开 terminal
cd /Users/liuyishou/usr/projects/MIT6.S081-2020fall`
make qemu



ps aux | grep qemu
kill <PID>

```

[ps](ps.md)



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/fork.md]
=======================================

---
draw:
tags: []
title: fork
date created: 2024-12-25
date modified: 2024-12-27
---

```Java
┌─────────────────────────────────────────────────────────────────────────────┐
│                  父进程 (在用户态) 调用 fork()                             │
└─────────────────────────────────────────────────────────────────────────────┘
              |
              | (触发系统调用，进入内核)
              v
┌─────────────────────────────────────────────────────────────────────────────┐
│  (父进程内核态) 执行 fork():                                              │
│                                                                           │
│  1) p = myproc();                   // 拿到当前(父)进程 p                 │
│  2) np = allocproc();               // 分配并初始化子进程 np              │
│  3) uvmcopy(...);                   // 拷贝父进程内存给子进程             │
│  4) np->trapframe->a0 = 0;          // "子进程"的返回值寄存器设为 0       │
│  5) for(i=0..NOFILE) filedup(...)   // 复制文件描述符给子进程            │
│     np->cwd = idup(p->cwd);         // 复制工作目录                      │
│     safestrcpy(np->name, p->name);  // 子进程继承名字                     │
│  6) pid = np->pid;                  // 记录子进程的PID                    │
│  7) np->state = RUNNABLE;           // 子进程置为就绪                      │
│  8) return pid;                     // 内核态返回 pid → 父进程用户态      │
└─────────────────────────────────────────────────────────────────────────────┘
              |
              | (切换回父进程用户态)
              v
┌─────────────────────────────────────────────────────────────────────────────┐
│ 父进程用户态得到 fork() == pid                                            │
│ (父进程继续执行自己的后续代码)                                            │
└─────────────────────────────────────────────────────────────────────────────┘


                             ---- (分割线) ----


┌─────────────────────────────────────────────────────────────────────────────┐
│    子进程此时并没有"执行 fork() 函数"                 │
│    它只是被内核放到了可运行队列 (RUNNABLE)，并还没真正运行。             │
└─────────────────────────────────────────────────────────────────────────────┘
                             |
                             | (调度器切换到子进程)
                             v
┌─────────────────────────────────────────────────────────────────────────────┐
│ 子进程"上 CPU"时，从用户态视角看，自己就像"从 fork() 返回"：              │
│                                                                           │
│    trapframe->a0 = 0   -->  fork() 的返回值==0                            │
│                                                                           │
│ 子进程其实是在"用户态的 fork() 调用点之后"继续执行，                       │
│ 自己看到 fork() 的返回值是 0                                             │
└─────────────────────────────────────────────────────────────────────────────┘

```

## 父进程视角

```c
// Create a new process, copying the parent.
// Sets up child kernel stack to return as if from fork() system call.
int
fork(void)
{
  int i, pid;
  struct proc *np;
  struct proc *p = myproc();  //Process p = Process.currentProcess();

  // Allocate process.
  if((np = allocproc()) == 0){
    return -1;  //返回 -1 表示错误
  }

  // Copy user memory from parent to child.
  // 复制内存空间 np.setMemory(p.getMemory().clone());
  if(uvmcopy(p->pagetable, np->pagetable, p->sz) < 0){
    freeproc(np);
    release(&np->lock);
    return -1;
  }
  np->sz = p->sz;

  np->parent = p; //np.setParent(p);

  np->mask = p->mask; //np.setMask(p.getMask());

  // copy saved user registers.
  *(np->trapframe) = *(p->trapframe);

  // Cause fork to return 0 in the child.
  // 修改子进程这边的返回值，
  // 父进程拿到的 fork() 返回值是 pid，子进程则是 0。这是 Unix fork() 的经典语义。
  np->trapframe->a0 = 0; 

  // increment reference counts on open file descriptors.
  for(i = 0; i < NOFILE; i++)
    if(p->ofile[i])
	  // child.openFiles[i] = new FileDescriptor(parent.openFiles[i])
      np->ofile[i] = filedup(p->ofile[i]);
  np->cwd = idup(p->cwd);

  safestrcpy(np->name, p->name, sizeof(p->name));

  pid = np->pid;

  np->state = RUNNABLE;

  release(&np->lock);

  return pid;
}
```

## 子进程视角

```c
// 在子进程 trapframe 中将 a0 设为 0
np->trapframe->a0 = 0;

```

这一切都只需要内核在**父进程的"fork"代码**里把子进程的运行现场（trapframe）"摆弄"好即可，子进程**不会**在内核里再去"走一遍"后续的赋值、循环、return 等 C 代码。

## 为什么子进程拿到的 fork() 返回值 0

1. RISC-V 架构中函数返回值保存在 `a0`
2. 这就让子进程的寄存器 `a0` 被强行置成 `0`。
3. 在 Unix 的进程模型中，"子进程"诞生时仅仅是「复制了父进程的地址空间和内核上下文」，自己**还没真正获得 CPU 执行时间**。它只有在内核把它标记为 `RUNNABLE` 之后，调度器才能在将来某个时刻"切换"到子进程，让它**从用户态**跑起来。
4. 等子进程要"返回到用户态"时，会把 `a0` 的值带到子进程的 C 调用栈环境，**从而子进程看到的 `fork()` 返回值是 0**。

也就是说，这些"拷贝内存、设置寄存器、设置返回值"的动作，**本质是父进程在干**，子进程并不需要"亲力亲为"。

因此，仅凭

```c
np->trapframe->a0 = 0;   // 对子进程设置返回值
return pid;             // 对父进程设置返回值
```

就能在父子两个进程中体现不同的返回值，达成 Unix `fork()` 的"一次调用、两次返回"效果。



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/kernel.md]
=========================================

---
draw:
tags: []
title: kernel
date created: 2024-12-25
date modified: 2024-12-27
---

```Java
. 
├── main.c        // lab1
├── console.c     // lab1
├── pipe.c        // lab1  进程通信，一个 pipe 有一个读端一个写端
├── printf.c      // lab1
├── sprintf.c     // lab1
├── start.c       // lab1
├── stats.c       // lab1
├── string.c      // lab1
├── uart.c        // lab1
├── exec.c        // lab2
├── proc.c        // lab2  进程
├── syscall.c     // lab2  系统调用
├── sysproc.c     // lab2
├── kalloc.c      // lab3, lab6
├── vm.c          // lab3  页表管理，Virtual Memory
├── vmcopyin.c    // lab3
├── plic.c        // lab4
├── trap.c        // lab4
├── spinlock.c    // lab5
├── bio.c         // lab8
├── file.c        // lab8
├── fs.c          // lab8
├── log.c         // lab8
├── sleeplock.c   // lab8
├── sysfile.c     // lab8
└── virtio_disk.c // lab8
```



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/pipe.md]
=======================================

---
draw:
tags: []
title: pipe
date created: 2024-12-22
date modified: 2024-12-27
---

so instead of one process writing data to a file and then another process reading that file，they can just send information back and forth through this pipe。因此，它们可以仅通过此管道来回发送信息，而不是一个进程将数据写入文件，然后另一个进程读取该文件。



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/system call.md]
==============================================

---
draw:
tags: []
title: system call
date created: 2024-12-23
date modified: 2024-12-27
---

```c
void
syscall(void)
{
  int num;
  struct proc *p = myproc();

  num = p->trapframe->a7;     // 从 a7 寄存器获取系统调用号
  if(num > 0 && num < NELEM(syscalls) && syscalls[num]) {
    p->trapframe->a0 = syscalls[num]();   // 执行系统调用并将返回值存在 a0
  } else {
    printf("%d %s: unknown sys call %d\n",
            p->pid, p->name, num);
    p->trapframe->a0 = -1;
  }
}
```

## 为什么从 a7 寄存器获取系统调用号？

这是用户程序和操作系统内核交互的关键接口。当用户程序需要进行系统调用时：

- 设置系统调用号到 a7
- 触发陷阱（trap）进入内核
- 内核执行这个 syscall() 函数
- 从 a0 获取返回值



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/xv6.md]
======================================

---
draw:
tags: []
title: xv6
date created: 2024-12-11
date modified: 2024-12-27
---

xv6是一个用于教学目的的操作系统，它是MIT为其操作系统课程(6.828)开发的。让我详细解释一下:

xv6是Unix Version 6(V6)的一个简化重写版本。Unix V6是1975年发布的经典Unix版本,而xv6则保留了V6的基本设计理念,但使用现代的C语言从头重写,并针对x86架构做了调整。

它的主要特点包括:

1. 代码简洁精炼 - 整个系统只有大约9000行代码,便于学习和理解操作系统的核心概念
2. 实现了基本的Unix功能:

- 进程管理
- 内存管理
- 文件系统
- 设备驱动
- 基本的系统调用

1. 教学价值高:

- 代码结构清晰
- 注释详尽
- 便于学生理解操作系统的工作原理
- 适合动手实践和修改

xv6最常被用于操作系统课程中,学生可以通过阅读和修改xv6的代码来学习:

- 进程调度
- 虚拟内存管理
- 文件系统实现
- 并发控制
- 系统调用机制

## 建议的阅读策略

- 初读时先抓主线：进程与调度（1-2章）、内存管理（3章）、系统调用（4章）、文件系统（8章），对大概框架有了认识后再细看其他章节细节（如设备驱动、日志、并发高级话题）。
- 若对 C 不熟，可先一边了解 C 基础一边阅读，必要时对照 xv6 的源码，运行 xv6（如在 QEMU 模拟器中）实际测试。
- 每章阅读后可尝试回答书后的练习题，以确认理解。
- 因为你有 Java 基础，可能已熟悉线程、同步等概念，所以在学习内核并发和锁机制时，可尝试类比 Java 的锁和线程模型，区别是操作系统是更低层、更贴近硬件的实现。

## 介绍

这本书是 MIT 6.S081/6.828 课程使用的 xv6 内核教材。它以一个名为 xv6 的教学用类 Unix 操作系统为例子，通过查看 xv6 源码和相应的讲解，让你理解操作系统的核心概念和机制。全书围绕操作系统的基本组成和工作原理展开，从进程、内存管理、系统调用、文件系统、设备驱动、中断与异常处理、调度、并发与锁、到日志（journal）与崩溃恢复等，都有详细剖析。通过阅读，你不仅能学习到传统 Unix 式接口和理念背后的思路，还能学到操作系统内核内部的结构和实现方法。

对于有一定 Java 基础但缺乏系统级 C 编程或操作系统背景的读者，可以按以下顺序和方法进行阅读和学习：

1. **准备工作**：
    在正式阅读前，先简单了解 C 语言基础（指针、数组、内存管理、结构体等），以及基本的计算机体系结构（寄存器、内存、CPU、指令执行）概念。对 RISC-V 架构可做简单了解（书中与 RISC-V 特定机制相关的点会有解释）。
    
2. **从整体概念入手（第1章：操作系统接口）**：
    首先阅读第1章，它介绍了操作系统为用户程序提供的接口（如进程、内存、文件、管道等）。这让你从用户角度理解操作系统的功能。即使你只熟悉 Java，也能通过这章了解基本的 Unix 编程模型（fork、exec、文件描述符、管道、文件系统接口等）。
    
3. **操作系统组织与进程（第2-3章）**：
    第2章介绍操作系统整体组织、用户态与内核态的转换、以及 xv6 的启动与第一个进程是如何诞生的。第3章则深入内存管理与页表概念。
    此时要多花时间理解进程地址空间和分页机制。这对理解后面章节的系统调用实现、中断陷入处理机制有帮助。
    
4. **陷入和系统调用（第4章）**：
    第4章重点在于陷入（系统调用、异常、外部中断）的硬件与软件协同。理解系统调用的路径，对后面阅读内核代码（如文件系统调用）很关键。
    
5. **中断、设备驱动和锁（第5-6章）**：
    第5章介绍硬件中断和简单的驱动程序。比如 UART（串口）与磁盘驱动器，对了解 OS 如何与硬件交互很重要。
    第6章的锁与并发控制很关键，因为内核是高度并发的。虽然 Java 有自己的并发机制，但是操作系统层面的锁（尤其是自旋锁、睡眠锁）及中断屏蔽策略是底层的基础。
    
6. **调度和同步（第7章）**：
    这一章讲解 CPU 如何在多个进程间切换并分配时间，深入理解调度器、上下文切换和进程状态变迁。在此基础上你会完全了解一个最小可行内核的多任务运行框架。
    
7. **文件系统（第8章及相关章节）**：
    文件系统的章节很多，内容较复杂，包括缓存（buffer cache）、日志（log）机制、inode、目录、路径名解析以及文件读写操作的底层实现。这是 Unix 操作系统的核心和精华所在，对于理解真实系统（如 Linux、BSD）的文件系统大有裨益。
    
8. **并发复习和附加议题（第9章及后续总结）**：
    第9章回顾内核的并发控制，对前面所学进行总结和进一步思考。第10章则总结全书的内容。
    

**建议的阅读策略**：

- 初读时先抓主线：进程与调度（1-2章）、内存管理（3章）、系统调用（4章）、文件系统（8章），对大概框架有了认识后再细看其他章节细节（如设备驱动、日志、并发高级话题）。
- 若对 C 不熟，可先一边了解 C 基础一边阅读，必要时对照 xv6 的源码，运行 xv6（如在 QEMU 模拟器中）实际测试。
- 每章阅读后可尝试回答书后的练习题，以确认理解。
- 因为你有 Java 基础，可能已熟悉线程、同步等概念，所以在学习内核并发和锁机制时，可尝试类比 Java 的锁和线程模型，区别是操作系统是更低层、更贴近硬件的实现。

通过上述顺序，你可以逐渐从用户层理解过渡到内核机制再到数据结构和并发的细节，从而对操作系统设计和实现获得全面的认识。



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/中断陷阱.md]
=======================================

---
draw:
tags: []
title: 中断陷阱
date created: 2024-12-16
date modified: 2024-12-27
---

下面是一段极为简化、概念性的 Java 代码，用来模拟 xv6 操作系统中"中断陷阱（interrupt/trap）"的过程。这段代码无关实际硬件，只是帮助理解概念。

在 xv6 中，当 CPU 在用户态执行用户程序时，如果发生中断（如时钟中断）或异常（如除零错误），CPU 会自动切换到内核态执行内核中的 trap 处理例程。处理例程根据中断号或异常类型采取相应的动作，然后再返回用户态继续执行或终止进程。

通过这段代码你可以看到以下概念：

- `UserCode` 类的 `runUserProgram()` 模拟用户程序运行。
- `TrapHandler` 类的 `handleTrap()` 模拟内核态的中断处理例程。
- 当用户代码中发生"事件"（这里用模拟的 `causeInterrupt()` 表达），就会调用 `TrapHandler.handleTrap()` 来处理。
- 处理完成后返回用户态继续运行用户程序。

```java
// 模拟用户程序的代码
class UserCode {
    public void runUserProgram() {
        System.out.println("[User] Running user code...");
        // 假设在用户程序运行时发生中断（如时钟中断）
        // 这里用一个函数来模拟中断的触发
        causeInterrupt();
        System.out.println("[User] User code resumes after interrupt handling.");
    }

    // 这个方法用于模拟用户态运行过程中发生中断（trap）
    // 实际xv6中中断由硬件和CPU模式自动完成，这里手动调用模拟
    private void causeInterrupt() {
        System.out.println("[User] An event occurred that requires kernel intervention (interrupt).");
        // 模拟陷入内核态的处理函数
        TrapHandler.handleTrap("TIMER INTERRUPT");
    }
}

// 模拟内核态的中断处理例程
class TrapHandler {
    public static void handleTrap(String trapType) {
        // 切换到内核态处理中断，这里用打印表示
        System.out.println("[Kernel] Entering kernel trap handler for: " + trapType);

        // 根据 trapType 不同，采取不同动作
        // 在真实的xv6中，这里会查看trap编号，然后执行相应的处理逻辑（例如时钟中断更新进程时间片，IO中断处理输入输出等）

        // 处理完毕后，返回用户态（这里直接返回上一级即可）
        System.out.println("[Kernel] Trap handling complete, returning to user space.");
    }
}

// 模拟系统启动及运行用户程序
public class Xv6InterruptTrapDemo {
    public static void main(String[] args) {
        // 在真实系统中：CPU最初在内核态启动，然后运行init进程，之后进程在用户态运行
        // 这里直接new一个用户程序示例来运行
        UserCode userCode = new UserCode();
        userCode.runUserProgram();
    }
}
```

**输出示例**（实际输出可能略有不同）：

```Java
[User] Running user code...
[User] An event occurred that requires kernel intervention (interrupt).
[Kernel] Entering kernel trap handler for: TIMER INTERRUPT
[Kernel] Trap handling complete, returning to user space.
[User] User code resumes after interrupt handling.
```

通过这段代码，你可以看到一个模拟流程：用户代码正在执行 → 遇到需要特权的事件（中断）→ 进入内核Trap处理函数（handleTrap）→ 内核处理完成后返回用户态继续执行。

在真正的 xv6 中：

- 中断发生时，硬件自动切换到内核态，压栈保存现场并跳转到特定的中断入口点。
- xv6 的中断服务例程（trap handler）根据中断类型（trap number）进行处理（参考 `trap()` 函数）。
- 完成后通过 `iret` 指令（x86）或相应的返回机制再回到用户态继续执行。

以上代码仅仅是一个高度抽象、概念化的示例，用来帮助你更直观地理解中断和陷入（trap）机制的基本工作方式。

## 陷阱类型

是的，在实际的 xv6 内核中，当出现陷阱（中断或异常）时，trap 处理函数会根据陷阱类型（trap number）来采取相应的策略。一般流程如下：

1. **统一入口点**：
    当 CPU 在用户态运行时发生中断、异常或系统调用陷阱时，会通过硬件机制（如中断描述符表 IDT）跳转到内核态的统一 trap 处理例程（在 xv6 中通常是 `trap()` 函数）。
    
2. **trap函数内部逻辑**：
    在 `trap()` 函数中，会查看当前 trap 的类型（通过 `tf->trapno` 获取陷阱号）。每种陷阱号都对应不同的事件，例如：
    
    - 时钟中断（时钟中断号），用于执行进程切换或更新调度信息。
    - 硬件设备中断（如 IDE 硬盘或 UART 串口中断），则会调用相应的驱动程序处理例程。
    - 系统调用陷阱（`T_SYSCALL`），会调用 `syscall()` 函数，根据系统调用号执行相应的系统调用逻辑。
    - 页故障异常（如 `T_PGFLT`），trap 会尝试处理缺页异常，如果无法处理则终止进程。
    - 其他异常（如除零错误 `T_DIVIDE`），根据情况进行处理，例如终止有问题的进程。
    

    换句话说，每种 trap 类型在 trap 函数里都有一套对应的处理策略和逻辑分支。

    
3. **最终返回**：
    处理完陷阱后，trap 例程会恢复上下文并返回到合适的地方（通常是原先的用户态进程继续执行，除非陷阱导致进程退出或切换）。
    

在你之前看到的 Java 概念代码中，为了简化，只用 `handleTrap()` 函数接收一个单一字符串类型的"trapType"进行处理。实际 xv6 内核中，这段逻辑会更加复杂和精细：

- trap 号（trap number）是枚举值（整数常量）。
- 根据 trap 号可以精确区分系统调用、时钟中断、键盘中断、缺页异常、算数异常等不同情况。
- 每种情况都会在代码中以 `if` / `switch` 等控制流结构处理相应的逻辑。

总结一下：是的，在 xv6 的陷阱处理框架中，所有可能的异常和中断类型在一个统一的处理函数（`trap()`）内会根据 trap number 进行判别，并针对各自的类型执行对应的处理策略。

## 为啥叫 trap

在操作系统设计领域中，"trap"一词有特定的技术含义。之所以使用"trap"这个名称，有以下原因和历史背景：

1. **历史命名和传统**：
    在早期的操作系统和计算机体系结构设计中，研究者和工程师将用户态与内核态之间的过渡机制称为"陷阱"（Trap）。这个词原本来自英文，意为"陷阱"或"诱捕"，含有使处理器"落入"内核控制的意向。也就是说，一旦用户程序触发了某种特殊的指令（如系统调用）、异常（如除零错误）或内存访问错误，CPU 就会"陷入"（Trap）到内核态的处理代码中。
    
2. **区别于中断(Interrupt)**：
    尽管"trap"与"interrupt"在处理流程上有相似之处（都需要切换到内核态的处理程序），但二者侧重点不同：
    
    - **Interrupt(中断)**：通常指来自外部硬件设备的异步事件（如定时器中断、IO设备完成中断）。
    - **Trap(陷阱)**：则更强调来自运行指令流本身的同步事件，如执行系统调用指令、遇到非法指令、算术异常或缺页异常。当 CPU 在执行指令的过程中"被迫"跳转到预定义的处理例程时，这个过程就叫做 trap。
3. **统一异常和系统调用处理**：
    在许多架构和操作系统中，系统调用和异常都通过 trap 机制统一进行处理。系统调用在本质上是用户程序有意通过一条特殊的"陷入"指令（如 `int` 指令、`syscall` 指令等）请求内核服务，从而使 CPU 从用户态进入内核态运行。这和异常处理（无意但同步产生的事件）在技术细节上有相似点，因此也常统一称为 trap。
    
4. **抽象层次**：
    "Trap"概念将所有导致模式切换的同步事件统称为陷阱，减少了对异常、系统调用、错误等概念的区分困难。程序员和操作系统设计者只需记住：当 CPU 遇到某些特殊条件（无论是故意的系统调用还是无意的异常）时，会"陷入"内核并在 `trap()` 函数内统一处理。
    

简单来说，"trap"这个名称强调的是：用户程序或CPU在执行一条指令时，遇到特定条件而导致执行路径被"捕获"到内核中进行特殊处理，无论是用户主动请求的系统调用，还是程序错误导致的异常，这个动作的核心点在于 CPU 的控制流被捕获、转移到一个更高权限（内核）的执行环境，因此这个过程就叫做"trap"。

## 除零异常

在 xv6 中，除零错误对应的陷阱类型通常为 `T_DIVIDE`。在 `trap.h` 文件中有如下定义（以 xv6 源码为例）：

```c
#define T_DIVIDE         0    // divide error
```

也就是说，除零错误（divide error）的陷阱号（trap number）是 0，即 `T_DIVIDE = 0`。



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/懒分配.md]
======================================

---
draw:
tags: []
title: 懒分配
date created: 2024-12-24
date modified: 2024-12-27
---

懒分配的反面是一划分就真给 如果一直没用 就会一直空着。



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/懒拷贝.md]
======================================

---
draw:
tags: []
title: 懒拷贝
date created: 2024-12-16
date modified: 2024-12-27
---

 copy on write类似享元模式

下面的 Java 示例代码无法真实还原 xv6 中内核级别的懒拷贝（Copy-On-Write, COW）机制，但可以通过面向对象的方式来帮助你理解这种思想。

在 xv6 中，懒拷贝主要出现在 `fork()` 系统调用中：当创建子进程时，并不立即将父进程的所有内存复制给子进程，而是让父子进程共享同一片内存区域（标记为只读）。只有当某一进程试图写入该共享内存时，才在那一刻实际进行拷贝，从而避免不必要的内存复制。

下面的 Java 代码用一个简化的模拟场景来说明该概念。我们定义了一个 `CopyOnWriteArray` 类，该类内部持有一个整数数组。当有多个 "引用"（类似父子进程）共享同一个底层数组时，如果任何一个引用尝试写入数组，它才会真正进行拷贝，确保对原数据的写入不会影响其他共享者。

```java
import java.util.Arrays;

class CopyOnWriteArray {
    private int[] data;
    // 引用计数或者共享标志，用于指示当前数组是否为共享状态
    // 在真正的COW中，这可能由内存页的只读权限和写时缺页异常来处理
    private boolean isShared;

    public CopyOnWriteArray(int size) {
        this.data = new int[size];
        Arrays.fill(data, 0);
        this.isShared = false;
    }

    // 私有构造，用于共享同一底层数组
    private CopyOnWriteArray(int[] data) {
        this.data = data;
        this.isShared = true;
    }

    // 模拟 "fork"：返回一个新的对象，该对象共享底层数组
    public CopyOnWriteArray forkLikeClone() {
        return new CopyOnWriteArray(this.data);
    }

    // 当需要写入时，如果是共享状态，则复制数组，从而分离自身与他者的共享
    public void set(int index, int value) {
        if (isShared) {
            // 只有当需要写入时，才真正拷贝数据，模拟"懒拷贝"
            data = Arrays.copyOf(data, data.length);
            isShared = false;
        }
        data[index] = value;
    }

    public int get(int index) {
        return data[index];
    }

    public void printData() {
        System.out.println(Arrays.toString(data));
    }

    public static void main(String[] args) {
        // 创建一个初始对象，类似父进程
        CopyOnWriteArray parent = new CopyOnWriteArray(5);
        
        // fork出"子进程"（实际上只是另一个对象引用）
        CopyOnWriteArray child = parent.forkLikeClone();

        System.out.println("初始状态（父进程数据）：");
        parent.printData();
        System.out.println("初始状态（子进程数据）：");
        child.printData();

        // 在子进程中进行写操作，此时才真正进行拷贝
        child.set(2, 99);

        System.out.println("写入后（父进程数据）：");
        parent.printData();
        System.out.println("写入后（子进程数据）：");
        child.printData();
    }
}
```

**代码解释：**

1. **初始创建：** `parent = new CopyOnWriteArray(5)` 会分配一个长度为5的数组，初始化全为0。
2. **forkLikeClone：** `child = parent.forkLikeClone()` 并未实际复制数组，而只是让 `child` 与 `parent` 共享同一个 `int[]data`，模拟 COW 中的父子进程共享相同内存。
3. **懒拷贝触发：** 当执行 `child.set(2, 99)` 时，`child` 检测到自己是共享状态，于是先拷贝数组，再在自己的独立副本中设置数据为99。此时 `parent` 和 `child` 不再共享同一个数组。
4. **结果对比：** `parent` 的数据未受影响，而 `child` 的数据已更新为 `[0, 0, 99, 0, 0]`。这样我们就模拟了 COW 的主要思想：只有在需要写入时才进行实际拷贝，从而节省资源。

通过这个示例，可以在抽象层面理解 xv6 中的懒拷贝：起初共享同一底层数据（类似同一块内存页），只有当写入时才进行真正的拷贝。这有助于减少不必要的复制开销。



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/文件系统.md]
=======================================

---
draw:
tags: []
title: 文件系统
date created: 2024-12-16
date modified: 2024-12-27
---

有两个关键概念，block 和 inode，块和inode。将块视为硬盘驱动器上的基本存储单元。一个固定大小的数据块，对吗？另一方面，一个索引节点就像一本书的索引卡。在我们的类库中，它保存了关于文件的基本信息，比如文件的大小、权限，以及最重要的是文件实际数据存储所在的那些块的地址。

both files and directories are represented by inodes. Yeah, the distinction lies in a special flag within the inode indicates its type. This flag tells by v 6 how to handle the inode, whether it should treat it as a regular file or as a directory.

在 xv6 中，文件系统的关键概念有：

- **Inode（索引节点）**：文件的元数据与数据块定位信息存放在 inode 中。
- **目录与文件**：目录是特殊的文件，它包含指向其他文件和子目录的条目。
- **打开文件**：通过路径查找（从根目录开始）找到对应文件的 inode，然后创建打开文件描述符。
- **读写操作**：通过在 inode 中的索引找到对应的物理数据块，对文件内容进行读写。

在下面的示例中：

- `FileSystem` 类模拟一个非常简单的文件系统。
- `Inode` 类模拟 inode，用来存储文件数据和文件名。
- `FileSystem.createFile()` 和 `FileSystem.openFile()` 模拟创建与打开文件的过程。
- `FileSystem.writeFile()` 和 `FileSystem.readFile()` 模拟对文件数据的写入和读取。

请注意，这只是概念演示，未涉及真正的磁盘操作、数据块分配、目录树遍历，以及权限、引用计数等复杂细节。

```java
import java.util.HashMap;
import java.util.Map;

// 模拟inode，用于存储文件的基本信息与数据
class Inode {
    String name;   // 文件名（简化处理，不含路径）
    StringBuilder data = new StringBuilder(); // 文件内容（简化起见，用字符串代替数据块）

    Inode(String name) {
        this.name = name;
    }
}

// 简化的文件系统类
class FileSystem {
    // 用Map模拟文件系统的索引结构，key为文件名（简化处理：无目录层级）
    // 在真实的xv6中，需要通过目录查询与遍历，这里直接用一个全局Map来管理文件
    private Map<String, Inode> inodes = new HashMap<>();

    // 创建文件（假设所有文件放在根目录）
    public boolean createFile(String filename) {
        if (inodes.containsKey(filename)) {
            System.out.println("[Kernel] File " + filename + " already exists.");
            return false;
        }
        Inode newInode = new Inode(filename);
        inodes.put(filename, newInode);
        System.out.println("[Kernel] File " + filename + " created.");
        return true;
    }

    // 打开文件，返回inode
    public Inode openFile(String filename) {
        Inode inode = inodes.get(filename);
        if (inode == null) {
            System.out.println("[Kernel] File " + filename + " not found.");
        } else {
            System.out.println("[Kernel] File " + filename + " opened.");
        }
        return inode;
    }

    // 写文件内容
    public void writeFile(Inode inode, String content) {
        if (inode == null) {
            System.out.println("[Kernel] Invalid inode.");
            return;
        }
        inode.data.append(content);
        System.out.println("[Kernel] Written data to file " + inode.name);
    }

    // 读文件内容
    public String readFile(Inode inode) {
        if (inode == null) {
            System.out.println("[Kernel] Invalid inode.");
            return null;
        }
        System.out.println("[Kernel] Read data from file " + inode.name);
        return inode.data.toString();
    }
}

public class Xv6FileSystemDemo {
    public static void main(String[] args) {
        FileSystem fs = new FileSystem();

        // 创建新文件
        fs.createFile("hello.txt");

        // 打开文件
        Inode helloInode = fs.openFile("hello.txt");

        // 写入文件数据
        fs.writeFile(helloInode, "Hello, xv6 filesystem!");

        // 读取文件数据
        String fileData = fs.readFile(helloInode);
        System.out.println("[User] File contents: " + fileData);

        // 尝试打开不存在的文件
        fs.openFile("nonexistent.txt");
    }
}
```

**示例输出：**

```Java
[Kernel] File hello.txt created.
[Kernel] File hello.txt opened.
[Kernel] Written data to file hello.txt
[Kernel] Read data from file hello.txt
[User] File contents: Hello, xv6 filesystem!
[Kernel] File nonexistent.txt not found.
```

通过这个示例代码你可以理解：

- 文件被创建后会在文件系统中有一个 inode 用来保存其内容和元数据（在此简化为文件名和字符串数据）。
- 打开文件时，通过文件名找到对应的 inode（在真实的 xv6 中则需要从根目录开始按路径查找目录和文件）。
- 写文件时把数据写入 inode 对应的数据区（在此用字符串代替实际的磁盘数据块）。
- 读文件时则返回 inode 中已经写入的数据。

在 xv6 中，文件系统还涉及到磁盘块分配、目录数据结构、缓存、日志、数据块指针（直接块、间接块）等复杂机制，这里未详细呈现。此 Java 示例只是从抽象层面帮助你理解 xv6 文件系统的基本概念和操作流程。



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/时钟.md]
=====================================

---
draw:
tags: []
title: 时钟
date created: 2024-12-25
date modified: 2024-12-27
---
- **操作系统要"分配时间"给大家**  
    你的电脑可能同时要运行很多程序，比如游戏、浏览器、音乐播放器。要是其中一个程序一直霸占着 CPU，不让出时间，其他程序就没法顺利工作。
- **时钟"叮"一下，是给操作系统发个信号**  
    操作系统设置了一个"小闹钟"（硬件定时器），让它定时"叮"一下，提醒操作系统："诶，现在过了一个时间片了，你要不要换个程序来跑啊？"
- **被时钟"打断"，进入内核**  
    当这个小闹钟响了，CPU 立刻暂停正在运行的用户程序，跳到内核里运行一段"特殊程序"（时钟中断处理例程）。这个程序会决定是否要让当前进程再继续跑，还是切换到别的进程去跑，或者做一些别的周期性工作（比如维护系统时间等）。
- **然后再回去**  
    处理完以后，操作系统又把 CPU 切回给某个进程（可能是原来那个，也可能是别的）。于是，用户程序继续从刚才被打断的地方继续运行。



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/用户态、内核态.md]
==========================================

---
draw:
tags: []
title: 用户态、内核态
date created: 2024-12-16
date modified: 2024-12-27
---

下面是一段非常简化、纯概念性质的 Java 代码示例，用以帮助理解 xv6 中用户态 (user space) 与内核态 (kernel space) 的概念。请注意，这段代码并非真实运行在 xv6 上，也无法真正体现硬件上的权限级别变化。它只是通过代码结构来表现"用户程序"与"内核函数"之间的调用关系，从而帮助理解 xv6 的用户态与内核态的基本思路。

在 xv6 中：

- 用户态的代码（用户程序）在受到内核严格控制的情况下执行，不可直接访问硬件和内核数据结构。
- 当用户态程序需要执行特权操作（如读写磁盘、创建进程、分配内存等）时，就需要通过系统调用 (system call) 的方式进入内核态，让内核代码替代用户程序执行特权操作，然后再返回用户态继续执行。

下面的示例代码中：

- `UserProcess` 类代表用户态的程序。
- `Kernel` 类代表"内核"提供的服务，只有在"系统调用"时才会进入它的方法中。
- `main` 方法中，用户程序通过 `Kernel.syscall()` 方法模拟进入内核执行特权操作，然后再返回用户态。

```java
// 内核类：模拟内核态代码
class Kernel {
    // 模拟内核提供的系统调用方法
    // 在真实的OS中，这个方法会有特权指令和内核数据访问
    public static void syscall(String operation) {
        // 此处仅是模拟：进入内核态
        System.out.println("[Kernel] Entering kernel mode to perform: " + operation);

        // 在这里可执行各类特权操作（如访问硬件、管理内存、文件系统操作）
        // ... 特权操作逻辑（模拟） ...

        // 完成操作后返回用户态
        System.out.println("[Kernel] Leaving kernel mode, returning to user space.");
    }
}

// 用户态类：模拟用户程序
public class UserProcess {
    public static void main(String[] args) {
        // 在用户态下运行的普通代码
        System.out.println("[User] Running in user space.");

        // 用户态无法直接执行特权操作，因此需要通过系统调用
        // 调用 Kernel.syscall 来进入内核态执行特权指令
        Kernel.syscall("Read from disk");

        // 内核完成特权操作后又返回用户态，继续执行用户的代码
        System.out.println("[User] Back to user space, continuing normal execution.");
    }
}
```

通过这段代码，你可以从概念上理解：

- `[User]` 前缀的打印代表用户态代码的执行位置。
- 用户代码需要特权操作时，通过调用 `Kernel.syscall` "陷入"到内核态（`[Kernel]` 前缀的打印）执行特权逻辑。
- 内核完成后返回用户态，继续用户程序的正常流程。

在真正的操作系统（如 xv6）中，这种转换由 CPU 的特权级别、内核栈、硬件中断和陷入机制来完成，而不是简单的 Java 方法调用。但这个示例可以帮助初学者从抽象的层面了解用户态与内核态的基本交互关系。



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/网络.md]
=====================================

---
draw:
tags: []
title: 网络
date created: 2024-12-16
date modified: 2024-12-27
---

需要先说明的是，xv6 本身是一个教学操作系统，在其标准版本中并没有内建的网络协议栈或网络功能。因此，这里无法呈现 xv6 原生网络相关的代码，因为 xv6 默认情况下并不提供完整的网络组件。本文所提供的代码只是一个极度简化的概念示例，用于帮助你理解一般操作系统中的网络通信思想，以及如果 xv6 要添加网络支持，大致会有怎样的模块与流程。

在一个简单的网络模型中，你可以想象以下步骤：

- 应用程序通过系统调用请求网络资源或发起网络连接（如"socket"操作）。
- 内核为应用程序分配网络套接字数据结构，并与网卡驱动交互。
- 当应用程序发送数据时，内核通过驱动程序将数据打包为网络数据包，并通过网卡发送到网络中。
- 当有数据包到来时，网卡产生中断，内核处理该中断，读取数据包，然后将数据递交给对应的进程中打开的套接字。

下面这段 Java 代码仅是从概念层面展示 "用户态程序" 通过某种系统调用访问 "内核网络栈" 并由 "驱动" 接口发送数据的流程。该示例不涉及实际的网络协议和数据传输，仅通过打印来模拟。

```java
// 模拟的“网络驱动”类：负责与“硬件”交互
class NetworkDriver {
    // 模拟发送数据包到“网络”
    public static void sendPacket(String packet) {
        System.out.println("[Driver] Sending packet to network: " + packet);
    }

    // 模拟接收数据包（实际中这将由中断触发并调用内核处理程序）
    public static void receivePacket(String packet) {
        System.out.println("[Driver] Packet received from network: " + packet);
        // 内核处理该数据包
        NetworkStack.handleIncomingPacket(packet);
    }
}

// 模拟的内核网络栈类：处理套接字和数据包的收发
class NetworkStack {
    // 简化处理：只维护一个全局的“连接”
    private static String buffer;

    // 模拟内核为用户程序提供的写数据的接口（如系统调用）
    public static void writeSocket(String data) {
        // 在真实系统中，会有路由、协议栈、打包等复杂过程
        // 这里直接将数据打包成一个“packet”
        String packet = "PACKET(" + data + ")";
        System.out.println("[Kernel] Preparing to send packet...");
        NetworkDriver.sendPacket(packet);
    }

    // 模拟内核处理收到的数据包
    public static void handleIncomingPacket(String packet) {
        // 拆包并存入内核缓冲（真实中有协议解析）
        buffer = packet;
        System.out.println("[Kernel] Packet stored in kernel buffer: " + buffer);
        // 通知用户态有新数据可读（实际通过阻塞唤醒或中断）
    }

    // 模拟用户从套接字读数据
    public static String readSocket() {
        System.out.println("[Kernel] Reading from kernel buffer...");
        String data = buffer;
        buffer = null;
        return data;
    }
}

// 模拟用户态程序
public class Xv6NetworkDemo {
    public static void main(String[] args) {
        System.out.println("[User] Trying to send data over network...");
        // 用户通过系统调用（在xv6中为trap进入内核）发送数据
        NetworkStack.writeSocket("Hello Network");

        // 模拟网络设备收到返回的数据包，驱动调用handleIncomingPacket
        NetworkDriver.receivePacket("REPLY(Hi from network)");

        // 用户读取返回数据（实际中用户态可能通过read()系统调用）
        String received = NetworkStack.readSocket();
        System.out.println("[User] Received data: " + received);
    }
}
```

**可能的输出**：

```Java
[User] Trying to send data over network...
[Kernel] Preparing to send packet...
[Driver] Sending packet to network: PACKET(Hello Network)
[Driver] Packet received from network: REPLY(Hi from network)
[Kernel] Packet stored in kernel buffer: REPLY(Hi from network)
[Kernel] Reading from kernel buffer...
[User] Received data: REPLY(Hi from network)
```

通过这个示例你可以看到：

- 用户程序通过一个内核提供的接口（如系统调用）发送数据 (`writeSocket`)。
- 内核网络栈将数据"打包"并通过驱动发送出去。
- 当有数据包到来时，驱动负责接收并调用内核相关函数处理数据包。
- 最终用户程序可以通过另一个系统调用（如 `readSocket`）从内核获取收到的数据。

在真正的 xv6 中，如果要实现网络，需要：

- 硬件设备驱动来控制网卡（NIC）。
- 中断和 trap 机制处理数据包的收发。
- 内核数据结构（如 socket、inode 或 file 机制）来对接用户进程的网络请求。
- 协议栈处理（如 TCP/IP 协议）解析数据包头、维护连接状态、可靠性传输等。

由于 xv6 官方并未实现这些组件，此示例仅作为概念层的说明，让你理解网络在操作系统架构中的典型位置与基本操作流程。



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/陷阱帧.md]
======================================

---
draw:
tags: []
title: 陷阱帧
date created: 2024-12-25
date modified: 2024-12-27
---

这就像：

- 你在工作（用户态）
- 突然有紧急会议（中断）
- 快速记下当前工作状态（保存到trapframe）
- 去开会（内核态）
- 会议结束后查看笔记（恢复trapframe）
- 继续之前的工作（返回用户态）



[CS/Q2：做CS的经典lab/MIT 6.828/xv6/页表.md]
=====================================

---
draw:
tags: []
title: 页表
date created: 2024-12-12
date modified: 2024-12-27
---

以下是一个示例，展示了如何用「省 / 市 / 县」来对应三层页表，并且直接在每一位上标注"这是省位、市位、县位"。为了便于演示，这里假设我们是 **32 位虚拟地址**，且使用了**三层页表**的典型切分方式：

- **顶层页表 (PDE)**：10 位——类比"省" Page Directory Entry（页目录项）
- **中间层页表 (PTE)**：10 位——类比"市" Page Table Entry（页表项）
- **页内偏移 (offset)**：12 位——类比"县"

这样，整个 32 位虚拟地址可分为：

```Java
[ PDE省(10 bits) | PTE市(10 bits) | offset县(12 bits) ]
```

下面给出一个具体的二进制示例，取一串虚拟地址的 32 位来演示（用空格分段）：

```Java
0101010101 1100110011 101010101010
↑         ↑          ↑           
省级PDE   市级PTE    县级offset
```

- **前 10 位 (0101010101)**：省级（PDE）
- **中 10 位 (1100110011)**：市级（PTE）
- **后 12 位 (101010101010)**：县级（offset）

如果你想"逐位"都标注上对应的"省 / 市 / 县"，可以写成：

1. **PDE（省）部分**：10 位

    ```Java
    省0  省1  省0  省1  省0  省1  省0  省1  省0  省1
    ```

（对应二进制 0 1 0 1 0 1 0 1 0 1）

    
2. **PTE（市）部分**：10 位

    ```Java
    市1  市1  市0  市0  市1  市1  市0  市0  市1  市1
    ```

（对应二进制 1 1 0 0 1 1 0 0 1 1）

    
3. **offset（县）部分**：12 位

    ```Java
    县1  县0  县1  县0  县1  县0  县1  县0  县1  县0  县1  县0
    ```

（对应二进制 1 0 1 0 1 0 1 0 1 0 1 0）

    

---

## 对应关系

- **"省"**（PDE）：确定**顶层页表**，先划定大范围
- **"市"**（PTE）：在"省"内再进一步定位市级范围
- **"县"**（offset）：在"市"里找到最终具体的"县"，也就是落到物理页框中的具体偏移

因此，整个地址翻译就像先选省，再选市，最后选到具体县一样，三级拆分后找到最终对应的物理内存位置。这样一来，每个比特都可以打上"省 / 市 / 县"的标签，看起来就很形象了。



[CS/Q2：做CS的经典lab/MIT6.824/Lab 1 MapReduce.md]
=============================================

---
draw:
tags: []
title: Lab 1 MapReduce
date created: 2024-08-07
date modified: 2024-12-27
---

N 个人去一个有 m 个坑位的大公共卫生间做清洁，该如何调度？这个实验的目标是实现一个分布式的 MapReduce 系统，包括协调器和工作进程，并确保它能正确处理并行任务、容错等情况。

<!-- more -->

## 运行

```bash
rm mr-out*
go run -race mrcoordinator.go pg-*.txt
go run -race mrworker.go wc.so
cat mr-out-* | sort | more
```

下面是典型的运行与测试 Lab 1 的步骤（假设你已经根据指导编写好 `mr/coordinator.go`, `mr/worker.go` 并有对应的代码实现）：

1. **编译插件（应用代码）**  
    首先需要编译你要运行的 MapReduce 应用插件（比如 word count 程序）。进入 `6.824/src/main` 目录，执行：

    ```bash
    cd ~/6.824/src/main
    go build -race -buildmode=plugin ../mrapps/wc.go
    ```

    成功后会生成 `wc.so` 文件。

    
2. **清理旧的输出文件**  
    运行前先确保没有残留的输出文件：

    ```bash
    rm mr-out*
    ```

3. **启动 coordinator**  
    在同一个目录下运行 `mrcoordinator.go`，并传入输入文件列表（比如 `pg-xxx.txt` 文件集）：

    ```bash
    go run -race mrcoordinator.go pg-*.txt
    ```

    该命令会启动一个 coordinator 进程，并监听来自 worker 的 RPC 请求。

    
4. **启动一个或多个 worker**  
    在另一个终端窗口中，同样在 `src/main` 目录下，启动 worker 进程，并加载你之前编译好的插件 `wc.so`：

    ```bash
    go run -race mrworker.go wc.so
    ```

    你可以多开几个终端窗口再运行数个相同命令，以模拟多个并发 worker。如果你的实现正确，coordinator 会分配任务给这些 worker，它们会处理完任务并写出最终的 `mr-out-X` 文件。

    
5. **查看输出结果**  
    当任务全部完成后（coordinator 进程会自动退出），你可以查看输出结果：

    ```bash
    cat mr-out-* | sort | more
    ```

    结果应与 `mrsequential.go` 单机版的输出一致。

    
6. **运行测试脚本**（可选）
    为了验证你的实现正确性，你还可以使用官方提供的测试脚本 `test-mr.sh`：

    ```bash
    bash test-mr.sh
    ```

    此脚本会依次测试 word-count、indexer、并行度测试以及崩溃恢复测试。如果全部通过，会显示：

    ```Java
    *** PASSED ALL TESTS
    ```

**总结**：
要取得实际执行的效果，你需要先编译插件，然后先启动 coordinator（给出输入文件），再启动 worker（加载插件），让 worker 请求并执行任务。最后，查看 `mr-out-*` 文件或运行官方测试脚本来验证结果。

## 资料

[6.5840 Lab 1: MapReduce (mit.edu)](https://pdos.csail.mit.edu/6.824/labs/lab-mr.html)

## 思想

[MapReduce](MapReduce.md)

## 入参

一些文章

```Java
.
├── pg-being_ernest.txt
├── pg-dorian_gray.txt
├── pg-frankenstein.txt
├── pg-grimm.txt
├── pg-huckleberry_finn.txt
├── pg-metamorphosis.txt
├── pg-sherlock_holmes.txt
├── pg-tom_sawyer.txt
```

## 处理

协调器和工作者的"main"例程位于 main/mrcoordinator.go 和 main/mrworker.go 中;不要更改这些文件。您应该将您的实现放在 mr/coordinator.go 、 mr/worker.go 和 mr/rpc.go 中。

## 出参

这些文章里单词的出现次数

## 代码流程

让我按照执行顺序来解释 MapReduce 的整个流程：

1. **初始化阶段**:

```go
// 1. Coordinator 启动并初始化
func MakeCoordinator(files []string, nReduce int) *Coordinator {
    c := Coordinator{
        files:       files,    // 输入文件列表
        nReduce:     nReduce,  // reduce任务数量
        nMap:        len(files), // map任务数量
        // ... 初始化各种channel
    }
    c.server()         // 启动RPC服务
    go c.schedule()    // 启动调度器
    return &c
}
```

2. **Map 阶段**:

```go
// 2.1 Worker不断发送心跳请求任务
func Worker(mapF func(string, string) []KeyValue, reduceF func(string, []string) string) {
    for {
        response := doHeartbeat()  // 请求任务
        switch response.JobType {
        case MapJob:
            doMapTask(mapF, response)  // 执行Map任务
        // ...
        }
    }
}

// 2.2 执行Map任务
func doMapTask(mapF func(string, string) []KeyValue, response *HeartbeatResponse) {
    // a. 读取输入文件
    content, _ := ioutil.ReadAll(file)
    
    // b. 调用用户定义的Map函数(wc.go中的Map)
    kva := mapF(fileName, string(content))
    
    // c. 按照key的hash值将结果分成nReduce份
    intermediates := make([][]KeyValue, response.NReduce)
    for _, kv := range kva {
        index := iHash(kv.Key) % response.NReduce
        intermediates[index] = append(intermediates[index], kv)
    }
    
    // d. 将中间结果写入文件
    // 每个Map任务会生成nReduce个中间文件
}
```

3. **Reduce 阶段**:

```go
// 3.1 所有Map任务完成后，Coordinator切换到Reduce阶段
func (c *Coordinator) schedule() {
    // ... Map阶段完成后
    c.initReducePhase()
}

// 3.2 Worker执行Reduce任务
func doReduceTask(reduceF func(string, []string) string, response *HeartbeatResponse) {
    // a. 读取所有相关的中间文件
    for i := 0; i < response.NMap; i++ {
        // 读取每个Map任务为当前Reduce任务生成的中间文件
    }
    
    // b. 按key归组
    results := make(map[string][]string)
    for _, kv := range kva {
        results[kv.Key] = append(results[kv.Key], kv.Value)
    }
    
    // c. 对每个key调用用户定义的Reduce函数(wc.go中的Reduce)
    for key, values := range results {
        output := reduceF(key, values)
        // 写入最终结果
    }
}
```

4. **任务协调**:

```go
// Coordinator通过心跳机制分配任务
func (c *Coordinator) selectTask(response *HeartbeatResponse) bool {
    // 1. 检查是否有空闲任务
    // 2. 检查是否有超时需要重新执行的任务
    // 3. 分配任务给Worker
    // 4. 检查是否所有任务都完成
}
```

关键流程：

1. Coordinator 初始化并加载输入文件
2. Worker 通过心跳获取任务
3. Map 任务处理单个输入文件，产生中间文件
4. Reduce 任务处理所有相关的中间文件，生成最终输出
5. 整个过程通过 RPC 进行通信和协调

这个实现类似于 Google 的 MapReduce 论文，但简化了一些。

## 文件解读

```sql

(base) liuyishou@MacBook-Pro-3 src % tree
.
├── go.mod
├── go.sum
├── kvraft
│   ├── client.go
│   ├── common.go
│   ├── config.go
│   ├── server.go
│   └── test_test.go
├── kvsrv
│   ├── client.go
│   ├── common.go
│   ├── config.go
│   ├── server.go
│   └── test_test.go
├── labgob
│   ├── labgob.go
│   └── test_test.go
├── labrpc
│   ├── labrpc.go
│   └── test_test.go
├── main
│   ├── diskvd.go
│   ├── lockc.go
│   ├── lockd.go
│   ├── pg-being_ernest.txt          0 pg*.txt是需要被统计单词数的原文件
│   ├── pg-dorian_gray.txt
│   ├── pg-frankenstein.txt
│   ├── pg-grimm.txt
│   ├── pg-huckleberry_finn.txt
│   ├── pg-metamorphosis.txt
│   ├── pg-sherlock_holmes.txt
│   ├── pg-tom_sawyer.txt
│   ├── wc.so                        0 单词计数(Word Count)的MapReduce应用,它被编译成了一个Go插件。
│   ├── mrsequential.go              0 运行的是一个单体（单机）版本的 MapReduce。
│   ├── mr-out-0                     0 是示例程序`go run mrsequential.go wc.so pg*.txt`的输出结果。

│   ├── mrcoordinator.go             1 多 worker 版本的 mapReduce
│   ├── mrworker.go
│   ├── pbc.go
│   ├── pbd.go
│   ├── test-mr-many.sh
│   ├── viewd.go
│   └── test-mr.sh                   2 打分的脚本，用来测试你的作业的完成情况
├── models
│   └── kv.go
├── mr
│   ├── coordinator.go
│   ├── rpc.go
│   └── worker.go
├── mrapps
│   ├── crash.go
│   ├── early_exit.go
│   ├── indexer.go
│   ├── jobcount.go
│   ├── mtiming.go
│   ├── nocrash.go
│   ├── rtiming.go
│   └── wc.go
├── porcupine
│   ├── bitset.go
│   ├── checker.go
│   ├── model.go
│   ├── porcupine.go
│   └── visualization.go
├── raft
│   ├── config.go
│   ├── persister.go
│   ├── raft.go
│   ├── test_test.go
│   └── util.go
├── shardctrler
│   ├── client.go
│   ├── common.go
│   ├── config.go
│   ├── server.go
│   └── test_test.go
└── shardkv
    ├── client.go
    ├── common.go
    ├── config.go
    ├── server.go
    └── test_test.go

```



[CS/Q2：做CS的经典lab/MIT6.824/Lab 3 Key Value server.md]
====================================================

---
draw:
tags: []
title: Lab 3 Key Value server
date created: 2024-08-07
date modified: 2024-12-27
---

## 资料

[Raft 算法介绍 - 谭新宇的博客 (tanxinyu.work)](https://tanxinyu.work/raft/)

## 过程

[Raft](Raft.md)

这个实验是关于实现一个单机版的键值存储服务器。以下是这个实验的主要内容和运行方法：

1. 实验目标：
    - 实现一个键值存储服务器，支持 Put、Append 和 Get 操作。
    - 确保每个操作只执行一次，即使在网络故障的情况下。
    - 实现线性化（linearizable）操作。
2. 准备工作：
    - 确保你在 ~/6.5840 目录下。
    - 使用 git pull 获取最新的实验代码。
3. 运行测试：
    `cd ~/6.5840 git pull cd src/kvsrv go test`

4. 实现步骤：
    - 修改 client.go 中的 Clerk Put/Append/Get 方法，添加 RPC 发送代码。
    - 在 server.go 中实现 Put、Append 和 Get 的 RPC 处理程序。
    - 修改代码以处理网络故障（丢包）情况。
    - 实现重试机制和重复请求过滤。
5. 测试通过标准：
    - 你的实现应该通过所有测试，包括单客户端、多客户端、不可靠网络等情况。
    - 使用 go test -race 确保代码没有竞态条件。
6. 关键实现点：
    - 确保每个操作只执行一次，即使在重试的情况下。
    - 实现唯一标识客户端操作的机制。
    - 考虑服务器需要维护哪些状态来处理重复请求。
    - 设计一个能快速释放服务器内存的重复检测方案。

这个实验主要挑战在于处理网络不可靠性的同时保证操作的正确性和一致性。你需要仔细考虑如何在客户端和服务器端实现重试和去重机制。



[CS/Q2：做CS的经典lab/MIT6.824/Lab2 raft.md]
=======================================

---
draw:
tags: []
title: Lab2 raft
date created: 2024-09-02
date modified: 2024-12-27
---

## 运行

在 Lab2 中，你的 Raft 实现代码主要位于 `6.824/src/raft/` 目录下。与 Lab1 不同，这里不需要像之前运行 MapReduce 一样先编译插件和手动启动进程；相反，你的测试是通过运行 `go test` 命令来自动执行的。

**主要命令**（在 `6.824/src/raft` 目录下执行）：

1. 进入 Raft 源码目录：

    ```bash
    cd ~/6.824/src/raft
    ```

2. 运行所有测试（包括 2A, 2B, 2C, 2D 全部）：

    ```bash
    go test -race
    ```

3. 单独测试某个部分（例如只测试2A部分）：

    ```bash
    go test -run 2A -race
    ```

    类似地：

    ```bash
    go test -run 2B -race
    go test -run 2C -race
    go test -run 2D -race
    ```

## 核心功能实现

1. **Leader 选举**:

```go
// 开始选举
func (rf *Raft) StartElection() {
    request := rf.genRequestVoteRequest()
    grantedVotes := 1  // 自己给自己投票
    rf.votedFor = rf.me
    
    // 向其他节点发送投票请求
    for peer := range rf.peers {
        if peer == rf.me {
            continue
        }
        go func(peer int) {
            response := new(RequestVoteResponse)
            if rf.sendRequestVote(peer, request, response) {
                rf.mu.Lock()
                defer rf.mu.Unlock()
                
                // 如果获得投票
                if response.VoteGranted {
                    grantedVotes += 1
                    // 获得多数票就成为 Leader
                    if grantedVotes > len(rf.peers)/2 {
                        rf.ChangeState(StateLeader)
                        rf.BroadcastHeartbeat(true)
                    }
                }
            }
        }(peer)
    }
}
```

2. **日志复制**:

```go
// Leader 广播日志条目
func (rf *Raft) BroadcastHeartbeat(isHeartBeat bool) {
    for peer := range rf.peers {
        if peer == rf.me {
            continue
        }
        if isHeartBeat {
            go rf.replicateOneRound(peer)
        } else {
            rf.replicatorCond[peer].Signal()
        }
    }
}

// 复制日志到某个 Follower
func (rf *Raft) replicateOneRound(peer int) {
    // ...
    request := rf.genAppendEntriesRequest(prevLogIndex)
    response := new(AppendEntriesResponse)
    if rf.sendAppendEntries(peer, request, response) {
        rf.mu.Lock()
        rf.handleAppendEntriesResponse(peer, request, response)
        rf.mu.Unlock()
    }
}
```

3. **提交日志**:

```go
// Leader 推进 commitIndex
func (rf *Raft) advanceCommitIndexForLeader() {
    // 统计有多少节点复制了日志
    n := len(rf.matchIndex)
    srt := make([]int, n)
    copy(srt, rf.matchIndex)
    insertionSort(srt)
    
    // 取得多数节点都复制的最大索引
    newCommitIndex := srt[n-(n/2+1)]
    if newCommitIndex > rf.commitIndex {
        // 只提交当前任期的日志
        if rf.matchLog(rf.currentTerm, newCommitIndex) {
            rf.commitIndex = newCommitIndex
            rf.applyCond.Signal()
        }
    }
}
```

4. **应用日志**:

```go
// 应用已提交的日志到状态机
func (rf *Raft) applier() {
    for rf.killed() == false {
        rf.mu.Lock()
        // 等待新的提交
        for rf.lastApplied >= rf.commitIndex {
            rf.applyCond.Wait()
        }
        
        // 获取需要应用的日志
        entries := make([]Entry, commitIndex-lastApplied)
        copy(entries, rf.logs[lastApplied+1-firstIndex:commitIndex+1-firstIndex])
        rf.mu.Unlock()
        
        // 应用日志到状态机
        for _, entry := range entries {
            rf.applyCh <- ApplyMsg{
                CommandValid: true,
                Command:     entry.Command,
                CommandIndex: entry.Index,
            }
        }
        
        rf.lastApplied = commitIndex
    }
}
```

5. **心跳机制**:

```go
func (rf *Raft) ticker() {
    for rf.killed() == false {
        select {
        case <-rf.electionTimer.C:
            // 超时未收到心跳，开始选举
            rf.mu.Lock()
            rf.ChangeState(StateCandidate)
            rf.currentTerm += 1
            rf.StartElection()
            rf.mu.Unlock()
            
        case <-rf.heartbeatTimer.C:
            // Leader 定期发送心跳
            rf.mu.Lock()
            if rf.state == StateLeader {
                rf.BroadcastHeartbeat(true)
            }
            rf.mu.Unlock()
        }
    }
}
```

这些代码实现了 Raft 的核心机制：

- Leader 选举通过投票实现
- 日志复制通过 AppendEntries RPC 实现
- 通过多数确认来提交日志
- 心跳机制维护 Leader 地位
- 持久化关键状态确保崩溃恢复

每个部分都通过精心的并发控制和状态管理来保证正确性。

## 测试

下面是一个针对 `raft_test.go` 中测试用例的说明表格。表格中列出了各测试方法的名称、主要测试目标和关键关注点。

|            | 测试名称                            | 测试内容                        | 重点关注点                                       |
| ---------- | ------------------------------- | --------------------------- | ------------------------------------------- |
| 领导选举       | TestInitialElection2A           | 测试初始节点选举                    | 检查初始条件下是否能在一段合理时间内选出领导者，以及 term 是否正确增长      |
|            | TestReElection2A                | 测试在网络故障后重新选举                | 当领导者断开后能否正确选出新领导者；旧领导者重连后系统的稳定性             |
|            | TestManyElections2A             | 测试多次随机断连和重新连接场景下的多轮选举       | 在反复断开、重连部分节点的环境中是否始终能维持或选出正确的领导者            |
| 基本一致性和日志复制 | TestBasicAgree2B                | 基本达成一致性测试                   | 测试在一个健康集群中对简单命令是否能正确提交日志条目                  |
|            | TestRPCBytes2B                  | 测试 RPC 字节数量                 | 确保同一命令不重复向各节点发送过多的 RPC；检查实现的高效性             |
|            | TestFailAgree2B                 | 测试在一个 follower 断开时的日志一致性    | 即使有 follower 掉线，领导者和剩余的 follower 也能对新日志达成一致 |
|            | TestFailNoAgree2B               | 测试过半 follower 断开后无法达成一致     | 若大多数 follower 不在线，则无法提交新日志条目                |
|            | TestConcurrentStarts2B          | 并发调用 Start() 的一致性测试         | 同一任期内，对多条并发提交的命令能否全部正确提交                    |
|            | TestRejoin2B                    | 测试领导者分区后再加入的场景              | 一个长期断开的领导者重新加入集群后，系统仍然能够保证一致性并选出合理的领导者      |
|            | TestBackup2B                    | 测试领导者对日志不一致的 follower 的快速回退 | 在 follower 日志落后且存在不一致的情况下，领导者是否能快速进行日志回退并同步 |
|            | TestCount2B                     | 测试 RPC 调用次数是否合理             | 检查正常操作下 RPC 调用数量不应过高或过低，以保证性能和正确性           |
| 持久化和恢复     | TestPersist12C                  | 基本持久化测试                     | 节点重启后能否正确恢复状态，从而保证系统安全性                     |
|            | TestPersist22C                  | 更复杂的持久化测试                   | 多次断开、重启和重新连接场景下，确保日志和状态机的一致性和持久性            |
|            | TestPersist32C                  | 测试分区的领导者和 follower 宕机后恢复    | 在有节点崩溃和网络分区的复杂场景下，重启后的节点能否正确恢复状态并加入新的任期     |
|            | TestFigure82C                   | 模拟 Figure 8 场景              | 随机节点崩溃、恢复、分区下，对日志提交和领导者选举的稳定性进行长时间大规模测试     |
|            | TestUnreliableAgree2C           | 在不可靠网络中达成一致性                | 测试在网络不可靠、丢包条件下，系统仍能提交大多数日志                  |
|            | TestFigure8Unreliable2C         | Figure 8 场景下的不可靠网络测试        | 结合随机崩溃、分区和不可靠网络的极端情况，确保系统的最终一致性和可用性         |
|            | TestReliableChurn2C             | 有限可靠性的节点频繁变化场景              | 不断重启、断线、重连节点，检查系统在大范围混乱中的表现                 |
|            | TestUnreliableChurn2C           | 不可靠网络下节点频繁变化场景              | 与上一个测试类似，但加入网络不可靠因素，进一步测试稳定性                |
| 快照         | TestSnapshotBasic2D             | 基本快照功能测试                    | 当日志不断增长时，领导者和 follower 是否能正确生成和安装快照以压缩日志    |
|            | TestSnapshotInstall2D           | 测试在断开连接条件下的快照安装             | follower 掉线后重新连接是否能通过接收快照快速追上最新日志           |
|            | TestSnapshotInstallUnreliable2D | 不可靠网络 + 断开连接下的快照安装          | 在不可靠网络和 follower 掉线下，检查快照同步的健壮性             |
|            | TestSnapshotInstallCrash2D      | 崩溃后重启下的快照安装测试               | 在节点崩溃并重启后，通过快照安装快速恢复一致性日志状态                 |
|            | TestSnapshotInstallUnCrash2D    | 不可靠网络 + 崩溃恢复下的快照安装          | 最复杂场景下，测试通过快照恢复日志一致性的能力                     |

这些测试全面覆盖了Raft算法的核心功能，包括领导选举、日志复制、持久化、恢复、快照等，并在各种网络条件（可靠、不可靠）和故障情况（节点崩溃、网络分区）下进行测试，以确保算法的正确性和鲁棒性。

## 游戏的全局设置器 make_config

下面是对该函数 `make_config` 的目的

- 创建多个 Raft 节点实例
- 控制节点间的通信（可以模拟网络分区、延迟等）
- 启动和停止节点
- 检查节点的状态（如谁是领导者，当前的任期等）
- 模拟客户端请求
- 验证一致性和正确性

**函数参数与关键变量**：

| 参数/变量        | 类型           | 含义                                                                          |
| ------------ | ------------ | --------------------------------------------------------------------------- |
| `t`          | `*testing.T` | Go 的测试框架中用于表示当前测试用例的对象。`make_config` 使用它来在测试过程中输出错误信息或终止测试。|
| `n`          | `int`        | 要创建的 Raft 节点个数，即集群中服务器的数量。|
| `unreliable` | `bool`       | 是否使用不可靠网络。当 `true` 时，模拟丢包、网络延迟、不按序到达的消息，从而测试 Raft 在不可靠通信环境下的表现。|
| `snapshot`   | `bool`       | 是否启用快照测试场景。如果为 `true`，则使用 `applierSnap` 来处理日志和快照，否则使用 `applier` 只对日志进行正常应用。|  

**关于网络可靠/不可靠的实现**：
在 `make_config` 函数中使用了 `cfg.net = labrpc.MakeNetwork()` 来创建一个模拟的网络对象。`labrpc.Network` 提供了对消息发送、接收的钩子，可通过配置指定如下特性：
- **可靠网络（unreliable = false）**：
    网络不会有意丢包或乱序，消息基本按照发送顺序和预期路线到达对端。
- **不可靠网络（unreliable = true）**：
    当将网络设置为不可靠后，这个 `labrpc.Network` 实例可能会丢弃某些 RPC 消息、对消息进行延迟、或是对消息重新排序。这就模拟了真实网络中的各种异常情况，使得 Raft 在面对丢包、乱序的环境下仍需要保持正确性与一致性。通过 `cfg.setunreliable(unreliable)` 来开启或关闭这种不确定性。
    
**关于 `snapshot` 参数的本质与形象解释**：
`setunreliable(true)` 会让网络按一定概率丢包、延迟消息；`setunreliable(false)` 则保证消息不丢不乱，从而达到控制网络可靠性的目的。
- **不使用快照时（snapshot=false）**：每次你从商店带回的新衣服（日志条目）都往衣柜里塞，衣柜越来越满（日志越来越长）。
- **使用快照时（snapshot=true）**：当衣柜塞满时，你会把一些旧衣服整理打包起来（生成快照），清空衣柜一部分空间，让衣柜保持较小的负载。随后的新衣服（日志条目）就不会因为衣柜满了而无处安置。


**总结**：
通过 `make_config` 函数，可以快速搭建一个包含 n 个 Raft 节点的测试环境，包括网络模拟、持久化存储初始化、日志处理逻辑设置以及节点连接状态的默认配置。这为后续测试用例（如选举、日志复制、持久化、网络分区和快照安装等）提供了基础的运行环境。

## 代码流程-TestInitialElection2A

TestInitialElection2A的主要思路是验证Raft集群在初始选举时的基本功能。

1. 初始设置:

```go
servers := 3
cfg := make_config(t, servers, false, false)
```

- 创建3个Raft服务器的集群
- false, false 参数表示网络是可靠的(不丢包),且不使用快照功能

1. 第一个检查点 - 选举出领导者:

```go
cfg.checkOneLeader()
```

- 这个函数会循环检查10次,每次等待450-550ms
- 检查是否有且仅有一个服务器认为自己是leader
- 如果发现多个leader或没有leader会失败

1. 第二个检查点 - term一致性检查:

```go
time.Sleep(50 * time.Millisecond)
term1 := cfg.checkTerms()
if term1 < 1 {
    t.Fatalf("term is %v, but should be at least 1", term1)
}
```

- 等待50ms让follower同步选举结果
- 检查所有服务器的term是否一致
- term必须至少为1(因为经过了一次选举)

1. 第三个检查点 - 稳定性检查:

```go
time.Sleep(2 * RaftElectionTimeout)
term2 := cfg.checkTerms()
```

- 等待2个选举超时时间
- 再次检查term
- 如果term改变了(意味着发生了新的选举),会打印警告
- 这说明在没有故障的情况下,leader应该保持稳定

1. 最后检查点:

```go
cfg.checkOneLeader()
```

- 再次确认仍然有一个leader

这个测试验证了Raft的几个基本特性:

1. 能够选出唯一的leader
2. 所有节点就term达成一致
3. 在无故障情况下保持稳定
4. leader能持续保持领导地位

代码执行流程:

1. Make_config创建集群 -> 启动3个Raft节点
2. 节点启动后开始选举过程
3. 等待并验证选举结果
4. 等待并验证集群稳定性
5. 最后验证leader状态

这是对Raft最基本功能的测试,确保实现的Raft能够正确完成领导者选举。

## 代码流程-TestPersist32C

`TestPersist32C` 的测试代码顺序如下：

1. **初始化设置**:

   ```go
   servers := 3
   cfg := make_config(t, servers, false, false)
   defer cfg.cleanup()
   ```

   - 创建一个包含3个服务器的Raft集群。
   - `make_config` 函数初始化网络、Raft实例、持久化存储等。

2. **开始测试**:

   ```go
   cfg.begin("Test (2C): partitioned leader and one follower crash, leader restarts")
   ```

3. **提交第一个命令**:

   ```go
   cfg.one(101, 3, true)
   ```

   - 提交命令 `101`，期望所有3个服务器都能达成一致。

4. **检查当前的领导者**:

   ```go
   leader := cfg.checkOneLeader()
   ```

5. **断开一个跟随者**:

   ```go
   cfg.disconnect((leader + 2) % servers)
   ```

   - 断开与领导者不相邻的一个跟随者。

6. **提交第二个命令**:

   ```go
   cfg.one(102, 2, true)
   ```

   - 提交命令 `102`，期望剩下的2个服务器达成一致。

7. **崩溃两个服务器**:

   ```go
   cfg.crash1((leader + 0) % servers)
   cfg.crash1((leader + 1) % servers)
   ```

   - 崩溃领导者和另一个跟随者。
   
8. **重新连接并重启一个崩溃的服务器**:

   ```go
   cfg.connect((leader + 2) % servers)
   cfg.start1((leader+0)%servers, cfg.applier)
   cfg.connect((leader + 0) % servers)
   ```

   - 重新连接之前断开的跟随者。
   - 重启并连接之前崩溃的领导者。

9. **提交第三个命令**:

   ```go
   cfg.one(103, 2, true)
   ```

   - 提交命令 `103`，期望2个服务器达成一致。

10. **重启并连接另一个崩溃的服务器**:

    ```go
    cfg.start1((leader+1)%servers, cfg.applier)
    cfg.connect((leader + 1) % servers)
    ```

11. **提交第四个命令**:

    ```go
    cfg.one(104, servers, true)
    ```

    - 提交命令 `104`，期望所有3个服务器都能达成一致。
    

12. **结束测试**:

    ```go
    cfg.end()
    ```

这个测试的目的是验证在领导者和一个跟随者崩溃后，系统能否正确恢复并继续达成一致。通过模拟网络分区和崩溃，测试Raft的持久化和恢复能力。

使用了不同的颜色来表示节点状态：

- 蓝色：领导者节点 leader
- 绿色：正常工作的节点 worker
- 红色：崩溃的节点
- 黄色：网络隔离的节点 脑裂

```mermaid
flowchart TD

    %% 定义阶段节点
    subgraph Stage1["阶段1: 初始状态 (提交 101)"]
        S0_1[["S0\n(Leader)\n日志:{101}"]]
        S1_1[["S1\n(Follower)\n日志:{101}"]]
        S2_1[["S2\n(Follower)\n日志:{101}"]]

        %% 所有节点初始连接且一致
        S0_1-->S1_1
        S0_1-->S2_1
    end

    subgraph Stage2["阶段2: 断开S2 (提交 102)"]
        S0_2[["S0\n(Leader)\n日志:{101,102}"]]
        S1_2[["S1\n(Follower)\n日志:{101,102}"]]
        S2_2[["S2\n(断开)\n日志:{101}"]]

        %% 断开S2后S0和S1还能提交新日志102
        S0_2-->S1_2
        %% S2断线,无法获得新日志,仍只有{101}
    end

    subgraph Stage3["阶段3: 崩溃S0和S1"]
        S0_3[["S0\n(Crashed)\n日志:{101,102}"]]
        S1_3[["S1\n(Crashed)\n日志:{101,102}"]]
        S2_3[["S2\n(活跃,单独)\n日志:{101}"]]

        %% 领导者S0和跟随者S1崩溃,只剩S2在线但落后
    end

    subgraph Stage4["阶段4: 重启S0并恢复连接S2 (提交 103)"]
        S0_4[["S0\n(Follower恢复)\n日志:{101,102}"]]
        S2_4[["S2\n(Leader选举)\n日志:{101,103}"]]

        %% S2被重新连接后可能成为新Leader,提交103
        %% S0重启后有旧日志{101,102},S2已提交{103}
        S2_4-->S0_4
    end

    subgraph Stage5["阶段5: 重启S1并全部同步 (提交 104)"]
        S0_5[["S0\n(Follower)\n日志:{101,102,103,104}"]]
        S1_5[["S1\n(Follower恢复)\n日志:{101,102,103,104}"]]
        S2_5[["S2\n(Leader)\n日志:{101,102,103,104}"]]

        %% S1重启后加入集群,通过Leader S2的日志同步拿到103和104
        S2_5-->S0_5
        S2_5-->S1_5
    end

    %% 流程连接
    Stage1 -->|"提交命令101(3台一致)"| Stage2
    Stage2 -->|"断开S2,提交命令102(2台一致)"| Stage3
    Stage3 -->|"崩溃S0&S1"| Stage4
    Stage4 -->|"恢复S0,连接S2,提交命令103(2台一致)"| Stage5
    Stage5 -->|"重启S1,提交命令104(3台一致)"| Done

    subgraph Done["阶段6: 测试结束"]
    end

    %% 样式定义
    classDef leader fill:#bbdefb,stroke:#1976d2,stroke-width:2px;
    classDef follower fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px;
    classDef crashed fill:#ffcdd2,stroke:#c62828,stroke-width:2px;
    classDef isolated fill:#fff9c4,stroke:#f9a825,stroke-width:2px;

    class S0_1 leader
    class S1_1,S2_1 follower

    class S0_2 leader
    class S1_2 follower
    class S2_2 isolated

    class S0_3,S1_3 crashed
    class S2_3 follower

    class S0_4 follower
    class S2_4 leader

    class S0_5,S1_5 follower
    class S2_5 leader

```

**为什么阶段4中 S0 拥有 {101,102} 而 S2 是 {101,103}？**  
- 当 S0 重启时，它是从持久化存储中恢复日志（S0 在崩溃前已经将 {101,102} 持久化）。此时 S2 虽然在线，但缺少 {102}。在重新连上 S0 后，Raft 必须重新选举领导者并保证日志一致。
- 由于测试代码 `cfg.one(103, 2, true)` 要求在这个场景下达成两个节点对于 `103` 的一致提交，那么不管最终是谁成为领导者（可能是 S2 也可能是 S0，在真实实现里通常是日志更"新"的节点成为领导者，但这里我们重点不纠结领导者身份转换的细节），领导者都会尝试让另一个存活节点复制最新的日志项 `103`。
- 在此过程中，领导者发现双方的日志不一致（S0 有 {102}，S2 没有）。Raft 协议会通过 AppendEntries RPC 尝试匹配日志索引和任期。如果发现不匹配，领导者会回退并尝试找到一致的前缀，然后通过发送缺失的日志项来使跟随者日志追上来。
- **102 已经在之前达成过多数派提交，它是不可逆的**。无论现在的领导者是谁，它都必须最终包含所有已提交的日志条目（包括 102）。
- 因此，领导者在尝试写入 `103` 时，会先确保所有参与的节点对于之前已经提交的条目达成一致。也就是说，最终 S2 将通过日志复制机制获得 {102}，然后才会正确接受 {103}。

这个过程可能会在内部进行多次 AppendEntries RPC 往返，直到 S2 更新日志变为 {101,102,103}。同时，S0 作为重启后加入集群的节点，已经有 {101,102}，现在再加上 {103}，两者同步达成了一致。

## 代码流程-TestSnapshotInstall2D



[CS/Q2：做CS的经典lab/MIT6.824/MIT 6.824 Distributed System.md]
==========================================================

---
draw:
tags: []
title: MIT 6.824 Distributed System
date created: 2024-07-11
date modified: 2025-01-27
---

计算机硕士开题想基于MIT 6.824做一个分布式存储引擎，请问想法是否可行？- 知乎  
https://www.zhihu.com/question/53471701

Lab：
https://www.youtube.com/watch?v=IdCbMO0Ey9I&feature=youtu.be

学习分布式系统需要怎样的知识？- 量子位的回答 - 知乎  
https://www.zhihu.com/question/23645117/answer/1053242777

- LEC 1: Introduction
    - 主题: MapReduce
    - 准备: 阅读 [MapReduce](MapReduce.md) (2004)
    - 作业: [Lab 1 MapReduce](Lab%201%20MapReduce.md)
- LEC 2: RPC and Threads
    - 示例: crawler.go, kv.go, vote
    - 准备: 完成在线 [Golang](Golang.md) 教程
- LEC 3: (因暴风雪取消)
    - 作业: [Lab 3 Key Value server](Lab%203%20Key%20Value%20server.md)
- LEC 4: Consistency and Linearizability
    - 准备: Linearizability Testing
- LEC 5: Go patterns (嘉宾讲座 - Russ Cox from Google/Go)
    - 准备: 阅读 The Go Programming Language and Environment
- LEC 6: GFS
    - 准备: 阅读 GFS (2003)
- LEC 7: Fault Tolerance: Raft (1)
    - 准备: 阅读 Raft (extended) (2014), 到第 5 节结束
- LEC 8: Fault Tolerance: Raft (2)
    - 准备: 阅读 Raft (extended) (2014), 第 7 节到结尾 (不包括第 6 节)
    - 作业: Final Project 分配
- LEC 9: Zookeeper
    - 准备: 阅读 ZooKeeper (2010)
- LEC 10: Q&A Lab 3A+B
    - 作业: [Lab 4: KV Raft](Lab%204:%20KV%20Raft)
- LEC 11: Verified Primary/Backup
    - 准备: 阅读 Grove (2023), 重点关注第 1, 2, 7 节
- LEC 12: Distributed Transactions
    - 准备: 阅读 6.033 Chapter 9, 仅 9.1.5, 9.1.6, 9.5.2, 9.5.3, 9.6.3 节
- LEC 13: Spanner
    - 准备: 阅读 Spanner (2012)
    - 作业: Lab 5: Sharded KV
- LEC 14: Chardonnay
    - 准备: 阅读 Chardonnay (2023)
- LEC 15: Optimistic Concurrency Control
    - 准备: 阅读 FaRM (2015)
- LEC 16: Amazon DynamoDB (虚拟讲座 by Doug Terry)
    - 准备: 阅读 DynamoDB (2022)
- LEC 17: Ray
    - 准备: 阅读 Ray (2021)
- LEC 18: Cache Consistency: Memcached at Facebook
    - 准备: 阅读 Memcached at Facebook (2013)
- LEC 19: AWS Lambda (虚拟讲座 by Marc Brooker)
    - 准备: 阅读 On-demand Container Loading (2023)
- LEC 20: Boki
    - 准备: 阅读 Boki (2021)
- LEC 21: Fork Consistency, SUNDR
    - 准备: 阅读 SUNDR (2004) (到 3.3.2 节结束)
- LEC 22: Byzantine Fault Tolerance
    - 准备: 阅读 Practical BFT (1999)
- LEC 23: Peer-to-peer: Bitcoin
    - 准备: 阅读 Bitcoin (2008) 和总结
- LEC 24: (取消)
- LEC 25: Project demos



[CS/Q2：做CS的经典lab/MIT6.824/MapReduce.md]
=======================================

---
draw: https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fliuyishou%2Ftmp%2F2024%2F07%2F29%2F14-29-11-250102658e306624126296f0a23acee6-mapreduce.drawio-e226df.png
aliases: [拆解-规约]
tags: []
title: MapReduce
date created: 2024-07-11
date modified: 2024-11-12
---

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F07%2F28%2F17-58-18-a630259d341bb3f30076abf7ae563b59-20240728175817-c677c1.png)

<!-- more -->

以统计一段长文本的各单词词频为例

1. split 任意切分原始输入——总的大文本是可以任意分割的
2. map 无依赖的计算——逐个统计就好，是无状态的  
3. Shuffle 整理牌，把相同 key 的放一起
4. reduce 有依赖的计算——按照同一个单词作为 key 进行规约
5. MR 容错—矩阵（包括数据）

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F07%2F29%2F10-47-40-c77de7f51ed2973f17e76e1fed9e1d52-20240729104739-183e3a.png)

1、内存和磁盘使用方面 [Spark](Spark) vs MapReduce 不等于内存 vs 磁盘，Spark 和 MapReduce 的计算都发生在内存中，区别在于：•MapReduce 需要将每次计算的结果写入磁盘，然后再从磁盘读取数据，从而导致了频繁的磁盘 10。•Spark 通常不需要将计算的结果写入磁盘，可以在内存中进行迭代计算 Q。这得益于 Spark 的 RDD 和 DAG（有向无环图 Q），其中 DAG 记录了 job 的 stage 以及在 job 执行过程中父 RDD 和子 RDD 之间的依赖关系。中间结果能够以 RDD 的形式存放在内存中，极大减少了磁盘 I0Q。
2、Shuffle 上的不同 Spark 和 MapReduce 在计算过程中通常都不可避免的会进行 Shuffle，Shuffle 都会落盘，但：• MapReduce 在 Shuffle 时需要花费大量时间进行排序，排序在 MapReduce 的 Shuffle 中似乎是不可避免的；• Spark 在 Shuffle 时则只有部分场景才需要排序，支持基于 Hash 的分布式聚合，更加省时；

## 特性

MapReduce的核心在于其任务的可分性和可规约性：

1. **可分性**：问题可以被分解为多个独立的子任务。
2. **可规约性**：这些子任务的结果可以被合并得到最终结果。

## 那么反面是啥，什么任务是不可规约的？

反过来说，不适合MapReduce的任务通常具有以下特征：

1. **强依赖性**：任务中的步骤彼此强烈依赖，无法并行处理。
2. **顺序性**：必须按特定顺序执行的任务。
3. **全局状态依赖**：需要访问或修改全局状态的任务。
4. **迭代性**：需要多次迭代的算法，每次迭代依赖于前一次的结果。
5. **实时性要求**：需要快速响应的实时或近实时处理任务。
6. **小数据量**：数据量较小，分布式处理的开销可能超过其带来的收益。
7. **复杂的数据关系**：涉及复杂的数据关系或图结构的任务。

## 具体例子

1. **图算法**：如PageRank，需要多次迭代且每个节点依赖于其他节点的状态。
2. **机器学习算法**：许多机器学习算法需要多次迭代和全局状态。
3. **事务处理**：需要保持全局一致性的数据库事务。
4. **自然语言处理**：某些NLP任务需要考虑整个文本的上下文。
5. **实时流处理**：需要立即处理和响应的数据流。

## 结论

虽然MapReduce强大且适用于许多大数据处理场景，但它并非万能的。对于具有强依赖性、顺序性、全局状态需求或迭代性的任务，可能需要考虑其他分布式计算模型，如图计算框架（如Pregel）、流处理系统（如Apache Flink）或更通用的分布式计算框架（如Apache Spark）。



[CS/Q2：做CS的经典lab/MIT6.824/Raft.md]
==================================

---
draw: 
tags: []
title: Raft
date created: 2024-08-07
date modified: 2024-12-27
---

简而言之，其名字即来自于 `R{eliable|plicated|dundant} And Fault-Tolerant`，也来自于这是一艘可以帮助你逃离 Paxos 小岛的救生筏（Raft）。

<!-- more -->

## 参考资料

[Lab2 raft](Lab2%20raft.md)

[6.824 Lab 2: Raft (mit.edu)](http://nil.csail.mit.edu/6.824/2022/labs/lab-raft.html)

[一文搞懂Raft算法 - 割肉机 - 博客园 (cnblogs.com)](https://www.cnblogs.com/williamjie/p/11137140.html)

[使用 “隐喻” 的方式帮你建立对 Raft 的直觉 | 木鸟杂记 (qtmuniao.com)](https://www.qtmuniao.com/2023/11/15/raft-explain/)

## 核心思想

- 选举班长:
    - 班级需要选出一个班长来领导大家。
    - 每个学生都可以竞选班长,获得多数票的人当选。
    - 班长任期有限,到期后需要重新选举。
- 日志复制:
    - 班长负责记录班级的重要决定(日志)。
    - 班长会把这些决定告诉其他同学,确保大家都知道。
    - 只有多数同学都记录了某个决定,这个决定才算正式生效。
- 安全性:
    - 如果班长缺席了,新班长上任后会先了解之前的决定,不会随意更改。
    - 所有正式生效的决定都会被保留下来,不会丢失。
- 一致性:
    - 通过以上机制,确保所有同学对班级的重要决定达成一致。
    - 即使有同学暂时离开或者新同学加入,最终都能同步到最新信息。

1. raft 共识算法的目的是实现不同节点有着相同的状态机流转
2. 状态机流转表现在相同的历史日志上
3. leader 意味着所有人的日志应该以它为准，只有 leader 有资格接收写命令，当 leader 确认一个日志条目被多数节点复制后，才意味着该条目被成功 commit。

```go
type Entry struct { 
	Index int 
	Term int 
	Command interface{} 
} 
```

所以所谓的共识算法，就是保证 entry 的 commit 顺序都相同

## 分工线程

1. 队长可以专注于接收新的游戏指令
2. Replicator是队长的特派员队伍，每个Replicator负责与一个特定的队员通信
3. Applier 负责是把大家同意的内容真正地写在各自的小本本上

## 节点

```go
type Raft struct {  
    mu        sync.RWMutex        // Lock to protect shared access to this peer's state  
    peers     []*labrpc.ClientEnd // 这是班级里所有同学的联系方式列表。  
    persister *Persister          //  Alice 的"日记本",用来永久记录重要信息,即使遇到意外(比如电脑重启)也不会丢失。  
    me        int                 // Alice 在班级里的学号  
    dead      int32               // 标记 Alice 是否还在"参与班级事务"。  
  
    applyCh        chan ApplyMsg //用来向"班级公告栏"(也就是状态机)发布已确认的决定。  
    applyCond      *sync.Cond    // 这些是 Alice 用来协调不同任务的"闹钟"。  
    replicatorCond []*sync.Cond  // 这些是 Alice 用来协调不同任务的"闹钟"。  
    state          NodeState     //表示 Alice 当前的角色(普通同学、候选人或班长)。  
  
    currentTerm int     // 当前的"学期"编号。  
    votedFor    int     // 在这个学期,Alice 投票给了谁(如果投过的话)。  
    logs        []Entry // Alice 的笔记本,记录了所有的班级决定。  
  
    commitIndex int   //Alice 确定全班大多数人都记下的最新笔记的页码。  
    lastApplied int   // Alice 最后一次在"班级公告栏"上贴出的笔记的页码。  
    nextIndex   []int //对于每个同学,Alice 准备发给他们的下一页笔记的页码。  
    matchIndex  []int //对于每个同学,Alice 确定他们已经记下的最新笔记的页码。  
  
    electionTimer  *time.Timer // "选举超时" 的责任闹钟,如果太久没收到班长的消息,就会响起。  
    heartbeatTimer *time.Timer // 如果 Alice 是班长,这个"心跳"闹钟会定期提醒她给其他同学发消息。  
}
```

electionTimer"责任闹钟"

- 主动性：
    - 在Raft算法中，electionTimer就像是每个同学都有的一个"责任闹钟"。
    - 它不是被动等待别人来组织班级，而是主动关注班级的状况。
- 自我管理：
    - 这个机制确保了即使当前的班长（Leader）出了问题（比如请假或转学），班级也不会陷入无人管理的混乱状态。
    - 每个同学都有潜力成为新的班长，保证了班级管理的连续性。
- 分布式特性：
    - 这种机制很好地体现了分布式系统的特点 - 没有单点故障。
    - 不依赖于固定的"权威"，而是通过群体的自组织来维持秩序。
- 平衡与竞争：
    - 但这也不意味着同学们会急于"篡位"。定时器的随机性（通常是150-300ms）确保了不会所有人同时站出来，避免了混乱。
    - 这就像是大家都在关注，但又不会争先恐后，保持了一种平衡。
- 容错性：
    - 这个机制也提供了很好的容错性。即使有些同学可能暂时"走神"（节点故障），只要还有超过半数的同学在认真听讲（正常工作的节点），班级就能继续正常运转。
- 民主过程：
    - 当有同学站出来要当班长时，还需要获得多数同学的同意（投票过程）。这确保了新班长的合法性和群体的认可。
    

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F08%2F14%2F11-47-56-cedb47e41a05d71a77491a740ea00ea9-202408141147535-efc62c.png)

持久化：currentTerm, voteFor 和 logs 这三个变量一旦发生变化就一定要在被其他协程感知到之前（释放锁之前，发送 rpc 之前）持久化，这样才能保证原子性。

锁操作：防止多个 [Goroutine](Goroutine.md) 同时改变关键状态（如 currentTerm, votedFor 和 logs）

## 选举与投票

想象我们有一个5人的班级:Alice, Bob, Charlie, David 和 Eva。

1. 初始状态: 所有人都是普通同学(Follower状态)。
2. 选举触发: 假设Alice的"选举定时器"先到期了。她决定竞选班长。
3. 请求投票:
    - Alice将自己的"任期号"增加到1。
    - 她给其他所有同学发送"投票请求"。
4. 投票过程:
    - Bob和Charlie收到请求后,发现这是他们见过的最新任期,所以同意投票给Alice。
    - David的网络延迟,暂时没收到请求。
    - Eva可能刚刚重启,也没来得及回应。
5. 统计选票:
    - Alice收到了自己、Bob和Charlie的3票,超过了半数(5/2 = 2.5)。
    - Alice宣布自己成为新班长(Leader)。
6. 巩固领导地位:
    - Alice立即向所有同学发送"心跳"消息,表明自己是班长。
    - 收到心跳的同学会重置他们的选举定时器,防止新的选举发生。（其实并不是这样实现的）
		- 这里强调，只有当节点给另一个节点投票（grant vote）时，才应该重置这个计时器。
		- 这与一些可能的直觉相反，因为你可能会认为收到心跳时也应该重置计时器。
			- 防止"活锁"（Livelock）:
			    - 如果每次收到心跳就重置计时器，在网络不稳定的情况下，可能会出现这样的情况：一个节点发送部分心跳成功，但不足以维持领导地位，却足以阻止其他节点发起新的选举。
			    - 这可能导致系统长时间处于没有有效领导者的状态，影响系统的可用性。
			- 加快选举过程:
			    - 只在投票时重置计时器可以让系统在失去领导者后更快地进入新的选举阶段。
			    - 如果每次心跳都重置，可能会延迟发现领导者已经离线的时间。
			- 增加选举的公平性:
			    - 这种机制给了其他节点更公平的机会来发起选举，而不是总是偏向于网络连接较好的节点。
7. 处理迟到的响应:
    - 如果David后来投票给Alice,这票会被接受但不会改变结果。
    - 如果Eva稍后尝试发起新的选举(因为她可能错过了整个过程),由于她的任期号不会高于Alice,所以不会成功。
8. 持续领导:
    - Alice会定期发送心跳来维持自己的领导地位。
    - 只要她能继续与多数同学保持联系,她就能继续担任班长。

这个例子展示了Raft选举的关键特点:

- 选举是由超时触发的
- 候选人寻求多数票
- 最先获得多数票的候选人获胜
- 通过心跳消息维持领导地位

通过这种机制,Raft确保在任何给定时刻,班级(集群)中最多只有一个公认的班长(Leader),从而保证了决策的一致性。

```go
type RequestVoteRequest struct {  
    Term         int // 表示 "我想在第几学期竞选班长"  
    CandidateId  int // 明确告诉大家 "我是谁，我在竞选班长"  
    LastLogIndex int // 相当于说 "我的笔记本最后一页是第几页"，表明自己了解班级事务的程度  
    LastLogTerm  int // 表示 "我笔记本最后一页是哪个学期的内容"，进一步证明自己的资格  
}  
  
type RequestVoteResponse struct {  
    Term        int  // 目的：告诉竞选者 "我当前认为我们在哪个学期"  
    VoteGranted bool // 目的：表示 "我是否同意你当班长" true: "我投你一票" false: "抱歉，这次我不投你"  
}

rf.votedFor = rf.me    // 投给自己
rf.votedFor = request.CandidateId   // 投给拉票的RequestVote RPC
rf.votedFor = -1       // 初始或者重置状态，标识未投票
rf.persist()          // 每次改变 votedFor 的值后,都需要持久化状态，保存到硬盘中，避免崩溃重启后丢失状态 

response.VoteGranted = true  // 表示投票
response.VoteGranted = false // 表示拒绝投票

`term`： Lamport 逻辑时钟
```

## 日志同步

```go

type Entry struct {
	Index   int     // 这是笔记本的第50页
	Term    int     // 这条规定是在第4学期做出的
	Command interface{} // 具体内容是"每周五放学后打扫教室"
}


type AppendEntriesRequest struct {  
    Term         int     //表示当前的"班级学期"。帮助同学们判断这个通知是否来自最新的班长。  
    LeaderId     int     //明确告诉大家是谁发的通知。如果有同学不知道谁是当前班长，可以直接认出来。  
    PrevLogIndex int     //相当于说"我要告诉你的新信息是接在第几页笔记之后的"。  
    PrevLogTerm  int     //确保大家的笔记本上这一页是在同一个"学期"写的。  
    LeaderCommit int     //告诉大家"到第几页的内容已经确定不会变了"。  
    Entries      []Entry //新的笔记内容。可能包含多条新记录，也可能为空（表示这是一个心跳消息）。  
}  
  
  
type AppendEntriesResponse struct {  
    Term          int  //告诉 Alice(leader) 自己当前所知的"班级学期"。如果这个值比 Alice 发来的更大,说明可能有更新的班长产生了。  
    Success       bool //告诉 Alice 是否成功接受并记录了她发来的新内容。 true: "好的,我已经记下来了" false: "抱歉,我没能记下来,可能我的笔记有些问题"  
    ConflictIndex int  //如果记录失败,告诉 Alice "从第几页开始,我的笔记可能和你的不一样"  
    ConflictTerm  int  //如果记录失败,告诉 Alice "在冲突的那一页,我记录的是哪个学期的内容"  
}

/**  
    这个方法可以理解为"检查对方的笔记是否比自己的更新"。  
    参数:  
    term: 对方笔记最后一页的学期  
    index: 对方笔记的总页数  
*/  
func (rf *Raft) isLogUpToDate(term, index int) bool {  
    lastLog := rf.getLastLog()  
    return term > lastLog.Term ||  
       (term == lastLog.Term && index >= lastLog.Index)  
}  
  
/**  
    这个方法可以理解为"检查对方提到的某页笔记是否与自己的一致"。  
    参数:  
    term: 要检查的那页笔记的学期  
    index: 要检查的是第几页笔记  
*/  
func (rf *Raft) matchLog(term, index int) bool { // bob说第50页是3年级上学期写的  
    // 首先Alice检查自己有没有写到第50页 && 如果有,就看看自己第50页是不是真的写着"3年级上学期"  
    return index <= rf.getLastLog().Index && rf.logs[index-rf.getFirstLog().Index].Term == term  
}
```

1. **乐观**：一开始心跳不附带任何日志，只带一些 "暗号" 过去。假如 Follower 的通过 "暗号" 发现自己日志跟 Leader 完全一致，就直接回：一致，之后的心跳不需附加任何日志。
2. **回撤**：如果 Follower 通过 "暗号" 发现自己和 Leader 日志并不一致，也会告诉 Leader——下次得附带日志。则 Leader 就附加一些末尾的日志，如果发现还是不一致，就要继续回撤，多向前附加一些日志，同时更新 "暗号"，直到收到 Follower 肯定回复，则继续恢复不附加任何日志的心跳。

`entry`：驱动生长出一个新的边。仅有 leader 可以生成 entry。Raft 中，将每一个事件都称为一个 entry，每一个 entry 都有一个表明它在 log 中位置的 index

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F08%2F14%2F14-54-04-f2a17acb23503eeb51273a68c30c7183-202408141454683-6c94c5.png)

```mermaid
graph TD
    A[开始] --> B{wrapper报价为空?}
    B -->|是| C[记录: wrapper报价为空，以order报价为准]
    C --> D[返回 true]
    B -->|否| E{是否动态佣金?}
    E -->|是| F[记录: D页定价超过原始卖价以wrapper为准]
    F --> G[返回 false]
    E -->|否| H{底卖价相同且开关打开?}
    H -->|是| I[记录: 底卖价相同以wrapper为准]
    I --> J[返回 false]
    H -->|否| K{是否现付?}
    K -->|是| L[记录: 现付报价以order报价为准]
    L --> M[返回 true]
    K -->|否| N{是否集团不定价?}
    N -->|是| O[记录: 集团不定价以order报价为准]
    O --> P[返回 true]
    N -->|否| Q{是否预付返现?}
    Q -->|是| R[记录: 预付返现报价以order报价为准]
    R --> S[返回 true]
    Q -->|否| T{是否集团亚朵会员?}
    T -->|是| U[记录: 集团亚朵会员以order报价为准]
    U --> V[返回 true]
    T -->|否| W[记录: 以wrapper报价为准]
    W --> X[返回 false]
```

```Java

```



[CS/Q2：做CS的经典lab/MIT6.824/共识算法.md]
==================================

---
draw:
tags: []
title: 共识算法
date created: 2024-08-07
date modified: 2024-11-12
---

换句话说，可以理解为共识算法就是用来确保每个节点上的日志顺序都是一致的。

<!-- more -->

[拜占庭将军问题](拜占庭将军问题)

两个反例：

1. 桃花源记中的不知有汉 无论魏晋
2. 有一名日本军官，二战结束后仍在菲律宾孤岛负隅顽抗30年

[共识算法](共识算法.md) 允许一组节点像一个整体一样一起工作，即使其中一些节点出现故障也能够继续工作下去，其正确性主要源于复制状态机的性质：

> 任何初始状态一样的状态机，如果执行的命令序列一样，则最终达到的状态也一样。如果将此特性应用在多参与者进行协商共识上，可以理解为系统中存在多个具有完全相同的状态机（参与者），这些状态机能最终保持一致的关键就是起始状态完全一致和执行命令序列完全一致。

众所周知，[Paxos](Paxos) 是一个非常划时代的共识算法。在 Raft 出现之前的 10 年里，Paxos 几乎统治着共识算法这一领域：因为绝大多数共识算法的实现都是基于 Paxos 或者受其影响，同时 Paxos 也成为了教学领域里讲解共识问题时的示例。但是不幸的是，尽管有很多工作都在尝试降低 Paxos 的复杂性，但是它依然十分难以理解。并且，Paxos 自身的算法结构需要进行大幅的修改才能够应用到实际的系统中。这些都导致了工业界和学术界都对 Paxos 算法感到十分头疼。比如 Google Chubby 的论文就提到，因为 Paxos 的描述和现实差距太大，所以最终人们总会实现一套未经证实的类 Paxos 协议。

基于以上背景，Diego Ongaro 在就读博士期间，深入研究 Paxos 协议后提出了 [Raft](Raft.md) 协议，旨在提供更为易于理解的共识算法。Raft 的宗旨在于可实践性和可理解性，并且相比 Paxos 几乎没有牺牲多少性能。



[CS/Q2：做CS的经典lab/UCB CS186 Introduction to Database System/UCB CS186 Introduction to Database System.md]
========================================================================================================

---
draw:
tags: []
title: UCB CS186 Introduction to Database System
date created: 2024-12-16
date modified: 2024-12-27
---



[CS/Q2：做CS的经典lab/lab 驱动的实践教学.md]
================================

---
draw:
tags: []
title: lab 驱动的实践教学
date created: 2025-01-29
date modified: 2025-01-29
---
- **MIT 6.828**（操作系统）用 xv6 做实验，从内核最小原型出发，逐步构建 OS 核心组件。
- **MIT 6.824**（分布式系统）强调容错、复制、一致性等分布式关键技术，lab 实现 Raft、KV 存储等。
- **CMU 15-445**（数据库）从存储结构到事务并发，让你自写内核组件。
    这样的课程最有价值之处在于：**它们不仅讲原理，还有循序渐进的代码实战**。同学边读论文、边阅读源码、边实现关键模块，最后把多个实验串起来，就能获得对"系统如何设计与实现"较为全面的感悟。

- **Lab 驱动，重视 debug 与设计思考**
    
    - 碰到无法通过的测试，不要只顾埋头改bug，要反思：系统抽象、接口、并发或锁策略、性能瓶颈……
    - 将自己卡住的问题做记录，然后查资料/看论文/看别人的代码思路，培养系统思考与调试习惯。
- **阅读论文或经典著作**
    
    - 课堂中会让你读一批论文 (如 GFS、MapReduce、BigTable、Spanner、FaRM)；在读论文时可多做笔记，关注问题、思路和权衡。
    - 常见书籍：CSAPP、DDIA(数据密集型应用系统设计)、Dragon Book(编译原理)、SICP(程序构造与解释)等，都能让你拓展眼界。



[CS/Q2：做CS的经典lab/团队协作.md]
=========================

---
draw:
tags: []
title: 团队协作
date created: 2024-08-29
date modified: 2024-11-12
---



[CS/Q2：做CS的经典lab/如何开始学习一个 Lab，以 MIT6.824 Distributed System为例 $.md]
===================================================================

---
draw:
tags: []
title: 如何开始学习一个 Lab，以 MIT6.824 Distributed System为例 $
date created: 2024-07-22
date modified: 2025-01-24
---

 [建立直觉](建立直觉)与代码落地相结合，尝试将[使用 “隐喻” 的方式帮你建立对 Raft 的直觉 | 木鸟杂记 (qtmuniao.com)](https://www.qtmuniao.com/2023/11/15/raft-explain/)中的故事作为注释标记到代码实现上。

## 为什么要做这件事？

利用好 AI 的思考力资源，最大化这个杠杆

## 最佳实践

1. 前置知识只有 [Git](Git.md) 一个，尤其是用好 Jetbrain 的 GUI
2. lab 一定要 commit 到能跑 test 的程度，这样才能给小节点一个彻底的完成，以后接进度的时候不需要多余的心智。
- 找到靠谱答案，去 [解耦学习](解耦学习.md)

git 抄作业  
![image.png|1800](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2025%2F01%2F06%2F22-13-04-2f118a96464afcf24cf4405b3c3a062e-202501062213608-cbec8d.png)

## 前期准备

思想性的，比如代码的实现思路，可以参考这个清华爷的笔记

- [6.824 分布式系统课程学习总结 - 谭新宇的博客 (tanxinyu.work)](https://tanxinyu.work/6-824/)

流程性的，比如每个 lab 需要哪些命令 run 啥的，都可以直接把官方文档丢给 AI，让他介绍

- ![image.png|500](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F07%2F22%2F20-44-20-f0f052bfa8ba4a920e9ae1b1a2a03da1-20240722204419-0473d4.png)
- [Claude](https://claude.ai/new)

语法性的，直接问 AI，然后让从 [Java@](Java@.md) 的角度去解释  
![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F07%2F22%2F21-15-45-bf9807da629dd1f451db91d4e59d154d-20240722211544-376838.png)

## 实践

1. 把 [github](github.md) 题解仓库克隆到本地
2. check out 到对应的 revision  
![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F07%2F22%2F23-03-42-238e1133e08385f20b438501eadec517-20240722230341-38ec0d.png)
3. 抄袭增量部分的代码，遇到不懂的问 [claude](claude.md) 这相当于 [Java@](Java@.md) 里面的啥
4. 琢磨这种解决方案背后的思想，尝试将[使用 “隐喻” 的方式帮你建立对 Raft 的直觉 | 木鸟杂记 (qtmuniao.com)](https://www.qtmuniao.com/2023/11/15/raft-explain/)中的故事作为注释标记到代码实现上。

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F07%2F24%2F23-53-14-ee45868ab8eddcb499b23b3b5dd3ecab-20240724235311-8c3a82.png)

通过这种抄作业的方式

- 一边学习语法，
- 一边从主方法开始，一步步把标红的方法补全，抄一段大概就明白这段在干嘛，然后加上关键性的梳理性的注释。慢慢就把整个项目吃透了

另外，也可以把好的文档给[claude](claude.md)，让他结合代码帮忙回答一些问题  
![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F08%2F14%2F12-02-28-77a02d3410d42a1641b69e8767c923e1-202408141202804-c40941.png)

## 课程视频部分

参考[如何畅游英语油管$](如何畅游英语油管$.md)

## 建立直觉与代码逻辑相结合

[一文搞懂Raft算法 - 割肉机 - 博客园 (cnblogs.com)](https://www.cnblogs.com/williamjie/p/11137140.html)



[CS/Q2：做CS的经典lab/解耦学习.md]
=========================

---
draw:
tags: []
title: 解耦学习
date created: 2024-07-22
date modified: 2024-11-12
---

定义：有最佳实践的解决过程，有能够判断对错的反馈，除了学习相应的思想外，无需操心任何无关的事情。

比如自己做 [Anki@](Anki@.md) 卡片，花了很大功夫，但是一旦做好就可以每天作为 [习惯](习惯) 去练习，就很解耦。

比如自己做 lab，初期找资料、找答案需要很多时间，但是一旦找好只需要抄一抄，想一想就好了，就很解耦



[CS/Q2：做CS的经典lab/谷歌三剑客.md]
==========================

---
draw:
title: 谷歌三剑客
date created: 2024-07-28
date modified: 2025-02-15
---

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F07%2F28%2F17-54-02-1f2507170892882f0f33041d5bc96dc1-20240728175401-324dd2.png)  
[MapReduce](MapReduce.md)



[Q2：如何达到 50%的三分命中率/$ 如何达到 50%的三分命中率.md]
=======================================

---
draw:
tags: []
title: $ 如何达到 50%的三分命中率
date created: 2024-07-20
date modified: 2024-11-12
---

## 第一次复盘

![image.png|700](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F07%2F20%2F18-10-28-e76d9e43a322236254d3da0f90adf008-20240720181028-4b448e.png)

 1. 只收了小臂，你应该收小臂时带动大臂上提，
 2. 出手点太低

## 第二次复盘

1. 重心过度压到前脚，屈髋的同时注意重心不要压到前脚
2. 下肢弹速太慢，跟腱发力参与的少，后脚跟不抬起
3. 胳膊和手腕太刻意。胳膊手腕发力放松，如果投不到，从下肢上找补，别从手部发力找补。



[Q2：如何达到 50%的三分命中率/一分钟自投自抢.md]
==============================

---
draw:
tags: []
title: 一分钟自投自抢
date created: 2024-06-12
date modified: 2024-11-12
---

标准是多少个算优秀

有什么练的技巧和途径



[Q2：如何达到 50%的三分命中率/寻找达人反馈.md]
=============================

---
draw:
title: 寻找达人反馈
tags: []
date created: 2024-06-12
date modified: 2025-01-05
---

## 投篮

1. 左脚指向太外，导致髋没有聚合向前
2. 整个投篮都是前脚掌参与，不要和脚跟发生关系
3. 膝盖不用弯曲特别多，感受股四头肌+脚踝的发力，而这都会被屈髋所带动
4. ==合球双手距离不要太近，起球点上来应该正对右胸==
5. 顶肘部发力，而不是腋下、大臂根部，对应的投篮之后的整个手臂也是指向前方的，而不是垂直向上的
6. 垫步不要卸力，下沉+回弹整个动作要快，越快初速度越大，投的越远

## 过人

1. 右手突破的时候注意左肩下沉的程度，可以对比一下左手突破的时候。

## 传球

1. 后场的四十五度三分区域是全场视野最佳的位置
2. 远距离快攻传球的目标最好是前场的油漆区两条直线的位置



[Q2：如何达到 50%的三分命中率/投篮.md]
=========================

---
draw: 
title: 投篮
date created: 2024-04-06
date modified: 2025-02-14
---

起球 [矢状面](矢状面) 方向，也就是脚尖的指向，这样最顺畅，最快。
[冠状面](冠状面) 上，肩膀位于 [髋](髋.md) 的正上方，防止重心过于靠前。

<!-- more -->

教授 Mitchell Ne。。。bkt护腰靠垫 谢尔比 friga dyson戴森吹。。jlew 刘守元巴黎野⋯。节奏大师Jlew与Friga突袭巴黎

## 朝向哪里

身体的话右脚略微在前

右手的话，起球位置虎口向前，用虎口瞄准。

## 如何发力

[髋](髋.md)主导，髋膝踝三倍伸展向上的力

## [辅助手](辅助手.md)

hooper  
quadratic functions

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F04%2F13%2F17-42-20-3a3e1390a5a5436828ce37b8aba2f38f-20240413174219-65501e.png)

## 一、垂直方向的自由落体运动

![image.png|400](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F04%2F13%2F17-18-44-f0a83307ef238fd3eb446c2ce0809789-20240413171844-097152.png)

根据身体重心和球的运动方向的异同，将投篮分为两个阶段。

<iframe src="https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fliuyishou%2Ftmp%2F2024%2F04%2F13%2F16-57-59-a53b030998c59cd68208ea6b300dc921-4%E6%9C%8813%E6%97%A5-1--69cce6.mp4" allowfullscreen="true" style="border-radius: 30px; overflow: hidden; border: 3px solid #ccc; width: 360px; height: 500px; display: block; margin: 20px auto; aspect-ratio: 9 / 16;" frameborder="0"></iframe><iframe src="https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fliuyishou%2Ftmp%2F2024%2F04%2F13%2F19-16-52-a08aab2aa94f35e06e07a9d0263e0606-4%E6%9C%8813%E6%97%A5-2--2e11a4.mp4" allowfullscreen="true" style="border-radius: 30px; overflow: hidden; border: 3px solid #ccc; width: 360px; height: 500px; display: block; margin: 20px auto; aspect-ratio: 9 / 16;" frameborder="0"></iframe>

### 1. 身体往下球往上（压）

这个阶段身体往下是因为屈髋

- 衡量屈髋一个很好的标准是，小腿骨和地面保持 90 度垂直的同时，降低身高，这样压缩的就是髋了，表现为屈膝的角度很小，最夸张的表现就是站着到往椅子上坐下的过程。
- 或者另一个最直观的形容就是屁股向后突出/移动。

球往上的方向是 [矢状面](矢状面) 方向，也就是脚尖的指向，这样最顺畅，最快。

### 2. 身体往上球往上（弹）

通过上个阶段髋和大腿都被压缩蓄力，然后依次伸髋，伸膝，伸脚踝，实现身体往上球往上，给球一个向上的初速度。
球往上的方向是人的中心和篮筐的连线方向。

而类似球到额头最高点停顿这种脱节的做法，其本质是降低了向上的初速度 cos V。反之越**连贯**，V 越大，最高点 h 越大，抛物线向篮筐的入射角变大，命中率提高。

**抛投**的本质就是不压腕，使得初始角度非常大的同时，增大力量和起球加速距离，给了一个极大的初速度，使得球能进入篮筐  
![image.png|400](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F04%2F13%2F18-15-33-3668bd112ca6f091d28ca85758c7c32c-20240413181532-7abb99.png)

![image.png|#pic_center|650](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F04%2F13%2F18-15-33-3668bd112ca6f091d28ca85758c7c32c-20240413181532-7abb99.png)

### 3. 甩腕

通过上个阶段，球已经有了很大的初速度，到达最高点时，通过甩腕可以导致甩鞭效应，进一步强化初速度 V

<iframe src="https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fliuyishou%2Ftmp%2F2024%2F04%2F27%2F00-35-34-43952f2736c922973fb6792112ebe1c8-iShot_2024-04-27_00.34.48-11d0c1.mp4" allowfullscreen="true" style="border-radius: 30px; overflow: hidden; border: 3px solid #ccc; width: 360px; height: 640px; display: block; margin: 20px auto; aspect-ratio: 9 / 16;" frameborder="0"></iframe>
所以到达高点，想要甩腕的时候，**手腕一定要放松**！篮球只有 600g，像一个小朋友的头一样，你会拼命去挤小朋友的头吗？

## 二、水平方向的匀速直线运动

![image.png|400](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F04%2F13%2F17-19-03-f0f0044d10f205a171fc786cce9f32ce-20240413171902-4adde5.png)

### 三、抛物线的平行线

将角度控制在 55 - 65 度，使得 V 不变的情况下，抛物线向篮筐的入射角变大，命中率提高。

## 三、流体力学

### 大小臂方向

![image.png|400](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F04%2F14%2F02-01-35-dba09f5d78f18c988e7c31404d7070c3-20240414020135-1777e0.png)  
最开始在矢状面，最后慢慢指向人的中心和篮筐的连线方向。也就是所谓的顶 - 转 - 压

<iframe src="https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2F9e20f478899dc29eb19741386f9343c8%2FVideo%2F2024%2F04%2F27%2F00-38-03-dfaf4be4fc8653117e29cc727d86c915-530_1714149450-748695.mp4" allowfullscreen="true" style="border-radius: 30px; overflow: hidden; border: 3px solid #ccc; width: 360px; height: 640px; display: block; margin: 20px auto; aspect-ratio: 9 / 16;" frameborder="0"></iframe>

## 四、实践

极限重心控制法：1 抬起脚尖 2 踮脚起跳，想象屁股往下坐

## 参考资料

[【篮球科学】投篮中的科学【数学】【物理】【英语】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1VT4y197cv/?spm_id_from=333.337.search-card.all.click&vd_source=dea414ee2d39e74f662ceec0edffdf24)



[Q2：如何达到 50%的三分命中率/辅助手.md]
==========================

---
draw:
tags: []
title: 辅助手
date created: 2024-08-25
date modified: 2024-11-12
---
 

B 站收藏了一个视频



[Q2：海燕计划/NFTurbo.md]
====================

---
draw:
tags: []
title: NFTurbo
date created: 2024-10-27
date modified: 2024-11-12
---

## 目标

熟悉一下[ElasticSearch](ElasticSearch.md)，对电商中除了报价之外的其他模块的代码实现有一个思路

## 模块

| 难度等级 | 适用人群 |
|----------|----------|
| ⭐☆☆☆☆ | 适合在校生、实习生 |
| ⭐⭐☆☆☆ | 适合实习生、应届生 |
| ⭐⭐⭐☆☆ | 适合应届生，工作1年内的人 |
| ⭐⭐⭐⭐☆ | 适合工作3年内的人 |
| ⭐⭐⭐⭐⭐ | 适合工作8年内的人 |

| 模块   | 模块名                  | 难度    | 难点&亮点                                                   | 技术列表                                                                               |
| ---- | -------------------- | ----- | ------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| 消息通知 | nft-turbo-notice     | ⭐☆☆☆☆ | 并发控制、CRUD、外部对接                                          | 滑动窗口、短信服务                                                                          |
| 网关   | nft-turbo-gateway    | ⭐⭐☆☆☆ | gateway的理解、统一鉴权、负载均衡、统一限流                               | spring-cloud-gateway、loadbalancer、nacos、sentinel、sa-token、switch表达式                |
| 认证   | nft-turbo-auth       | ⭐⭐☆☆☆ | 统一认证、分布式session                                         | sa-token、redis、dubbo、springmvc                                                     |
| 区块链  | nft-turbo-chain      | ⭐⭐⭐☆☆ | 外部对接、多chain切换、限流降级                                      | 策略+工厂模式、文昌链对接、sentinel、rocketmq                                                    |
| 用户   | nft-turbo-user       | ⭐⭐⭐☆☆ | 用户信息缓存、用户名判重、数据脱敏、数据加解密、分布式锁、短信发送、滑动窗口限流、缓存一致性、实名认证、排行榜 | caffeine、redis、jetcache、redisson、bloomfilter、oss、dubbo、虚拟线程                        |
| 藏品   | nft-turbo-collection | ⭐⭐⭐⭐☆ | 搜索、ES和DB动态切换、限流、防超卖                                     | elasticsearch、canal、redis、caffeine、jetcache、dubbo                                  |
| 订单   | nft-turbo-order      | ⭐⭐⭐⭐☆ | 订单重复提交、订单到期关闭、分库分表                                      | xxl-job、token、rocketmq、springevent、shardingjdbc、雪花算法、dubbo、责任链模式、虚拟线程、textblock    |
| 支付   | nft-turbo-pay        | ⭐⭐⭐⭐☆ | 重复支付、数据一致性、支付回调、数据对账、多付退款、渠道对接                          | seata、微信支付对接、xxl-job、分布式锁、限流、分布式事务                                                 |
| 交易   | nft-turbo-trade      | ⭐⭐⭐⭐⭐ | 秒杀、幂等控制、事务消息、分布式ID、库存扣减                                 | xxl-job、token、rocketmq、springevent、dubbo、责任链模式、虚拟线程、lua脚本、textblock、inventory hint |
| 后台管理 | nft-trubo-admin      | ⭐⭐⭐☆☆ | 改价、修改库存、用户冻结、下架                                         | sa-token、redis、延迟双删、缓存失效、鉴权                                                        |
| 玩法   |                      | ⭐⭐⭐⭐⭐ |                                                         |                                                                                    |
|      |                      |       |                                                         |                                                                                    |



[Q2：海燕计划/海燕计划$.md]
==================

---
draw:
tags: []
title: 海燕计划$
date created: 2024-08-12
date modified: 2024-11-12
---

1. hot200 足够
2. 极客时间 mysql 和 redis 的深度
3. 业务方面偏自由作文向，几乎一套模版可以应对全部

## [SMART 指标](SMART%20指标.md)

将五篇纯问题型的八股纳入到第二大脑中

[面试](面试.md)



[Q2：海燕计划/面试.md]
===============

---
draw:
tags: []
title: 面试
date created: 2024-09-29
date modified: 2024-11-12
---

我将课程内容整理成了以下 Markdown 表格格式:

| 模块             | 讲次   | 标题                                                   | 时长    |     |
| -------------- | ---- | ---------------------------------------------------- | ----- | --- |
| 开篇词            | 开篇词  | 以面试题为切入点，有效提升你的Java内功                                | 05:54 |     |
| 模块一 Java基础     | 第1讲  | 谈谈你对Java平台的理解？| 08:04 |     |
|                | 第2讲  | Exception和Error有什么区别？| 11:15 |     |
|                | 第3讲  | 谈谈final、finally、finalize有什么不同？| 11:04 |     |
|                | 第4讲  | 强引用、软引用、弱引用、幻象引用有什么区别？| 10:24 |     |
|                | 第5讲  | String、StringBuffer、StringBuilder有什么区别？| 12:00 |     |
|                | 第6讲  | 动态代理是基于什么原理？| 11:25 |     |
|                | 第7讲  | int和Integer有什么区别？| 11:05 |     |
|                | 第8讲  | 对比Vector、ArrayList、LinkedList有何区别？| 12:46 |     |
|                | 第9讲  | 对比Hashtable、HashMap、TreeMap有什么不同？| 12:16 |     |
|                | 第10讲 | 如何保证集合是线程安全的? ConcurrentHashMap如何实现高效地线程安全？| 10:47 |     |
|                | 第11讲 | Java提供了哪些IO方式？NIO如何实现多路复用？| 11:41 |     |
|                | 第12讲 | Java有几种文件拷贝方式？哪一种最高效？| 12:39 |     |
|                | 第13讲 | 谈谈接口和抽象类有什么区别？| 11:07 |     |
|                | 第14讲 | 谈谈你知道的设计模式？| 08:24 |     |
| 模块二 Java进阶     | 第15讲 | synchronized和ReentrantLock有什么区别呢？| 09:37 |     |
|                | 第16讲 | synchronized底层如何实现？什么是锁的升级、降级？| 11:03 |     |
|                | 第17讲 | 一个线程两次调用start()方法会出现什么情况？| 10:02 |     |
|                | 第18讲 | 什么情况下Java程序会产生死锁？如何定位、修复？| 09:38 |     |
|                | 第19讲 | Java并发包提供了哪些并发工具类？| 10:33 |     |
|                | 第20讲 | 并发包中的ConcurrentLinkedQueue和LinkedBlockingQueue有什么区别？| 08:37 |     |
|                | 第21讲 | Java并发类库提供的线程池有哪几种？分别有什么特点？| 12:31 |     |
|                | 第22讲 | AtomicInteger底层实现原理是什么？如何在自己的产品代码中应用CAS操作？| 11:04 |     |
|                | 第23讲 | 请介绍类加载过程，什么是双亲委派模型？| 13:59 |     |
|                | 第24讲 | 有哪些方法可以在运行时动态生成一个Java类？| 08:30 |     |
|                | 第25讲 | 谈谈JVM内存区域的划分，哪些区域可能发生OutOfMemoryError?               | 10:43 |     |
|                | 第26讲 | 如何监控和诊断JVM堆内和堆外内存使用？| 12:37 |     |
|                | 第27讲 | Java常见的垃圾收集器有哪些？| 12:26 |     |
|                | 第28讲 | 谈谈你的GC调优思路?                                          | 14:36 |     |
|                | 第29讲 | Java内存模型中的happen-before是什么？| 10:18 |     |
|                | 第30讲 | Java程序运行在Docker等容器环境有哪些新问题？| 10:25 |     |
| 模块三 Java安全基础   | 第31讲 | 你了解Java应用开发中的注入攻击吗？| 09:56 |     |
|                | 第32讲 | 如何写出安全的Java代码？| 10:32 |     |
| 模块四 Java性能基础   | 第33讲 | 后台服务出现明显"变慢"，谈谈你的诊断思路？| 10:26 |     |
|                | 第34讲 | 有人说"Lambda能让Java程序慢30倍"，你怎么看？| 11:20 |     |
|                | 第35讲 | JVM优化Java代码时都做了什么？| 10:08 |     |
| 模块5 Java应用开发扩展 | 第36讲 | 谈谈MySQL支持的事务隔离级别，以及悲观锁和乐观锁的原理和应用场景？| 09:25 |     |
|                | 第37讲 | 谈谈Spring Bean的生命周期和作用域？| 10:00 |     |
|                | 第38讲 | 对比Java标准NIO类库，你知道Netty是如何实现更高性能的吗？| 09:28 |     |
|                | 第39讲 | 谈谈常用的分布式ID的设计方案？Snowflake是否受冬令时切换影响？| 10:03 |     |
| 周末福利           | 周末福利 | 谈谈我对Java学习和面试的看法                                     | 07:19 |     |
|                | 周末福利 | 一份Java工程师必读书单                                        | 08:53 |     |
| 结束语            | 结束语  | 技术没有终点                                               | 03:36 |     |
|                | 结课测试 | Java核心技术的这些知识，你真的掌握了吗？| 00:38 |     |



[Q2：自媒体一万粉丝/爬虫计划.md]
====================

---
draw:
tags: []
title: 爬虫计划
date created: 2024-11-05
date modified: 2024-12-27
---

理解流媒体技术的原理：

HLS协议的工作机制  
m3u8播放列表的结构  
视频分片和串流的原理  
自适应码率的实现方式

我来解释一下你看到的网络请求情况：

从截图中可以看到，这些请求主要分为两种类型的文件：

1. `.mp4` 格式的文件 - 这些是视频片段
2. `.m3u8` 格式的文件 - 这是一种播放列表文件

这种模式是现代视频网站普遍采用的 HLS (HTTP Live Streaming) 流媒体技术：

1. `.m3u8` 文件是索引文件，包含了视频片段的播放顺序和地址信息。它告诉播放器应该按什么顺序播放这些视频片段。
2. `.mp4` 文件是实际的视频数据，但被切成了小片段（比如每个片段几秒钟）。从文件名可以看出，这些片段都是 160p 分辨率的视频片段。

这种技术的优点是：

- 可以实现自适应码率流媒体（根据网络状况自动调整视频质量）
- 支持视频跳转（不用下载整个视频文件）
- 有利于CDN缓存和分发
- 降低视频加载失败的风险（只需重新加载出错的片段）

所以真正的视频数据是那些.mp4 文件，但它们需要按照.m3u8 文件的指示顺序播放才能形成完整的视频内容。这就是为什么你会看到持续的网络请求 - 播放器在按需获取新的视频片段。



[Q3：Viva/python/SQLAlchemy.md]
==============================

---
draw:
tags: []
title: SQLAlchemy
date created: 2024-10-13
date modified: 2024-12-27
---

SQLAlchemy确实提供了连接池管理的功能，类似于MyBatis。SQLAlchemy的连接池管理是自动的，但你可以根据需要进行配置。以下是如何在SQLAlchemy中管理连接池的简要说明：

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import QueuePool

# 创建带有连接池配置的引擎
engine = create_engine('postgresql://user:password@localhost/dbname',
                       poolclass=QueuePool,
                       pool_size=5,
                       max_overflow=10,
                       pool_timeout=30,
                       pool_recycle=1800)

# 创建会话工厂
Session = sessionmaker(bind=engine)

# 使用会话
def use_session():
    session = Session()
    try:
        # 使用会话进行数据库操作
        # ...
        session.commit()
    except:
        session.rollback()
        raise
    finally:
        session.close()

# 在应用程序结束时
engine.dispose()

```

这个示例展示了如何在SQLAlchemy中配置和使用连接池：

1. 创建引擎时，我们使用 `create_engine` 函数并指定连接池的参数：
   - `poolclass=QueuePool`: 使用QueuePool作为连接池类型。
   - `pool_size=5`: 连接池中保持的连接数。
   - `max_overflow=10`: 允许创建的超出 `pool_size` 的额外连接数。
   - `pool_timeout=30`: 在等待连接超时前的秒数。
   - `pool_recycle=1800`: 自动回收连接的时间（秒）。

2. 创建会话工厂（Session）绑定到这个引擎。
3. 使用会话时，我们从会话工厂创建一个新的会话，使用完毕后关闭它。
4. 在应用程序结束时，调用 `engine.dispose()` 来正确关闭所有连接。

与MyBatis相比，SQLAlchemy的主要区别在于：

1. 自动管理：SQLAlchemy的连接池管理大部分是自动的，你只需要在创建引擎时进行配置。
2. 会话概念：SQLAlchemy使用会话（Session）概念，而不是直接管理连接。会话会自动从连接池获取和释放连接。
3. 配置灵活性：SQLAlchemy提供了多种连接池类型和丰富的配置选项，可以根据需求进行精细调整。
4. ORM集成：SQLAlchemy的连接池管理与其ORM功能紧密集成，使用起来更加无缝。

如果你需要更精细的控制，SQLAlchemy还提供了事件系统，允许你监听连接创建、连接池检出/归还等事件，以实现更复杂的连接管理逻辑。

你对SQLAlchemy的连接池管理还有其他问题吗？或者你想了解如何处理特定的连接池场景？



[Q3：Viva/python/pycache.md]
===========================

---
draw:
tags: []
title: pycache
date created: 2024-10-13
date modified: 2024-12-27
---

.pycache 文件夹是 [Python](Python.md) 的一个重要优化机制，用于存储字节码缓存文件。让我详细解释一下它的作用和工作原理：

```mermaid
graph TD
    A[Python 源文件 .py] -->|首次运行| B[编译为字节码]
    B --> C[执行字节码]
    B --> D[存储在 .pycache]
    D -->|后续运行| E{检查源文件是否修改}
    E -->|未修改| F[直接加载字节码]
    E -->|已修改| G[重新编译]
    F --> C
    G --> C
    G --> D

```

.pycache 文件夹的主要作用和特点：

1. 性能优化：
   - 当 Python 脚本首次运行时，解释器会将源代码编译成字节码。
   - 这些编译后的字节码被保存在.pycache 文件夹中，文件扩展名为.pyc。
   - 在后续运行时，如果源文件没有被修改，Python 可以直接加载字节码，避免重新编译，从而加快程序的启动时间。

2. 自动管理：
   - Python 解释器会自动创建和管理.pycache 文件夹。
   - 你通常不需要手动干预这个过程。

3. 版本兼容：
   - .pyc 文件名中包含 Python 解释器的版本信息，确保不同版本的 Python 不会混用字节码文件。

4. 源文件变更检测：
   - Python 会检查.pyc 文件的时间戳与对应的.py 文件。
   - 如果.py 文件被修改，Python 会重新编译并更新.pyc 文件。

5. 清理：
   - 你可以安全地删除.pycache 文件夹，Python 会在需要时重新创建它。
   - 这在某些情况下（如排除缓存引起的问题）可能会有用。

6. 发布考虑：
   - 在发布 Python 应用时，通常不需要包含.pycache 文件夹。
   - 可以在.gitignore 文件中添加 **pycache** 来避免将其包含在版本控制中。

7. 环境变量控制：
   - 可以通过设置 PYTHONDONTWRITEBYTECODE 环境变量为非空值来阻止 Python 写入.pyc 文件。

8. Python 3.8+的新特性：
   - 从 Python 3.8 开始，引入了 PEP 552，实现了更安全和高效的字节码缓存机制。

.pycache 是 Python 运行时优化的重要部分，通常不需要直接操作它。它的存在有助于提高 Python 程序的运行效率，特别是在大型项目中。

你对 .pycache 的工作机制或者如何在特定情况下管理它还有其他问题吗？



[Q3：Viva/python/包.md]
=====================

---
draw:
tags: []
title: 包
date created: 2024-10-13
date modified: 2024-12-27
---

## 包

**init**.py 文件在 Python 包中有特殊的作用。它主要有以下几个功能：
将目录标记为 Python 包：
当一个目录中包含 **init**.py 文件时，Python 会将该目录视为一个包。这允许你使用 import 语句从该包中导入模块。
初始化包：
你可以在 **init**.py 中放置一些初始化代码，这些代码会在包被导入时执行。
定义包的公共接口：
你可以在 **init**.py 中导入特定的类、函数或变量，使它们在包级别可用。
控制导入行为：
通过在 **init**.py 中定义 **all** 列表，你可以指定使用 from package import * 时应该导入哪些模块。
在你的情况下，infrastructure/**init**.py 是空的，这意味着它仅仅是将 infrastructure 目录标记为一个 Python 包。你可以根据需要在其中添加代码，例如：

这个理解不完全准确。让我来解释一下 Python 中关于模块和包的概念：

1. 模块（Module）:
   - 任何.py 文件都可以被视为一个模块。
   - 不需要 **init**.py 文件就可以成为模块。
2. 包（Package）:
   - 包是一种特殊的模块，它是一个包含 **init**.py 文件的目录。
   - **init**.py 文件（即使是空的）使得一个目录成为一个 Python 包。

```plaintext
my_project/
│
├── module_example.py
│
└── package_example/
    ├── __init__.py
    ├── module1.py
    └── module2.py

```

解释：

1. `module_example.py` 是一个模块。它可以直接被导入，如 `import module_example`。
2. `package_example` 是一个包。
   - 它包含 `__init__.py` 文件，这使得它成为一个包。
   - `module1.py` 和 `module2.py` 是包内的模块。

3. **init**.py 的作用：
   - 在 Python 3.3 之前，**init**.py 是必需的，用来将目录标记为 Python 包。
   - 在 Python 3.3 及以后，引入了"命名空间包"的概念，允许没有 **init**.py 文件的目录也被视为包，但这种包有一些限制。

4. 导入方式：
   - 模块：`import module_example`
   - 包：`import package_example` 或 `from package_example import module1`

5. **init**.py 的其他用途：
   - 可以用来执行包的初始化代码
   - 可以定义 `__all__` 变量来指定使用 `from package import *` 时应该导入的内容

总结：

- 单个.py 文件就是一个模块
- 包含 **init**.py 文件的目录是一个包
- 包是一种特殊的模块，它可以包含其他模块

这种结构允许你组织和管理大型 Python 项目，提高代码的可维护性和可重用性。你对模块和包的概念还有其他问题吗？



[Q3：Viva/viva/0. 我的奥德赛.md]
==========================

---
draw:
tags: []
title: 0. 我的奥德赛
date created: 2024-01-03
date modified: 2024-12-27
---
- [一、动机、渴望](#%E4%B8%80%E3%80%81%E5%8A%A8%E6%9C%BA%E3%80%81%E6%B8%B4%E6%9C%9B)
	- [1.1 为什么学，后高考时代？](#1.1%20%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AD%A6%EF%BC%8C%E5%90%8E%E9%AB%98%E8%80%83%E6%97%B6%E4%BB%A3%EF%BC%9F)
	- [1.2 心理准备](#1.2%20%E5%BF%83%E7%90%86%E5%87%86%E5%A4%87)
- [二、预置概念](#%E4%BA%8C%E3%80%81%E9%A2%84%E7%BD%AE%E6%A6%82%E5%BF%B5)
	- [2.1 四种单词量](#2.1%20%E5%9B%9B%E7%A7%8D%E5%8D%95%E8%AF%8D%E9%87%8F)
	- [2.1 词频意识](#2.1%20%E8%AF%8D%E9%A2%91%E6%84%8F%E8%AF%86)
	- [2.2 间隔复习](#2.2%20%E9%97%B4%E9%9A%94%E5%A4%8D%E4%B9%A0)
- [三、输入](#%E4%B8%89%E3%80%81%E8%BE%93%E5%85%A5)
	- [3.1 阅读a阶段](#3.1%20%E9%98%85%E8%AF%BBa%E9%98%B6%E6%AE%B5)
		- [逻辑推导](#%E9%80%BB%E8%BE%91%E6%8E%A8%E5%AF%BC)
		- [刻意练习](#%E5%88%BB%E6%84%8F%E7%BB%83%E4%B9%A0)
	- [3.2 听力a阶段](#3.2%20%E5%90%AC%E5%8A%9Ba%E9%98%B6%E6%AE%B5)
		- [逻辑推导](#%E9%80%BB%E8%BE%91%E6%8E%A8%E5%AF%BC)
		- [刻意练习](#%E5%88%BB%E6%84%8F%E7%BB%83%E4%B9%A0)
- [四、输出](#%E5%9B%9B%E3%80%81%E8%BE%93%E5%87%BA)
	- [4.1 口语a阶段](#4.1%20%E5%8F%A3%E8%AF%ADa%E9%98%B6%E6%AE%B5)
		- [几个里程碑：](#%E5%87%A0%E4%B8%AA%E9%87%8C%E7%A8%8B%E7%A2%91%EF%BC%9A)
		- [逻辑推导](#%E9%80%BB%E8%BE%91%E6%8E%A8%E5%AF%BC)
		- [刻意练习](#%E5%88%BB%E6%84%8F%E7%BB%83%E4%B9%A0)
	- [4.2 写作a阶段](#4.2%20%E5%86%99%E4%BD%9Ca%E9%98%B6%E6%AE%B5)
		- [本质连接是什么？终点是什么？](#%E6%9C%AC%E8%B4%A8%E8%BF%9E%E6%8E%A5%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E7%BB%88%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F)
		- [语法的掌握程度？](#%E8%AF%AD%E6%B3%95%E7%9A%84%E6%8E%8C%E6%8F%A1%E7%A8%8B%E5%BA%A6%EF%BC%9F)
		- [沉浸在写作社区中](#%E6%B2%89%E6%B5%B8%E5%9C%A8%E5%86%99%E4%BD%9C%E7%A4%BE%E5%8C%BA%E4%B8%AD)
	- [4.3 口语b阶段](#4.3%20%E5%8F%A3%E8%AF%ADb%E9%98%B6%E6%AE%B5)
- [五、资料与工具](#%E4%BA%94%E3%80%81%E8%B5%84%E6%96%99%E4%B8%8E%E5%B7%A5%E5%85%B7)

## 前言

The English Odyssey.  
记录一个传统的中国学生，  
起点：在结束中学的英语教育后，  
终点：变成一个能真正运用这门语言的人  
之间的冒险，奥德赛！

我一共花了 xxxx 时间，取得了什么结果，托福多少分，油管无压力，进外企。想把整个过程中总结出的一些最佳实践分享出来。当然也可能会有知识诅咒。

Well-educated 的英语和三十秒露馅的英语，深入的文化理解，深入的一对一的理解

知识诅咒：指的就是**一旦我们自己知道某样东西，我们就会发现很难想象不知道它的时候会是什么样子**。

## 一、动机、渴望

### 1.1 后高考时代为什么还要学习英语

- 装逼
- 提高上限：更多的信息，比如很多框架的官方文档，可以模糊提高各个领域的上限
- 语言之美：When you are old
- 沉没成本：三年级到大学，前期投入太多，不转化成真正能力的话，==沉没成本太大==
- 存量转化：而且之前输入积累远远大于输出，即使不学习新的输入，将已有的全部转化成主动输出，就已经非常可观了

### 1.2 心理准备

- 没有速成，需要大量时间，但是好的方法和认知可以让你找到门路，不会有劲无处使，会让过程变得科学可靠，可落地，易于操作。

## 二、几个前置概念

### 2.1 四种单词量

 拆解成一个个映射关系

 1. 形 - 意单词量 视力 - 想法
 2. 音 - 意单词量 听音 - 想法
 3. 意 - 声单词量 想法 - 发音
 4. 意 - 形单词量 想法 - 拼写/手写

### 2.1 词频意识

1. 不同的词频意味着不同的性价比
2. 克拉克语言习得的 N+1 原理的本质是词汇由高频到低频的逐步扩展
 

### 2.2 间隔复习

间隔复习是最省时高效的记忆方式！遗忘曲线、supermo 算法

### 2.3 多义词

先记住一个意思，不要让其他意思把自己搞乱，以后再去语境中慢慢体会各个意思的潜在联系

## 三、输入

### 3.1 阅读 a 阶段

#### 逻辑推导

本质连接是什么？终点是什么？

1、听力和手没有关系， 做笔记、边听边写，全是假的，没有用，做听力就要解放双手用耳朵

2、听力的本质是：一个==外界的声音==和你==脑中的意思==能产生联系 安瑞额 -> 区域 穿克 ->轨迹

3、听力好的绝对终点是：一大段英语的声音袭来，你能清晰地知道她的每个词表达了啥，需要苦练十年

4、单词的性价比不一样，有的值钱有的不值钱，听懂 " 一 z"，对 99% 的句子都有帮助，能听懂 " 安色弱怕了几 "，对 0.001% 的句子有用

5、听力好的相对终点是：一大段英语声音袭来，其中==高频单词==的声音，和你脑中的意思，已经具备了联系，只需要苦练几月

#### 刻意练习

词频驱动  
很多词频透析类型的软件，可以标注出待阅读文本中的生词  
文本透析

### 3.2 听力 a 阶段

#### 逻辑推导

本质连接是什么？终点是什么？

1、听力和手没有关系，做笔记、边听边写，全是假的，没有用，做听力就要解放双手用耳朵

2、听力的本质是：一个==外界的声音==和你==脑中的意思==能产生联系 安瑞额 -> 区域 穿克 ->轨迹

3、听力好的绝对终点是：一大段英语的声音袭来，你能清晰地知道她的每个词表达了啥，需要苦练十年

4、单词的性价比不一样，有的值钱有的不值钱，听懂 " 一 z"，对 99% 的句子都有帮助，能听懂 " 安色弱怕了几 "，对 0.001% 的句子有用

5、听力好的相对终点是：一大段英语声音袭来，其中==高频单词==的声音，和你脑中的意思，已经具备了联系，只需要苦练几月

#### 刻意练习

1. 低效版：闲看美剧、闲听 bbc，因为不符合刻意练习
2. 背！
3. 怎么背？
	1. 用软件、间隔复习更科学
	2. 闭眼背！因为 3.2.1.1
	3. 两种路径
		1. 高效枯燥版：直接找一本单词书，选高频的 7000 个背完背熟
		2. 高效有趣版：==频率逼近法==，
			1. 选一门考试，找到所有试卷，比如 100 篇，写个脚本跑出 100 篇的总词频，选出 100 篇的高词频
			2. 每天 n 篇，做之前，写个脚本找到这 n 篇中属于高词频的部分，导入墨墨背单词，背完，然后听录音做题
			3. 优点：依托于具体内容不枯燥、先背后听没有痛苦、有答题和分数反馈爽歪歪、顺便捞一门成绩
4. 背多少？
	1. 5000，初步具备听力能力
	2. 无痛听懂油管的常见内容可能需要一万加

https://v2ex.com/t/489712

## 四、输出

对于主动输出来说，无论是口语还是写作，都是一个模式，即 ==准确的用词 -> 准确的词间关系 -> 准确的句子 -> 准确的句间关系 -> 好的篇==。从实施上，关键要素有二，一是起码拥有上千个脑中意到合理单词选择的映射关系，二是需要有完备的语法知识体系。关于 " 准确的用词 "，我主要放在了口语 a 阶段。关于 " 准确的词间关系 -> 准确的句子 -> 准确的句间关系 -> 好的篇 "，我主要放在了写作 a 阶段。  
上面这个阶段结束以后有了基本的词汇的驾驭能力，才可以真正雕琢写作，不然比如简单的 " 真的首饰假的首饰 " authentic fake 都用不好。

### 4.1 口语 a 阶段

#### 几个里程碑

1. 音标体系
2. 高频主动词汇积累
3. 语法体系的应用
4. 人工智能和实地生活 打磨细节

#### 逻辑推导

1. 口语的本质是：一个==脑中的念头==一动，立刻想到==发什么声音== 具体的 - 死波赛飞可 确实 - 一个 zan 克特力
2. 口语好的绝对终点是：脑中的任何念头一起，你都能立刻发出对应的声音，需要苦练十年
3. 单词的性价比不一样，有的值钱有的不值钱，脑中 " 是 " 的念头一起，想到发出 " 一 z"，对 99% 的表达场景都有帮助，脑中 " 人类学 " 的念头一起，想到发出 " 安色弱怕了几 "，对 0.001% 的表达场景有用
4. 口语好的相对终点是：脑中的任何念头一起，其中高频意念，你都能立刻发出对应的声音，只需要苦练几月
5. 如何达到==高频意念==和发什么声音的连接？

#### 刻意练习

1. 音标的符号体系，从发错了，到发不准。发错和发不准是有区别的！
2. 听力词汇 5000 以上，具体实操可参考听力章节

```Java
	听力是口语的前提，没有说不出来的话，只有听不懂的话
```

1. 舌位知识

```Java
	不求地道，只求自洽，以后慢慢迭代，我现在的认知是 /a/ 你叫他阿猫阿狗都行，但是你每次都得这么叫！
	
	现在不管怎么进化，都是自以为的更标准，没用！
	
	现在练一百遍不得其意，后面慢慢就会懂，因为会发生，你说了一个单词，连外教听不懂，那么就是你这个单词的元音辅音出现问题了  
	
	根据天赋，会越来越趋近标准的
	
	以后不管中国人还是外国人说你发音有问题，那你就进一步问他，哪个单词，哪个音节，哪个元音或者辅音？然后基于以上认知，你就可以改变了！
```

1. 背音标

```Java
	背一遍单词音标（前3000） electronic    approach -> /əˈproʊtʃ/

	重音
```

1. 连读知识 一周

```Java
	任何让你口语更地道炫酷的东西，都没必要话太多时间
	
	连读、吞音等等不需要怎么看，慢慢就懂了，核心就是尽可能凑成   辅元
	
	口语的本质不在于此！
```

1. 找老外说话

```Java
	开始找老外说话，从来不和中国人练口语、英语角全是假的，低效，很难让你持久地输出！

	录音！永远不要在没有录音的情况下说英语

```

1. 复盘录音，梳理其中反应的你的 念头 - 单词 的映射，把不对的地方查字典纠正并导入到墨墨背单词
2. 用看句子挖空想单词的方法 背单词！具体可参考****

### 4.2 写作 a 阶段

#### 逻辑推导

本质连接是什么？终点是什么？

1. 与谈吐一样，写作能力是超越语言的，具体到英语的写作能力，又可以拆为用词准确能力，如口语 a 阶段，逻辑表述能力，类比与或非循环结构的使用，和词句意境能力。
2. 什么是好的写作成果 ==准确的用词 -> 准确的词间关系 -> 准确的句子 -> 准确的句间关系 -> 好的篇==
3. ==准确的用词== 这部分在口语 a 阶段已经初步达到
4. ==准确的词间关系 -> 准确的句子 -> 准确的句间关系 -> 好的篇== 这部分对于口语交流和书面写作来说精确和严谨程度的要求是不同的
5. 重点是词 ->篇的网络结构 完整，每个小点都能做到主动输出

```Java
	对于语法的掌握应该兼具体系化与模糊化，高频、常见即可
```

#### 刻意练习

1. 沉浸在写作社区中，如 Medium，摒弃旧的经济学人，利用好目前 app 的兴趣推流
2. 找托福的五道题，先完成，再完美
3. 利用《新编高级英语语法》搭建 ==准确的词间关系 -> 准确的句子 -> 准确的句间关系 -> 好的篇==网络，语法是逻辑工具，主动词汇的积累好比编程中的变量赋值，而语法相当于与或非的操作符和 ifelse 的控制语句，有了数据再加上动作，才能让一个程序跑起来。
4. 在 Medium 中逐个写托福题目，用 Grammarly 修改语法，并纳入 3 的网络体系中，用 ChatGPT 精进用词
5. 找大量的托福雅思高分范文，分析他们的词 ->篇结构，纳入到 3 点网络体系中

比如这是一个中国人的写作，找到错的最厉害的五个点  
通过大量的写进入写作的阶段，通过少量的改，实现高频和准确高性价比的提高  
明天尝试打通 anki 的 api

口录，转文字整理到，每天刷 medium 找到适合自己的英语 feeds，培养英文阅读习惯  
两个月的时间，先争取把 69 个 tpo 写完，再看一个新的境界，收集 180 个错误，表达 200 多个观点  
写完 30 篇的时候发视频

语言最终是用来表达思想的。所以，思考方式格外重要。大多数人不懂得如何正确思考，乃至于所要表达的东西漏洞百出却又不自知。这个问题导致的更多麻烦，很多的时候被一些英语老师归咎于 " 东西方文化差异 "，这是和稀泥的一种说法——事实上，这跟文化没多大关系，而是思考能力问题。关于文化差异，首先需要的是尊重，其次需要的是理解。而思考能力差异，则是另外一回事，所表达的内容千疮百孔，无论如何都只能怪思考能力差，而与文化差异毫无关系。

不能 " 只学英语 "，这就好像用计算机绘图，只学 "Photoshop" 是不够的，要有足够的色彩、手绘基本训练，才能够自由创作一样。有几本书，建议所有想获得正确思考能力，进而表达清晰的人阅读。

- Thought and Knowledge: An Introduction to Critical Thinking 4th edition, by Diane F. Halpern
- Thinking and Deciding, 4th edition, by Jonathan Baron
- Argumentation: The Study of Effective Reasoning by Zarefsky David (Audiobook)
- Craft of Research, by Wayne C. Booth, Gregory G. Colomb, Joseph M. Williams
- Good Reasoning Matters!: A Constructive Approach to Critical Thinking 3rd edition, by Leo A. Groarke, Christopher W. Tindale

另外，还有三本关于文风（Style）的书籍也一定要看：

- A Plain English Handbook (1998), from Security and Exchange Commission, Prefaced by Warren E. Buffett ([http://www.plainlanguage.gov](http://www.plainlanguage.gov/))
- On Writing: A Memoir of The Craft by Stephen King (with audiobook) Style: Toward Clarity and Grace by Joseph M. Williams

另外，还有一个 TTC (The Teaching Company) 的视频教程，是我所见过最好的写作课程：

- Building Great Sentences。

## 五、精进

### 阅读 b 阶段

沉浸在写作社区中，如 Medium，摒弃旧的经济学人，利用好目前 app 的兴趣推流

推特、DIscord 等英语社区中学习 " 当代 " 的英语表达，各种梗

### 听力 b 阶段

油管、博客、美剧

### 口语 b 阶段

口语音视频被 AssemblyAI 处理成分角色文本，ChatGPT 根据语境提出修改意见，我讲修改意见归纳到主动词汇积累和语法网络中。
ChatGPT 的语境处理能力弥补了直接将 STT 的文本丢给 Grammarly 的缺陷

ElevenLabs 生成自己的口语模型，听到自己口语练习的终极形态

出国旅游、出国工作

### 写作 b 阶段

写作的话，是思想打底，文以载道

逐渐体现对于词汇细微之处的驾驭，是上一个阶段的深化。" 雪下得紧了 "

学会反范式，通过破坏语法规则，实现一些高阶表达

## 写在最后

花了多少时间精力，收获后端开发的雏形，尝试很多工具

写作是超越语言的，技能的学习其实也是超越语言的，什么语言的资料并不重要，重要的是什么能让你接近本质，掌握精髓

学习一门技能最高效的方式是，找到这项技能的第一性 (各种映射)，通过持续可理解性输入，做大量刻意练习！

而所有的这一切的第一性原理是：兴趣和热爱！

## 资料与工具

- 间隔复习工具：默默背单词、Anki(API)
- 单词词库：
- 音标知识：B 站英语兔
- 读音示范：B 站英语兔、油管瑞秋英语
- 语法知识：B 站英语兔可提供知识体系、章振邦的《新编高级英语语法》可提供详尽细节
- 音频转文字：网易见外工作台、AssemblyAI
- 油管字幕下载：DownSUB
- 活的 native speaker：很多口语平台，如 Cambly，可以低至 xx/h
- 考试目标物：托福雅思，在小站、考满分等平台均有高质量真题资料
- 精确的语法修改：Grammarly(比 GPT 精确)
- 为语境提供贴切的词汇建议：ChatGPT

## 详细操作展示

### 挖空句背单词

### 词汇量不是问题，概念量才是瓶颈



[Q3：Viva/viva/CICD for viva.md]
===============================

---
draw:
tags: []
title: CICD for viva
date created: 2024-12-27
date modified: 2024-12-28
---
1. 本地开发 Viva 项目代码
2. 推送到 GitHub，触发 GitHub Actions, GitHub Actions 执行以下步骤：
	- 检出代码
	- 设置 Docker Buildx
	- 登录 Docker Hub
	- 构建 Docker 镜像并推送到 Docker Hub
	- 前端镜像：your-dockerhub-username/viva-frontend:latest
	- 后端镜像：your-dockerhub-username/viva-backend:latest
	- 将配置文件传输到服务器 B：
3. 在服务器 B 上执行部署：
	- 拉取最新的 Docker 镜像
	- 停止旧容器
	- 启动新容器

## 团队协作开发

1. **仓库访问权限设置**：

```plaintext
GitHub 仓库设置流程：
1. 进入仓库 Settings → Collaborators and teams
2. 添加团队成员，可以设置不同权限级别：
   - Read: 只能查看和克隆
   - Write: 可以推送代码
   - Admin: 可以管理仓库设置
```

2. **分支保护策略**：

```plaintext
保护主分支设置流程：
1. 进入 Settings → Branches
2. 添加分支保护规则：
   - 要求 Pull Request 审查
   - 禁止直接推送到主分支
   - 要求状态检查通过才能合并
```

3. **工作流程建议**：

```yaml:.github/workflows/deploy.yml
name: Deploy Viva Project

on:
  push:
    branches:
      - main          # 生产环境部署
  pull_request:       # 添加 PR 触发器
    branches:
      - main

jobs:
  test:               # 添加测试任务
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Run tests
        run: |
          # 添加测试命令
          echo "Running tests..."
          
  deploy:
    needs: test       # 需要测试通过才能部署
    if: github.ref == 'refs/heads/main'  # 只在主分支部署
    runs-on: ubuntu-latest
    
    steps:
      # ... 之前的部署步骤保持不变 ...
```

1. **环境分离**：

```yaml:docker-compose.yml
services:
  frontend:
    image: your-dockerhub-username/viva-frontend:${TAG:-latest}  # 使用环境变量
    # ...

  backend:
    image: your-dockerhub-username/viva-backend:${TAG:-latest}   # 使用环境变量
    environment:
      - NODE_ENV=${NODE_ENV:-production}
      - DATABASE_URL=${DATABASE_URL}
    # ...
```

1. **团队开发流程**：

```plaintext
标准开发流程：

1. 克隆仓库
   git clone https://github.com/your-org/viva.git

2. 创建功能分支
   git checkout -b feature/new-feature

3. 开发并提交
   git add .
   git commit -m "feat: add new feature"
   git push origin feature/new-feature

4. 创建 Pull Request
   - 在 GitHub 上创建 PR
   - 等待代码审查
   - CI 自动运行测试

5. 合并到主分支
   - 审查通过后合并
   - 自动触发部署
```

1. **建议的项目结构**：

```Java
.
├── .github
│   └── workflows
│       ├── deploy.yml          # 生产环境部署
│       └── pr-check.yml        # PR 检查
├── docker-compose.yml          # 生产环境配置
├── docker-compose.dev.yml      # 开发环境配置
├── viva-frontend
│   ├── Dockerfile
│   └── nginx.conf
└── viva-backend
    └── Dockerfile
```

2. **环境变量管理**：
- 在 GitHub 仓库设置中配置 Secrets：
  - `DOCKERHUB_USERNAME` louisleonard
  - `DOCKERHUB_TOKEN` dckr_pat_U2MIMviLW8x3YR5kAQS5k8RBrkA
  - `SERVER_B_HOST` 43.134.93.160
  - `SERVER_B_USERNAME` root
  - `SERVER_B_PASSWORD` Lys876538875..
  - `DATABASE_URL`
  - 其他环境特定的变量

1. **代码审查流程**：

```plaintext
PR 审查清单：
1. 代码质量
2. 测试覆盖
3. 文档更新
4. 性能影响
5. 安全考虑
```

这样的设置可以让团队成员：

- 使用自己的 GitHub 账号进行开发
- 通过 PR 提交代码变更
- 自动运行测试和检查
- 只有通过审查的代码才能部署到生产环境
- 保持代码质量和部署的安全性

建议：

1. 为团队制定明确的开发规范
2. 使用语义化版本控制
3. 保持良好的文档习惯
4. 定期进行代码审查会议
5. 建立开发环境和测试环境

这样可以确保团队协作顺畅，同时保持部署的安全性和可控性。

## 为什么用 docker

### 通过 Docker 镜像部署（当前方式）

优点：

1. **环境一致性**
   - 开发、测试和生产环境完全一致
   - 避免了"在我电脑上能运行"的问题

2. **版本控制**
   - 每个镜像都有具体的标签
   - 方便回滚到之前的版本
   - 可以保留历史版本

3. **隔离性**
   - 应用和依赖都封装在容器内
   - 不同应用间互不影响
   - 更安全，降低安全风险

4. **可移植性**
   - 可以轻松迁移到其他服务器
   - 支持横向扩展
   - 便于实现负载均衡

缺点：

1. **构建时间**
   - 需要额外的时间构建镜像
   - 需要推送到镜像仓库

2. **存储空间**
   - 需要额外的镜像存储空间
   - Docker Hub 带宽消耗

3. **学习成本**
   - 需要了解 Docker 相关知识
   - 配置相对复杂

### 直接部署源代码

优点：

1. **部署速度快**
   - 无需构建镜像
   - 直接传输源代码

2. **简单直接**
   - 配置较少
   - 学习成本低

3. **资源占用少**
   - 不需要额外的镜像存储
   - 服务器负担小

缺点：

1. **环境依赖**
   - 需要在服务器上安装所有依赖
   - 可能出现环境不一致问题

2. **版本管理难**
   - 回滚较麻烦
   - 难以维护多个版本

3. **扩展性差**
   - 迁移服务器需要重新配置环境
   - 横向扩展复杂

### 建议

对于现代应用开发，特别是微服务架构，推荐使用 Docker 方式，因为：

1. 长期维护成本更低
2. 更适合团队协作
3. 更容易实现 DevOps
4. 更好的可扩展性

但如果是小型项目或个人项目，直接部署源码也是可以的，主要看：

1. 项目规模和复杂度
2. 团队大小
3. 维护周期
4. 扩展需求



[Q3：Viva/viva/Session Auth.md]
==============================

---
draw:
tags: []
title: Session Auth
date created: 2024-10-02
date modified: 2024-11-12
---



[Q3：Viva/viva/Untitled.md]
==========================

---
draw:
tags: []
title: Untitled
date created: 2024-10-07
date modified: 2025-01-08
---

er



[Q3：Viva/viva/产品/Untitled.md]
=============================

---
draw:
tags: []
title: Untitled
date created: 2024-10-26
date modified: 2024-11-12
---



[Q3：Viva/viva/产品/Viva.md]
=========================

---
draw:
tags: []
title: Viva
date created: 2024-02-16
date modified: 2024-12-27
---

## 商业化

**国内的英语学习是没有全链路数字化的管理系统的**

## 主要功能

功能是用户上传中文文章后，会分词，然后用户输入自己认为的英文翻译，网站会帮忙检查对错，并生成闪卡用于间隔复习。

with the function of users uploading Chinese articles, which will be segmented, and then users inputting their own English translations, and the website will help check the correctness and generate flashcards for spaced repetition review.

## 愿景

**我想做的是英语世界的溯源系统**  
还是软件开发中数字化+智能化的方式  
网站本身首先是一个传感器，将过去白白流逝掉的英语使用信息数字化保存  
其次是一个算法平台，从数据中挖掘规律，找到学习中最痒的那个点，反馈用户




**通过两三年的努力，终于能把问题和需求定清楚，这样占了我之前那个想法，后代产品的核心竞争力就在于他能够提出需求，相比之下实现需求反而变得简单**

最终的目的是在英语这个一切有解，一切能具体到生活的，理想的，学习试验田上，实现学习过程的数字化，找到学习中最痒的那个点，实现有效积累，快速进步，降低总的时间成本。

- 抖音、小红书能利用算法挖掘自媒体内容中的优点、卖点，同时激发新的内容创作。我们为什么不能数字化我们和世界的交互过程，并利用算法挖掘和作用呢？
- 好的产品，应该帮助用户回归本真，回到第一性原理最能发挥的动作中，并让其低成本重复  
 英语相比其他学科的知识图谱来说已经是一个结构最简单的图结构了

## 主要思想

对英语的掌握，本质是脑中四种映射的数量。

```mermaid
graph TD
    A((英语学习)) --> B[被动映射]
    A --> C[主动映射]
    
    B --> D[听力词汇量]
    B --> E[阅读词汇量]

	C --> T[想法]
	T --> H[表达（单词或短语）]
    H --> F[口头版本——声音]
    H --> G[书面版本——拼写]
style H fill:#bbf,stroke:#333,stroke-width:4px;
style T fill:#bbf,stroke:#333,stroke-width:4px;
```

|      |                |                                                                                     |
| ---- | -------------- | ----------------------------------------------------------------------------------- |
| 主动映射 | active_mapping | 我想『打』死你 -> beat<br>给我一『打』鸡蛋 -> dosen                                                |
| 想法   | thought        | 是主动映射的 key，由 sentence 和 句子中的聚焦位置 focus_word 共同组成，这样就能精确定位出一个想法，比如：我想『打』死你，给我一『打』鸡蛋。|
| 表达   | expression     | 是主动映射的 value的 object，包括『口头版本』written_expression『书面版本』oral_expression                |
|      |                |                                                                                     |

## 写作部分，通过 essay 增加 mapping 数量

```mermaid
graph LR
E[essay] --> |分词| A
A[active_mapping_list] --> m1[mapping1]
A --> m2[mapping2]
A --> m3[mapping3]
A --> mn[mapping ……]
```

|        |                 |                                          |
| ------ | --------------- | ---------------------------------------- |
| 文章     | essay           | 是用户输入，是念头的集合                             |
| 用户的表达  | user_expression |                                          |
| AI 的建议 | ai_review       | 包括 is_correct 和 ai_expression            |
| 映射卡片   | mapping_card    | 包括 thought，user_expression，ai_expression |

### 做题

|     | 流程                                          |     |
| --- | ------------------------------------------- | --- |
|     | 上传 essay                                    |     |
|     | 中文分词，获得 active_mapping_list，返回 mapping_card |     |
|     | 用户在 sentence_card 中填写 mapping_card          |     |
|     | ai 检查 mapping_card                          |     |
|     | 用户添加需要间隔复习的 mapping_card                    |     |

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F28%2F21-05-19-605ee8a2ee4a897dae41daca6d5f1458-202409282105036-5ffdda.png)

![image.png|1800](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F28%2F21-05-47-b3740f1d2f46b7b4d79b02c6451c08c7-202409282105783-53b3e4.png)

### 错题本

```mermaid
graph LR
C[mapping_card] --> D[做题阶段]
D --> d1[1. 填入自认为的表达]
D --> d2[2. AI review]
D --> d3[3. 加入错题本]
C --> A[错题本阶段]
A --> e1[1. 正面看到 thought、之前的错误表达]
A --> e2[2. 填入自认为的表达]
A --> e3[3. 和正确答案对照]
```

---

[口语课复盘 demo](口语课复盘%20demo.md)

[Streamlit](Streamlit.md)

![image.png#pic_center|650](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F06%2F19%2F15-29-45-ac8cf2bc82ad77af24083935650a61e1-20240619152945-4a0165.png)

```mermaid
graph TD
    subgraph Browser["浏览器"]
        B1["文章"]
        B2["文章"]
        B3["音频、视频"]
    end

    subgraph Observation["观测与衡量"]
        O1["COCA"]
        O2["真题"]
    end

    A[ANKI]
    Brain["Brain"]
    R["真人对话"]

    B1 -->|场景驱动| A
    B2 -->|场景驱动| A
    B3 -->|场景驱动| A
    R -->|场景驱动| A
    O1 -->|词频驱动| A
    O2 -->|词频驱动| A
    Browser -->|主动词汇与被动词汇的差| A
    A --> Brain
    Brain -->|声音听写| A

    classDef default fill:#f9f,stroke:#333,stroke-width:2px;
    classDef observation fill:#bfb,stroke:#333,stroke-width:2px;
    class O1,O2 observation;
    
    style A fill:#bbf,stroke:#333,stroke-width:4px;
    style Brain fill:#fbb,stroke:#333,stroke-width:4px;
```

## 过去的痛点

1. 自己无法控制复习强度，要么记不牢，要么记忆负担过大，
	1. 通过自定义间隔复习的参数可以解决
2. 出现的同义词无法纵览、辨析、比较记忆
	1. 通过自己 select 数据可以解决
3. 无法与口语课的数据联动
4. 本质是因为数据不在我们自己的手里

### 功能

#### 利用 Anki 实现间隔复习

- 卡片正面
	- 该单词用 ChatGPT 生成的中文例句
- 卡片背面
	1. 单词的拼写
	2. 单词的发音
		- 用 11labs 生成的自己的声音模型

#### 一键同步到 excel

excel 与 anki 的公用主键应该是单词的拼写  
将 excel 作为同义词、例句用法、聚类整理的地方  
同时将 anki 记忆数据定期 attach 到 excel

- anki connect 取出单词数据
- 将关心的字段增量更新到 excel 表格

#### 一键聚类

- telegram 中 /cluster 触发
- 复习的时候对于脑中无法正确定位的同义词打 flag1
- Anki-Connect 找到 flag1 的单词
- GPT 做聚类分析

### 一键生成 instruction

将外教课的 url 传给 bot，返回 instruction 的 txt 文件  
可以用 txt 文件让 chatgpt 生成复习 json

### Vault 内近义词检索

#### 一键同步口语课数据

- 上传口语课视频
- 得到复盘内容的同时
- 将本次口语课自己的单词使用情况累计到词库数据上

### 需要的工具

#### 从墨墨导出数据

- app 端加入收藏
- 网页端云词库复制出来

#### Anki 的原理和使用

#### Python

- Telegram Bot 开发
- elevenlabs、openai 的接口调用
- Anki connect 实现 Anki 数据的可编程

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F10%2F10%2F01-24-30-765108e31a7ab1fa22828f6997611245-202410100124997-2c5b4a.png)



[Q3：Viva/viva/产品/向业内人士征求意见.md]
==============================

---
draw:
tags: []
title: 向业内人士征求意见
date created: 2024-11-06
date modified: 2024-11-12
---
1. 共识是否成立，
	1. 无关语言的行文思路
	2. 英语写作能力，能把思想变成英文表达
2. 在托福写作的一对一中，上述哪个阶段占大头
3. 对于1.2 来说，过去的一对一是如何训练的
4. 1.2 做好后的提分空间是多少到多少
5. 对于 viva 中的语料有什么建议吗？
6. 第一步忽略同义词的错误是否合理



[Q3：Viva/viva/产品/哈希主义.md]
=========================

---
draw:
tags: []
title: 哈希主义
date created: 2025-01-26
date modified: 2025-01-26
---

## 1. 名称背后的"主义"感

1. **理念或美学流派**
    
    - 在艺术、思想史上，"××主义"通常意味着一种独特的视角、方法或主张，例如"抽象主义""未来主义"等。
    - "哈希主义"则让人联想到"以哈希为中心的世界观"——也许是某种"碎片化、再映射、再组合"的哲学或思维体系。
2. **技术与思想的融合**
    
    - 如果你想突出"哈希"不仅是技术工具，更是一种"思维方式"或"组织方式"，称之为"主义"能给人强烈的概念冲击。
    - 这可以暗示：在"哈希主义"下，一切信息（文本、图像、数据、甚至概念）都可以被"切碎、映射、加密、组合"，从而形成新的秩序。
3. **带有先锋、颠覆性**
    
    - "主义"往往意味着某种宣言或呼吁，带点"运动"或"革命"色彩。
    - 选择这个名字，会给外界一种"这是一个全新流派或思潮"的感觉，而不仅仅是一个产品或公司名。



[Q3：Viva/viva/产品/需求池.md]
========================

---
draw:
tags: []
title: 需求池
date created: 2024-01-05
date modified: 2024-12-27
---
- 主被动词汇的差导入 anki 时打上特殊 tag
- 同义词辨析的 api 换成 gemini 的
- viva 改成 stream 似乎能实现补全完整格式的 json
- 每周是不是可以搞一个爽读时间
- 每天执行一次定时任务，把同义词加成标签
- 做一个同步机制，使得 anki 的数据能同步更新到 excel 的 vault 数据库
- 对比 geminipro 和 gpt 的复盘效果
- 将近义词辨析改成 stream 输出
- 写作的复盘也可以检索自己 vault 的近义词做辨析
- Anki 中真正要记的东西一定 要满足一个非常高的阈值，比如这次的 activities 和 programs，不然就会陷入无意义的记忆中。可能半个小时的课，找到十张卡片就已经横多了，更多的时间放到夺取接触老外，多交流本身上
- 复盘的 json 结果中 speaker A 和 B 有些地方标注不对，可能并没有真正生效
- 红色 flag 的单词们能否做近义词分析，然后分组进行辨析
	- 看能否利用一些接口，比如 ankiconnect 直接拿到对应的数据，然后分组，使整个过程一件触发，这样就随时可以整理学习了
	- 这些接口对于其他的数据分析也很有意义，不然只能从 browse 里面手动复制了，难以实现自动化
	- `去重按钮`，找到重复单词的卡片
	- `聚类按钮`，一键将混淆词汇聚类，辨析记忆
- 将 chatgpt 矫正的句子的语音加入到原外教课视频中
- 对于极端顽固单词用 sora 生成一分钟的场景视频

先做一个统计累积次频的输入输出  
输出仍旧可以是词频建议  
用 es 存储和检索？  
推荐的可以宽松一些，比如一百个

职责链模式实现视频复盘的处理过程  
引入 Jacoco 做代码覆盖率  
canva aj  
vpc-subnet 内互通是靠 icmp？  
沃伦·巴菲特的文章，他写给股东的信  
查理芒格的一些东西

## 测试一下各种 parser 的自动注入是真的还是假的

gitaction

总的单词库  
每部电影的字幕文件的搜索

## 阐述英语思想的文章里，可以加入已知的未知和未知的未知的概念对比，阐述英语学习的意义

可能需要三轮才能完整获取

- 等大成之后，当一个英语教头？只提供课程资源链接、方法、工具，客单价高，但随时支持按照进度退款？
- 和健身房一样，赚坚持不下去的人的钱



[Q3：Viva/viva/前端/viva UI UE.md]
===============================

---
draw:
tags: []
title: viva UI UE
date created: 2024-01-09
date modified: 2024-12-27
---
**可以借鉴** [Kaggle](Kaggle.md)官网的** **UI**，颜色上参考下面这种黑白按钮、蓝色背景、绿色对、黄色错的配色方案吧

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F11%2F24%2F02-33-06-665a4084a0e1ae7f8af39c1063569718-202411240233006-cc9f93.png)

## 作文模块

按 h 之后出现一个下拉屏，显示推荐单词，根据最佳可视化确定显示个数

## Bot

- 个人网站中的文章反应学习和认知思路
- 嵌入其中的 telegram bot 作为具体的实现工具
	- 给单词，自动添加到 anki
	- 给学习的视频音频自动给出复习文本
	- 给油管视频自动给出可能不会的单词



[Q3：Viva/viva/前端/viva 前端.md]
============================

---
draw:
tags: []
title: viva 前端
date created: 2024-10-01
date modified: 2024-12-27
---

```mermaid
graph LR
    A[EssayList.vue] -->|选择文章| B[EssayDetail.vue]
    B -->|开始练习| C[SentenceCard.vue]
    C -->|第一页| D[WordInput.vue]
    D -->|输入单词| E[AI 检测、加入 Anki]

    style D fill:#ffd,stroke:#333,stroke-width:2px
```

```mermaid
graph TD
	Z((App.vue)) --> A[MainLayout]
	A --> E[随URL动态路由：ContentArea]
    A --> D[InnerTopBar]
    A --> B[SideBar]
    A --> C[TopBar]
    E --> F[EssayList]
    F --> G[EssayDetail]
    F --> H[AddEssay]
    B --> I[LoginButton]

    style E fill:#bbf,stroke:#333,stroke-width:4px;
```

## 工程结构

```Java
.
├── main.js  # Vue 应用的入口文件
├── App.vue

// 路由配置
├── router
│   └── index.js

// 页面布局模板 定义整体布局框架
// 多个页面对应一个布局，是 view 的抽象类
├── layouts
│   ├── AuthLayout.vue  # 主布局
│   └── MainLayout.vue  # 认证页面布局

// 页面视图、与路由对应， 页面级组件，处理业务逻辑
// 每个路由对应一个页面
└── views
    ├── Home.vue
    └── essay
        ├── EssayDetailModal.vue
        ├── EssayList.vue
        ├── MappingGridCard.vue
        └── SentenceCard.vue
        
// 被 views 和 layouts 复用的 UI 组件
// 通过 props 接收数据，通过 emit 发送事件
// 提供具体功能的、较小的组件,通常不与路由直接关联,
// 例如按钮、表单输入、卡片
├── components
│   ├── ActiveVocabulary.vue
│   ├── AddEssay.vue
│   ├── AddEssayModal.vue
│   ├── EssayInput.vue
│   ├── InnerTopBar.vue
│   ├── Portal.vue
│   └── layout
│       ├── ContentArea.vue
│       ├── LoginButton.vue
│       ├── SideBar.vue
│       ├── Toast.vue
│       └── TopBar.vue
├── components.d.ts

// 插件配置
├── plugins
│   ├── element-plus.js
│   └── googleAuth.js

// API 服务
├── services
│   └── api.js

// 状态管理
├── stores
│   ├── index.js
│   └── user.js
```

## 渲染

```Java
App.vue
  └─ <router-view>
      └─ MainLayout.vue
          └─ <router-view>
              └─ EssayList.vue
                  └─ ContentArea
                      └─ 文章列表内容
```

## 路由

```js
  {
	// 对应 layouts 中的布局
    path: '/',
    component: () => import('@/layouts/MainLayout.vue'),
    children: [
      // 每个 child 对应 views 中的一个页面
      {
        path: '',
        name: 'essay-list',
        component: () => import('@/views/essay/EssayList.vue'),
        meta: { 
          layout: 'main',
          requiresAuth: false 
        }
      },
    ]
  },
  ```

- 需要将所有页面内容移到 views 目录下
- 使用 ContentArea 作为页面内容的包装器
- 可以在路由配置中添加 meta 信息来控制权限等
- 可以使用路由守卫来处理权限验证

## 登录

```json
{"web":{"client_id":"","project_id":"linen-option-420316","auth_uri":"https://accounts.google.com/o/oauth2/auth","token_uri":"https://oauth2.googleapis.com/token","auth_provider_x509_cert_url":"https://www.googleapis.com/oauth2/v1/certs","client_secret":"、","redirect_uris":["http://localhost:5173"],"javascript_origins":["http://localhost:5173"]}}
```

```js
// 添加请求拦截器来设置 JWT token，以后所有的请求都带了能表示用户身份的 token
api.interceptors.request.use((config) => {
  const token = localStorage.getItem('jwt_token');
  if (token) {
    config.headers['Authorization'] = `Bearer ${token}`;
  }
  return config;
}, (error) => {
  return Promise.reject(error);
});
```

### mock 数据

```shell
json-server --watch db.json --port 8000
```

### 模态框的使用

模态框（Modal）是一种常见的用户界面元素，在网页设计和应用程序开发中广泛使用。从 AddEssay 组件转变为 AddEssayModal 组件，这是一种模态框（Modal）设计模式。这种设计模式有以下几个特点和优势：

- 分离关注点：将添加文章的功能从主界面分离出来，成为一个独立的模态框组件，使得代码结构更清晰，更易于维护。
- 模态框是一个出现在当前页面之上的内容框，通常用于显示重要信息、获取用户输入或确认操作。它的主要特点包括：
	1. 覆盖性：模态框会覆盖在主要内容之上，通常会有一个半透明的背景遮罩。
	2. 焦点转移：当模态框打开时，用户的注意力会被引导到模态框内容上。

### 状态管理

- 用户状态（UserStore）是全局状态，只需要在应用的入口点（通常是 App.vue）初始化一次就够了。
- 在 App.vue 中初始化后，所有子组件都可以访问到已经初始化的状态。

## Vue 和 JavaScript 方法使用频率统计（表格版）

### Vue 相关方法

| 方法          | 使用次数 |             |
| ----------- | ---- | ----------- |
| ref         | 18   |             |
| computed    | 2    |             |
| onMounted   | 2    |             |
| provide     | 2    |             |
| inject      | 1    |             |
| reactive    | 1    |             |
| watch       | 1    |             |
| defineProps | 1    |             |
| defineEmits | 1    |             |
| emit        |      | 本质是发射一个事件出去 |

### JavaScript 数组方法

| 方法 | 使用次数 |
|------|----------|
| map | 3 |
| forEach | 2 |
| filter | 1 |

### JavaScript 对象方法

| 方法 | 使用次数 |
|------|----------|
| Object.keys | 1 |

### DOM 操作方法

| 方法 | 使用次数 |
|------|----------|
| addEventListener | 1 |
| removeEventListener | 1 |
| appendChild | 1 |

### 其他常用 JavaScript 方法

| 方法 | 使用次数 |
|------|----------|
| setTimeout | 2 |
| console.log | 多次 (用于调试) |
| console.error | 多次 (用于错误处理) |

### Vue Router 相关方法

| 方法 | 使用次数 |
|------|----------|
| useRouter | 2 |
| push (router.push) | 2 |

### Axios 相关方法

| 方法 | 使用次数 |
|------|----------|
| post | 5 |
| get | 3 |
| create | 1 |
| interceptors.request.use | 1 |

### Pinia 相关方法

| 方法 | 使用次数 |
|------|----------|
| defineStore | 1 |

### 其他注意到的模式

- 大量使用了 ES6+ 的箭头函数语法
- 使用了解构赋值
- 使用了模板字符串
- 使用了 async/await 进行异步操作



[Q3：Viva/viva/口语课复盘 demo.md]
============================

---
draw:
tags: []
title: 口语课复盘 demo
date created: 2024-02-22
date modified: 2024-12-27
---

```json

{

"Round": "1",

"speaker_A": "Hello.",

"speaker_B": "Hey. Hi, Leo. Can you see me?",

"authentic_expression": "Hey. Hi, Leo. Can you see me?",

"tags": "CORRECT"

},

{

"Round": "2",

"speaker_A": "Right. Is this the first time we talking in the morning?",

"speaker_B": "Me?",

"authentic_expression": "Me? Or 'us' do you mean?",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "3",

"speaker_A": "We?",

"speaker_B": "Oh, us. Oh, really? I didn't think about that. Did you just wake up?",

"authentic_expression": "Oh, us. Oh, really? I hadn't thought about that. Have you just woken up?",

"tags": "TENSE_ERROR"

},

{

"Round": "4",

"speaker_A": "Yeah.",

"speaker_B": "Why did you decide to start doing lessons in the morning.",

"authentic_expression": "Why did you decide to start having lessons in the morning?",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "5",

"speaker_A": "Before every day? We woke up at 10:00 a.m. This year I want to make it early, so I think schedule a lesson is a good way to make me wake up earlier.",

"speaker_B": "Okay. So usually you wake up at 10:00 a.m. You can have a Coca Cola at 09:00 a.m.?",

"authentic_expression": "Okay, so you normally wake up at 10:00 a.m. Can you really have a Coca-Cola at 9:00 a.m.?",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "6",

"speaker_A": "Right?",

"speaker_B": "Is that Coca Cola?",

"authentic_expression": "Is that Coca Cola?",

"tags": "CORRECT"

},

{

"Round": "7",

"speaker_A": "Yeah.",

"speaker_B": "At 09:00 a.m. I never saw that before. I never saw anyone drink soda with breakfast. Shocked. Do you feel like a lot of people drink Coca Cola in, in China? Is it really popular?",

"authentic_expression": "At 9:00 a.m.? That's the first for me. I've never seen anyone drinking soda for breakfast. I'm shocked. Do a lot of people in China drink Coca-Cola? Is it really that popular?",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "8",

"speaker_A": "Yeah, my team leader. Really?",

"speaker_B": "Your team leader? Oh, so interesting. Okay. Oh, that's crazy. But what about coffee? You don't want coffee in the morning?",

"authentic_expression": "Your team leader? Oh, that's intriguing. Okay. Wow, that's surprising. But what about coffee? Don't you prefer coffee in the morning?",

"tags": "VOCABULARY"

},

{

"Round": "9",

"speaker_A": "I don't because I have happy to have a nap at noon. Maybe it will influence it.",

"speaker_B": "So you have a nap and then after your nap you will have coffee.",

"authentic_expression": "I don't, because I like to take a nap at noon and I'm afraid it might affect that.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "10",

"speaker_A": "Right.",

"speaker_B": "Interesting. Okay, I got it. What about like tea? You don't have any tea to help you wake up. Oh, I cannot see you.",

"authentic_expression": "Interesting. Okay, I understand. But what about tea? Don't you drink any tea to wake you up? Oh, I can't see you now.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "11",

"speaker_A": "Hello?",

"speaker_B": "I cannot see you.",

"authentic_expression": "I can't see you anymore.",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "12",

"speaker_A": "Can you know.",

"speaker_B": "Let me refresh. What about now? I think it's good. I can see you clearly now. Oh, no, you cannot hear me. Can you hear me? Right, okay, so you said you usually wake up at 10:00 a.m. But I want to know if you start waking up earlier, like at 830 or nine, then will you start going to sleep earlier?",

"authentic_expression": "Can you know?",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "13",

"speaker_A": "Yeah, that's what I want to have. What time? 02:00 a.m. After. I know what you want to know.",

"speaker_B": "02:00 a.m. But now what time will you start going to sleep?",

"authentic_expression": "Yes, I plan to. What time? After 2:00 a.m., right? I understand what you're asking.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "14",

"speaker_A": "Midnight.",

"speaker_B": "Eight or nine.",

"authentic_expression": "Eight or nine? You mean, you'll be going to bed at eight or nine o'clock?",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "15",

"speaker_A": "Midnight.",

"speaker_B": "Midnight. Midnight. Okay. When you say midnight, when you say mid, it sounds like you say need. So just try to make a short. I.",

"authentic_expression": "Midnight, midnight. Okay, I hear you. When you say 'midnight,' make sure to pronounce the 'd' sound in 'mid' so it's clear. Just try to pronounce a short 'i'.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "16",

"speaker_A": "Yeah, I know these things. Mid the night.",

"speaker_B": "Okay.",

"authentic_expression": "Okay.",

"tags": "CORRECT"

},

{

"Round": "17",

"speaker_A": "Mid and I.",

"speaker_B": "Okay, I understand. So like 8 hours of sleep or eight and a half hours of sleep. It's good, right? It's good. I feel like a lot of people have the problem for not getting enough sleep. I know a lot of people who only sleep 6 hours a night, but I just feel like I could not function if I did that. Are you the type of person that can get like 6 hours or 5 hours of sleep and still feel good?",

"authentic_expression": "Okay, I understand. So, about 8 hours of sleep, or maybe eight and a half hours? That's good, right? A lot of people have problems not getting enough sleep. I know many who only sleep for about 6 hours a night, but I feel I couldn't function on so little sleep. Can you function well on just 6 or 5 hours of sleep?",

"tags": "GRAMMAR"

},

{

"Round": "18",

"speaker_A": "Yeah. All right. I think I am really?",

"speaker_B": "Okay. But it sounds like you still get more than 8 hours of sleep a night.",

"authentic_expression": "Yes, I am, really.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "19",

"speaker_A": "No more than.",

"speaker_B": "Okay around 8 hours of sleep a night right? Yeah but in the past you had less sleep.",

"authentic_expression": "Around 8 hours of sleep a night, right? But you've had less sleep in the past?",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "20",

"speaker_A": "College 02:00 a.m. To 10:00 a.m. 8 hours it's still 8 hours but almost after 02:00 a.m..",

"speaker_B": "Okay but when you stay awake until 02:00 a.m. Is it because you go out or you're at home staying awake until 02:00 a.m. At a home?",

"authentic_expression": "In college, from 2:00 a.m. to 10:00 a.m., that's still 8 hours, but it was usually after 2:00 a.m.",

"tags": "GRAMMAR"

},

{

"Round": "21",

"speaker_A": "Play the phone.",

"speaker_B": "Just like using your phone? Yeah we don't usually say like play on our phone unless you can say like playing a game on my phone but we would just say like using my phone or sometimes we say something like scrolling. Scrolling means for example if I'm on. Yeah okay. Like social media and I just scroll scroll scroll scroll. This is scrolling and your page is like you could just do this for like an hour and you don't even know. Right? Yeah. Okay I got you. Nice. Okay so we're going to be doing morning lessons now. Good for you. It sounds difficult though. I started doing morning classes at like 08:00 a.m. Which I think for other people 08:00 a.m. Does not sound early but for me that means I have to wake up at 730 so it's really early for me. I don't like waking up so early.",

"authentic_expression": "Just using your phone? Right, we don't usually say 'play on our phone' unless it's 'playing a game on my phone'. We usually just say 'using my phone' or sometimes 'scrolling'. Scrolling, like when you're on social media and you keep scrolling and scrolling. This is called scrolling, and you might not even notice how long you've been doing it. Right? Got it. Great, so morning lessons it is then. That's good for you, but it does sound tough. I started doing morning classes at 8:00 a.m. which might not seem early to some, but it means I need to be up by 7:30 a.m., which is really early for me. I'm not a fan of waking up that early.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "22",

"speaker_A": "Yeah what's the first thing after you wake up?",

"speaker_B": "Like what do I do? My morning routine.",

"authentic_expression": "You mean, what's the first thing I do when I get up? My morning routine?",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "23",

"speaker_A": "What team? Pardon?",

"speaker_B": "Morning routine?",

"authentic_expression": "Morning routine? Yes, what's my morning routine?",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "24",

"speaker_A": "Yeah what's her morning routine?",

"speaker_B": "I usually get up and make my bed and then I drink warm water. One sway and then I get dressed and I don't do anything else. I check all my messages because when I'm sleeping it's the daytime in the US so usually when I wake up I have a lot of messages because it's people's daytime so then I will spend the time to respond to everything and then I just start teaching. So I already taught for like 1 hour. Yeah just finished 1 hour. Yeah but I'm not really a morning person like naturally I think maybe also not you.",

"authentic_expression": "I usually get up, make my bed, then drink warm water and get dressed. That's about it. Then I check my messages because while I'm asleep, it's daytime in the US, so usually, I'll have lots of messages to respond to when I wake up. After that, I start teaching. I've already taught for about an hour – just finished an hour ago. I'm not naturally a morning person, and maybe you aren't either.",

"tags": "GRAMMAR"

},

{

"Round": "25",

"speaker_A": "Yeah when I went to work I just sit here pretend I'm working with further lunch.",

"speaker_B": "Okay so afternoon you're more productive.",

"authentic_expression": "Yeah, when I go to work, I just sit here pretending to work until lunchtime.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "26",

"speaker_A": "All right.",

"speaker_B": "But I am definitely the type of person who likes to eat breakfast. Many times I can skip lunch or skip dinner but I don't like to skip breakfast because I feel much better if I eat breakfast.",

"authentic_expression": "I'm definitely the type of person who needs to eat breakfast. I can often skip lunch or dinner, but never breakfast because I feel much better when I've had it.",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "27",

"speaker_A": "Okay I never eat breakfast.",

"speaker_B": "Oh you never eat breakfast? Just Coca Cola?",

"authentic_expression": "Oh, you never have breakfast? Just Coca-Cola then?",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "28",

"speaker_A": "No, it's not the routine.",

"speaker_B": "Okay.",

"authentic_expression": "No, that’s not my usual routine.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "29",

"speaker_A": "I just want to drink something. So I find a coca Cola in the refrigerator.",

"speaker_B": "Sorry, it's just crazy to me. I could never drink that. Okay, so you never eat breakfast, right. Really? My sister is like that too, because.",

"authentic_expression": "I just want something to drink, so I grab a Coca-Cola from the fridge.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "30",

"speaker_A": "In the college I got up later even more, maybe 11:00 so it's not sort of time to eat breakfast.",

"speaker_B": "But would you still eat when you woke up?",

"authentic_expression": "In college, I used to wake up even later, maybe around 11:00, so it wasn't really time for breakfast.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "31",

"speaker_A": "I just eat lunch.",

"speaker_B": "Yeah, but that's still like 1 hour after you wake up, so it's technically like your morning.",

"authentic_expression": "Yeah, but that's still about an hour after you wake up, so it's technically still your 'morning meal'.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "32",

"speaker_A": "Yeah, but I maybe wash something. Okay. When I went to dining hall, it's.",

"speaker_B": "I'm curious. I kind of know what the common breakfast is to have in Taiwan. But what are the common breakfast for people in, like, what do people usually.",

"authentic_expression": "Yes, but maybe I'll wash up first. When I would go to the dining hall, it's…",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "33",

"speaker_A": "Have for breakfast in Beijing? In all the area of north China? The bald.",

"speaker_B": "Yotiao.",

"authentic_expression": "What do people usually have for breakfast in Beijing? Or in northern China in general? You mean 'youtiao'?",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "34",

"speaker_A": "Bald. And milk. Do you know dofano?",

"speaker_B": "Dofino.",

"authentic_expression": "Baozi, and soy milk. Do you know 'doufunao'?",

"tags": "VOCABULARY"

},

{

"Round": "35",

"speaker_A": "Made of beans, but it's like dozen.",

"speaker_B": "Okay. I don't know it. Dofa.",

"authentic_expression": "It's made of beans, but it's like soft tofu.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "36",

"speaker_A": "No, tofu. Translation is.",

"speaker_B": "Oh, you call it this. You don't call it dohua?",

"authentic_expression": "No, tofu. Its translation is...",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "37",

"speaker_A": "Yeah. In the north we call it.",

"speaker_B": "Oh, cool. I didn't know that. Okay, nice. Cool. This is really nice because actually a lot of places I go, they don't have the savory dohua. Usually people eat the sweet dohua here, but that looks really good. And then it's like protein in the morning.",

"authentic_expression": "Yes, in the north we call it that.",

"tags": "CORRECT"

},

{

"Round": "38",

"speaker_A": "Yeah, protein enriched protein.",

"speaker_B": "But what do people put on top of doha? The tofu. For the breakfast, what is usually goes on top?",

"authentic_expression": "Yes, it's rich in protein.",

"tags": "WORD_USAGE_ERROR"

},

```json

{

"Round": "39",

"speaker_A": "A kind of sauce can make it salty.",

"speaker_B": "Salty?",

"authentic_expression": "They put a kind of sauce on it to make it salty.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "40",

"speaker_A": "Yeah. Otherwise it's like water.",

"speaker_B": "Right. It's kind of like soy sauce. The sauce. Or like chili oil.",

"authentic_expression": "Yes, otherwise it would be too bland, like water.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "41",

"speaker_A": "Chili oil, how to spell it?",

"speaker_B": "Yeah. Like chili oil. No, sorry, chili oil. Like this. Ah, the Sauce you talk about is from like meat, Ryan. Totally. Okay. Yeah, it has a lot of flavor. It's very salty. Okay, interesting. You know, a lot of people here, they just go out for breakfast, but actually, I think since food is expensive in the US, we cook a lot of our meals, so we'll make something fast at home. But I noticed most people here, they don't make breakfast. They'll go out and get something fast.",

"authentic_expression": "Yeah, you spell it like 'chili oil'. No, sorry, 'chili oil'. Like this. Oh, the sauce you're talking about is like a savory sauce, possibly with meat? Okay, got it. It's very flavorful and salty. Interesting. You know, a lot of people where I'm from just go out for breakfast, but because food can be expensive in the US, many of us prepare our own meals at home, something quick. But I've noticed that here, most people don't make breakfast at home. They prefer to go out and get something quickly.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "42",

"speaker_A": "Yeah. Some of my colleagues would do it by themselves in the morning. So fast. Breakfast.",

"speaker_B": "Do people eat like, manto?",

"authentic_expression": "Yeah, some of my colleagues make it themselves in the morning. A quick breakfast.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "43",

"speaker_A": "Manto is more common at noon. At lunch.",

"speaker_B": "At lunch it's just bread. Okay, that's so funny. Okay. Are a lot of people concerned, like, in the US, we have a lot of different diet trends that people do. Are people in China very concerned with doing diets or. No.",

"authentic_expression": "Mantou is more commonly eaten at lunchtime.",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "44",

"speaker_A": "What's the meaning of concerned? Diets pay more than.",

"speaker_B": "No, what I mean is, I don't just mean, like, what they eat. I mean, like making a diet. Like, do people have diet plans in China? Often?",

"authentic_expression": "What do you mean by 'concerned'? People care more about the cost?",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "45",

"speaker_A": "Not common.",

"speaker_B": "Yeah, it's not common. Okay. Yeah.",

"authentic_expression": "It's not common.",

"tags": "CORRECT"

},

{

"Round": "46",

"speaker_A": "Because most people, I think most people don't have basic knowledge about nutrition. Actually, their knowledge about food is from the traditional medicine, I think. But they're not scientific.",

"speaker_B": "Yeah, I know you don't like the chinese medicine. Okay. But I'm mostly wondering, because in the US, we have diet plans that are more like, we have one called Atkins diet, or there's types of diets where people count calories, which is really old. People don't do this so much now, but there's also one recently. This is the one I want to talk to you about. Recently, some Americans have been doing a carnivore diet. Have you heard about this.",

"authentic_expression": "That's because I think most people lack basic nutritional knowledge. Their understanding of food often comes from traditional medicine, which isn't always scientifically grounded.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "47",

"speaker_A": "Carnival?",

"speaker_B": "No. We are all omnivores.",

"authentic_expression": "You mean 'carnivorous' diet?",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "48",

"speaker_A": "They don't eat the foods from plants. Not at all.",

"speaker_B": "Not at all, bro. It's so weird.",

"authentic_expression": "So they don't eat any plant-based foods at all.",

"tags": "CORRECT"

},

{

"Round": "49",

"speaker_A": "I don't know why. Because if you want to distinguish the food from plants, it's also difficult, isn't it?",

"speaker_B": "No, because no pasta, no rice, no vegetables. They eat, like, beef and fish and butter. Butter. And they eat nothing else. Maybe eggs. Eggs. Eggs.",

"authentic_expression": "I don't know why they do that. Because wouldn't it be difficult to exclude all plant-based foods?",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "50",

"speaker_A": "Why do they go like this?",

"speaker_B": "Because I'm telling you, this is a trend. Like a fad, right? People believe, they did a lot of research on diets, on healthy diets, and they believe that a very high protein and fat diet, and no carbs, no sugars, like these types of things. It's like an extreme diet. We would call this an extreme diet, but my aunt and my uncle have been doing this diet for a year, which means they have eaten nothing but meat and eggs and butter for a year.",

"authentic_expression": "Why do they choose to eat this way?",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "51",

"speaker_A": "How does it affect.",

"speaker_B": "So, a lot of the time, the reason people do these diets is because they want to have the high muscle mass.",

"authentic_expression": "What's the impact?",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "52",

"speaker_A": "Right?",

"speaker_B": "High muscle mass and low fat. They don't want to have a lot of fat. So my auntie and my uncle, they lost a lot of weight. Yeah, a lot. They lost a lot of weight. But it's really weird because they kind of ignore the things that are not good that are happening. Like, for example, they cannot poop very well. And they're just like. It's because our body is adjusting or because we don't have a lot of vegetables.",

"authentic_expression": "They aim for high muscle mass and low body fat. They don't want to carry extra fat. My aunt and uncle have lost a significant amount of weight on this diet. However, they tend to overlook the negative side effects. For instance, they have trouble with bowel movements. They brush it off, saying it's either the body adjusting or the lack of vegetables in their diet.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "53",

"speaker_A": "He cannot poop poop, poop.",

"speaker_B": "Go to the bathroom. Poop. Poop. Not pee. Poop. Go it.",

"authentic_expression": "They've had difficulty with bowel movements – with pooping, to be clear.",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "54",

"speaker_A": "Okay, I got.",

"speaker_B": "Yeah. Okay. Yeah. No, Dabian. They cannot poop because their digestive system is not good now anyway. Okay, so I was wondering, China doesn't have any weird diets or something that people try.",

"authentic_expression": "Okay, I understand.",

"tags": "CORRECT"

},

{

"Round": "55",

"speaker_A": "The people in the room pay more attention on it.",

"speaker_B": "Who?",

"authentic_expression": "People who are more health-conscious tend to pay attention to it.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "56",

"speaker_A": "Fitness in Jane.",

"speaker_B": "Fitness. Fitness.",

"authentic_expression": "Fitness enthusiasts.",

"tags": "VOCABULARY"

},

{

"Round": "57",

"speaker_A": "Fitness. Yeah.",

"speaker_B": "Like people who train at the. Okay, sure.",

"authentic_expression": "Yes, people who are into fitness.",

"tags": "CORRECT"

},

{

"Round": "58",

"speaker_A": "Could this kind of exercise totally were from foreign countries. So all the theory are modern. So they focus on the diet. Actually.",

"speaker_B": "All of what is modern?",

"authentic_expression": "These exercise trends mostly come from foreign countries, and all the theories behind them are modern, so they emphasize diet as well.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "59",

"speaker_A": "The workout.",

"speaker_B": "The workout plan is like a modern workout plan. I never heard anyone say that.",

"authentic_expression": "The exercise regimes and workout plans.",

"tags": "WORD_USAGE_ERROR"

},

{

"Round": "60",

"speaker_A": "This kind of sport you work out from abroad.",

"speaker_B": "Crossfit.",

"authentic_expression": "These kinds of workouts are adopted from abroad.",

"tags": "CLARITY_AND_FLUENCY"

},

{

"Round": "61",

"speaker_A": "I search it.",

"speaker_B": "It's like crossfit. Okay, I think I know what you're talking about. Or maybe not. Or is it bodybuilding?",

"authentic_expression": "I'll look it up.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "62",

"speaker_A": "Yeah. Even the bar. What's the bar? Bell bar and stick bar? Yeah. All of them are abroad.",

"speaker_B": "You mean imported from abroad?",

"authentic_expression": "Yes. Even the equipment, like the barbell and the pull-up bar. Yeah, all of those are from abroad.",

"tags": "VOCABULARY"

},

{

"Round": "63",

"speaker_A": "Yeah, imported.",

"speaker_B": "Okay, so you're just saying that they do, like, foreign style exercise plans.",

"authentic_expression": "Yes, imported.",

"tags": "CORRECT"

},

{

"Round": "64",

"speaker_A": "Diet plan and.",

"speaker_B": "Diet plan.",

"authentic_expression": "And the diet plans.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "65",

"speaker_A": "Yeah. Okay. This kind of sport was imported. So the people getting involved in are not influenced by the traditional medicine. So they have the same modern diet theory.",

"speaker_B": "Okay, I understand what you're saying. They don't subscribe to the traditional way of thinking. Okay.",

"authentic_expression": "Yes. So these sports and fitness methods were adopted from abroad. Therefore, the people who engage in them are not influenced by traditional medicine, and they follow modern dietary theories instead.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "66",

"speaker_A": "Yeah. And there is also a slogan that 30 percentage exercise and 70% age 18.",

"speaker_B": "70% age 1818.",

"authentic_expression": "Yes, and there's a slogan that says '30% exercise, 70% diet'.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "67",

"speaker_A": "Right. 70%.",

"speaker_B": "Okay, I see what you're saying. Yeah. Do a lot of people do, like, protein shakes for that? Okay, I know that you work out sometimes. Do you do anything for your diet or do you just eat whatever you want and then exercise?",

"authentic_expression": "Right, 70%.",

"tags": "CORRECT"

},

{

"Round": "68",

"speaker_A": "Coach, make a priority of gona diet about control the calorie, about your calories. Yeah. So I lost weight so fast, but someday he went to his hometown to get married, so there's nobody regulating me. So I went back again.",

"speaker_B": "Wait, how long has he been gone?",

"authentic_expression": "My coach prioritized a diet plan focused on controlling calories. Yeah, so I lost weight quickly. But then he went back to his hometown to get married, and without someone to guide me, I gained weight again.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "69",

"speaker_A": "Three months.",

"speaker_B": "He's not coming back, right?",

"authentic_expression": "Three months.",

"tags": "CORRECT"

},

{

"Round": "70",

"speaker_A": "Not in Beijing.",

"speaker_B": "You need a new trainer. We usually call this a trainer, not a coach.",

"authentic_expression": "Not in Beijing.",

"tags": "CORRECT"

},

{

"Round": "71",

"speaker_A": "I found a trainer last night.",

"speaker_B": "You did? Nice.",

"authentic_expression": "I found a new trainer last night.",

"tags": "CORRECT"

},

{

"Round": "72",

"speaker_A": "Yeah.",

"speaker_B": "Okay, cool. At the same gym?",

"authentic_expression": "Yeah.",

"tags": "CORRECT"

},

{

"Round": "73",

"speaker_A": "No, a new gym. A new gym around my.",

"speaker_B": "Nice. So usually we'll say trainer for someone who's helping you go to the gym and train your body. Coach is more for like, for example, if you do like, a b side, if you're doing a competition, then you will have a coach to teach you how to prepare for that.",

"authentic_expression": "No, a new gym. A new gym around my place.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "74",

"speaker_A": "Yeah.",

"speaker_B": "Just a little bit different. Yeah. Okay, so for your workouts, do you mostly do weightlifting or do you also do cardio weightlifting? Okay. Do you do stretching? Do you stretch? Oh, sorry. Go ahead.",

"authentic_expression": "Yeah.",

"tags": "CORRECT"

},

{

"Round": "75",

"speaker_A": "Yeah. After the weight lifting, maybe ten minute stretch is important.",

"speaker_B": "Okay. Wait, you stretch for 10 minutes? Oh, that's so good. My boyfriend will never stretch with me. I always tell him, you should stretch, and he skip it. He's like, I'm too lazy because I feel like men usually skip stretching and the women always stretch. You know what I mean? But it's important.",

"authentic_expression": "Yes. After weightlifting, I usually do a ten-minute stretch. It's important.",

"tags": "CORRECT"

},

{

"Round": "76",

"speaker_A": "It night. Patient. Patient. Women are more.",

"speaker_B": "Yeah, when you say, don't say woman, you say women.",

"authentic_expression": "At night, it's important. Women are more patient.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "77",

"speaker_A": "Oh, yeah, women.",

"speaker_B": "Okay. You do weightlifting. So do you usually do. I just want to go through some gym words with you that maybe you don't know. So are you doing endurance lifts like, you try to see how long you can go, or do you do power lifts? Do you know the difference?",

"authentic_expression": "Oh, women.",

"tags": "CORRECT"

},

{

"Round": "78",

"speaker_A": "I. Power lift. Emphasize the maximum weight that you can lift.",

"speaker_B": "Yeah. Power lifts is the maximum weight you can do. And then maybe you just do a few of them, like maybe one, two, three.",

"authentic_expression": "I focus on powerlifting, emphasizing the maximum weight I can lift.",

"tags": "SENTENCE_STRUCTURE_ERROR"

},

{

"Round": "79",

"speaker_A": "I think I prefer which one? Endurance. Yeah, endurance.",

"speaker_B": "Yeah. I think it's better for you anyway because you're not preparing for, like, a competition or something. Same. Okay. Sorry. The time is running out. Have a good day. Bye.",

"authentic_expression": "I think I prefer endurance. Yes, endurance training.",

"tags": "SENTENCE_STRUCTURE_ERROR"

}

]

}

```

```Java



[Q3：Viva/viva/后端/Anki 模块.md]
============================

---
draw:
tags: []
title: Anki 模块
date created: 2024-10-27
date modified: 2024-12-27
---

让我帮你看看Anki的开源网页版选项。

```tsx
import React, { useState } from 'react';
import { Card, CardContent } from '@/components/ui/card';
import { Button } from '@/components/ui/button';

const FlashCard = () => {
  const [isFlipped, setIsFlipped] = useState(false);
  const [currentCard, setCurrentCard] = useState(0);

  // 示例卡片数据
  const cards = [
    { front: "什么是React?", back: "React是一个用于构建用户界面的JavaScript库" },
    { front: "什么是状态管理?", back: "状态管理是一种在应用程序中管理数据流的方式" }
  ];

  const handleFlip = () => {
    setIsFlipped(!isFlipped);
  };

  const handleNext = () => {
    setCurrentCard((prev) => (prev + 1) % cards.length);
    setIsFlipped(false);
  };

  return (
    <div className="w-full max-w-md mx-auto p-4">
      <Card className="min-h-[200px] cursor-pointer" onClick={handleFlip}>
        <CardContent className="flex items-center justify-center h-full p-6">
          <div className="text-center">
            {isFlipped ? cards[currentCard].back : cards[currentCard].front}
          </div>
        </CardContent>
      </Card>
      
      <div className="mt-4 flex justify-between">
        <Button onClick={handleFlip}>
          翻转卡片
        </Button>
        <Button onClick={handleNext}>
          下一张
        </Button>
      </div>
    </div>
  );
};

export default FlashCard;

```

对于Anki的开源网页版本，有几个选择：

1. **AnkiWeb**  
虽然官方的AnkiWeb不是开源的，但你可以使用以下开源替代方案：

2. **OpenAnki**
- 这是一个基于Web的开源实现
- GitHub: https://github.com/openanki/openanki
- 可以自行部署和集成

1. **Anki-web-client**
- 一个轻量级的Web客户端实现
- GitHub: https://github.com/ankicommunity/anki-sync-server

1. **SuperMemo.js**
- 基于SuperMemo SM-2算法的JavaScript实现
- 适合集成到现有项目中

我上面提供的React组件示例展示了如何创建一个基本的闪卡系统。你可以：

- 修改数据结构来适应你的需求
- 添加进度跟踪
- 实现间隔重复算法
- 添加更多交互功能

如果你想要完整的Anki功能，建议：

1. 使用开源的sync-server搭建后端
2. 前端可以参考open-anki的实现
3. 或者直接使用我提供的组件为基础，逐步添加所需功能

你想了解具体哪方面的实现细节吗？



[Q3：Viva/viva/后端/viva 后端.md]
============================

---
draw:
tags: []
title: viva 后端
date created: 2024-10-07
date modified: 2024-12-27
---

```mermaid
classDiagram
    class essays {
        +serial essay_id PK
        +varchar(255) title
        +text content
        +varchar(255) image_url
        +varchar user_id
        +timestamp create_time
        +timestamp update_time
        +boolean is_deleted
        +timestamp deleted_at
    }
    class sentences {
        +serial sentence_id PK
        +text sentence
        +timestamp create_time
        +timestamp update_time
        +boolean is_deleted
    }
    class essay_sentences {
        +integer essay_id PK, FK
        +integer sentence_id PK, FK
        +timestamp create_time
        +timestamp update_time
        +boolean is_deleted
    }
    class active_mappings {
        +serial mapping_id PK
        +integer sentence_id FK
        +integer focus_start
        +integer focus_end
        +text user_expression
        +boolean ai_review_is_correct
        +text ai_review_expression
        +timestamp create_time
        +timestamp update_time
        +boolean is_deleted
    }
    essays "1" -- "0..*" essay_sentences : has
    sentences "1" -- "0..*" essay_sentences : has
    sentences "1" -- "0..*" active_mappings : has
```

## 新建文章

文章 - 分句 - 分词

## 文章 detail

文章 id，查出 content 展示

## 进入学习模式

返回有序的句子 id，然后发起第一个 id 的学习卡片的请求

## 学习卡片

一张卡片内是该句子下的 mappingsVo

## 核心处理流程

```mermaid
flowchart TB
    Start([开始]) --> Split[将作文分割成句子]
    Split --> Process{对每个句子处理}
    
    subgraph Phase1[第一步：英文翻译和分词]
        Process --> TranslateSegment[使用LLM翻译并分词]
        TranslateSegment --> P1Result[得到初步分词结果]
    end
    
    subgraph Phase2[第二步：中文分词填充]
        P1Result --> FillChinese[使用LLM填充中文分词]
        FillChinese --> P2Result[得到中文对应结果]
    end
    
    subgraph Phase3[第三步：分词索引填充]
        P2Result --> FillIndex[使用LLM填充索引信息]
        FillIndex --> P3Result[得到带索引的分词结果]
    end
    
    subgraph Phase4[第四步：同义词扩展]
        P3Result --> GetSynonyms[获取WordNet同义词]
        GetSynonyms --> FilterFreq[过滤高频词<br/>词频排名<=2000]
        FilterFreq --> Transform[转换为原词形式]
        Transform --> FinalResult[最终扩展结果]
    end
    
    FinalResult --> End([结束])

    %% 样式定义
    classDef phase fill:#e1f3d8,stroke:#82c91e
    classDef process fill:#fff5f5,stroke:#ff8787
    
    class Phase1,Phase2,Phase3,Phase4 phase
    class TranslateSegment,FillChinese,FillIndex,GetSynonyms,FilterFreq,Transform process
```

## VO

是的，你的理解是正确的。在 Python 中，特别是在使用 FastAPI 和 Pydantic 的上下文中，`EssaySchema` 确实相当于 Java 后端中的 VO（View Object）或 DTO（Data Transfer Object）。

让我们来比较一下：

1. 用途：
   - Java VO/DTO：用于在不同层之间传输数据，通常是从后端到前端。
   - Python `EssaySchema`：同样用于数据传输，特别是在 API 响应中。

2. 数据验证：
   - Java VO/DTO：通常使用像 Bean Validation 这样的注解来进行验证。
   - Python `EssaySchema`：使用 Pydantic 的字段类型和验证器来确保数据的正确性。

3. 序列化：
   - Java VO/DTO：通常使用 Jackson 或 Gson 等库进行 JSON 序列化。
   - Python `EssaySchema`：Pydantic 模型可以直接序列化为 JSON。

4. 文档生成：
   - Java：通常需要额外的工具如 Swagger 来生成 API 文档。
   - Python：FastAPI 可以根据 Pydantic 模型自动生成 OpenAPI（Swagger）文档。

这里是一个简化的对比：

Java (Spring Boot) 版本：

```java
public class EssayVO {
    private int essayId;
    private String userId;
    private String title;
    private String content;
    private LocalDateTime createTime;
    private LocalDateTime updateTime;
    private boolean isDeleted;

    // getters, setters, constructors
}
```

Python (FastAPI/Pydantic) 版本：

```python
from pydantic import BaseModel
from datetime import datetime

class EssaySchema(BaseModel):
    essay_id: int
    user_id: str
    title: str
    content: str
    create_time: datetime
    update_time: datetime
    is_deleted: bool

    class Config:
        from_attributes = True
```

主要区别：

1. Python 版本更简洁，不需要显式的 getters 和 setters。
2. Python 版本的类型注解直接用于验证和文档生成。
3. `from_attributes = True` 允许直接从 ORM 模型创建 schema 实例。

总的来说，`EssaySchema` 在 Python/FastAPI 中的角色确实与 Java 后端中的 VO 非常相似，都用于数据传输和 API 响应的结构定义。



[Q3：Viva/viva/后端/viva 数据库.md]
=============================

---
draw:
tags: []
title: viva 数据库
date created: 2024-10-02
date modified: 2024-12-27
---

## 更新时间和创建时间字段

1. 为这俩字段设置默认值`CURRENT_TIMESTAMP`
2. 设置函数
3. 为表增加触发器，自动调用更新函数

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F10%2F02%2F19-01-18-de2c47b30144bfa9d697a4669946b81e-202410021901492-72bd13.png)

## 链路

当然可以。作为一个 Java 和 MyBatis 的使用者，我理解你可能对 Python 中的数据库交互流程感到陌生。让我为你详细解释一下 Python 中的这个链路，特别是在这个项目中使用的模式。

1. 数据库连接和会话管理

在 Python 中，我们通常使用 SQLAlchemy 这个 ORM（对象关系映射）库来处理数据库操作。它类似于 Java 中的 Hibernate。

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# 创建数据库引擎
engine = create_engine('postgresql://user:password@localhost/dbname')

# 创建会话工厂
SessionLocal = sessionmaker(bind=engine)

# 创建会话
session = SessionLocal()
```

1. 模型定义

模型定义类似于 Java 中的实体类。在 SQLAlchemy 中，我们这样定义：

```python
from sqlalchemy import Column, Integer, String, DateTime, Boolean
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Essay(Base):
    __tablename__ = 'essays'

    essay_id = Column(Integer, primary_key=True)
    user_id = Column(String)
    title = Column(String)
    content = Column(String)
    create_time = Column(DateTime)
    update_time = Column(DateTime)
    is_deleted = Column(Boolean)
```

1. 仓库层 (Repository)

仓库层负责与数据库交互，类似于 MyBatis 中的 Mapper。

```python
class EssayRepository:
    def get_essays_by_user_id(self, user_id: str) -> List[Dict]:
        with db_manager.session_scope() as session:
            essays = session.query(Essay).filter(
                Essay.user_id == user_id,
                Essay.is_deleted == False
            ).all()
            
            return [essay.__dict__ for essay in essays]
```

这里的 `db_manager.session_scope()` 是一个上下文管理器，用于管理数据库会话的生命周期。

1. 用例层 (Use Case)

用例层包含业务逻辑，调用仓库层来获取数据。

```python
class GetUserEssaysUseCase:
    def __init__(self, essay_repository: EssayRepository):
        self.essay_repository = essay_repository

    def execute(self, user_id: str) -> List[Dict]:
        return self.essay_repository.get_essays_by_user_id(user_id)
```

1. 控制器层 (Controller)

控制器层处理 HTTP 请求，调用用例层，并返回响应。

```python
@router.get("/essays", response_model=List[EssaySchema])
async def get_user_essays(
    current_user: dict = Depends(get_current_user),
    essay_repository: EssayRepository = Depends(EssayRepository)
):
    use_case = GetUserEssaysUseCase(essay_repository)
    essays_data = use_case.execute(current_user['id'])
    return [EssaySchema(**essay_data) for essay_data in essays_data]
```

1. 模式验证 (Schema Validation)

Pydantic 用于数据验证和序列化，类似于 Java 中的 Jackson 或 Gson。

```python
class EssaySchema(BaseModel):
    essay_id: int
    user_id: str
    title: str
    content: str
    create_time: datetime
    update_time: datetime
    is_deleted: bool
```

整体流程：

1. HTTP 请求到达控制器
2. 控制器创建用例实例
3. 用例调用仓库方法
4. 仓库方法打开数据库会话，执行查询
5. 查询结果返回给用例
6. 用例将结果返回给控制器
7. 控制器使用 Pydantic 模型验证和序列化数据
8. 序列化后的数据作为 HTTP 响应返回

与 MyBatis 的主要区别：

1. Python 使用动态类型，不需要像 Java 那样显式定义返回类型
2. SQLAlchemy 的查询构建更加 Pythonic，不需要 XML 映射文件
3. 依赖注入通过 FastAPI 的 Depends 实现，而不是像 Spring 那样使用注解

希望这个解释能帮助你理解 Python 中的数据库交互流程！



[Q3：Viva/viva/后端/后端方案.md]
=========================

---
draw:
tags: []
title: 后端方案
date created: 2024-11-25
date modified: 2024-12-27
---



[Q3：Viva/viva/后端/接口文档.md]
=========================

---
draw:
tags: []
title: 接口文档
date created: 2024-10-01
date modified: 2024-11-12
---

[Vocabulary Management API - Swagger UI](http://localhost:8000/docs#/)

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F10%2F27%2F03-49-52-95955f5fee6973ce9d1c2c1e64f3d3d8-202410270349947-f5ea18.png)



[Q3：Viva/viva/后端/数据库.md]
========================

---
draw:
tags: []
title: 数据库
date created: 2024-10-22
date modified: 2024-12-27
---

针对这种中英文对照的分词数据，我来建议几种数据库存储方案：

1. 关系型数据库设计：

```sql
-- 原文表
CREATE TABLE translations (
    id SERIAL PRIMARY KEY,
    original_text TEXT NOT NULL,
    translated_text TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 分词表
CREATE TABLE segments (
    id SERIAL PRIMARY KEY,
    translation_id INTEGER REFERENCES translations(id),
    english TEXT,
    chinese TEXT,
    segment_order INTEGER,  -- 保持词语顺序
    CONSTRAINT fk_translation 
        FOREIGN KEY (translation_id) 
        REFERENCES translations(id) 
        ON DELETE CASCADE
);

-- 索引
CREATE INDEX idx_translation_id ON segments(translation_id);
CREATE INDEX idx_english ON segments(english);
CREATE INDEX idx_chinese ON segments(chinese);
```

1. MongoDB (文档型数据库) 方案：

```javascript
{
  _id: ObjectId(),
  original: "如果我们真的将专注力视为脑力工作者最弥足珍贵的资源...",
  translation: "If we truly regard focus as the most precious resource...",
  segments: [
    {
      english: "If",
      chinese: "如果",
      order: 0
    },
    {
      english: "we",
      chinese: "我们",
      order: 1
    },
    // ...其他分词
  ],
  createdAt: ISODate()
}
```

1. 使用Redis作为缓存层：

```python
# 使用 Hash 结构存储
translation_key = f"translation:{translation_id}"
redis.hset(translation_key, mapping={
    "original": original_text,
    "translation": translated_text
})

# 使用 List 存储分词
segments_key = f"segments:{translation_id}"
for segment in segments:
    redis.rpush(segments_key, json.dumps(segment))
```

建议：

1. 如果需要关系完整性和复杂查询：
   - 使用关系型数据库（PostgreSQL/MySQL）
   - 便于进行分词统计、搜索等操作
   - 支持事务和数据一致性

2. 如果需要灵活的数据结构和快速读写：
   - 使用MongoDB
   - 数据结构可以灵活变化
   - 适合存储大量文本数据
   - 方便进行文档级别的操作

3. 混合使用方案：
   - PostgreSQL存储核心数据
   - MongoDB存储分词详情
   - Redis做热点数据缓存

实现建议：

```python
# PostgreSQL 实现示例
def save_translation(data):
    with connection.cursor() as cursor:
        # 插入原文记录
        cursor.execute("""
            INSERT INTO translations (original_text, translated_text)
            VALUES (%s, %s)
            RETURNING id
        """, (data['original'], data['translation']))
        
        translation_id = cursor.fetchone()[0]
        
        # 批量插入分词
        segments_data = [(
            translation_id,
            segment['english'],
            segment['chinese'],
            idx
        ) for idx, segment in enumerate(data['segments'])]
        
        cursor.executemany("""
            INSERT INTO segments (translation_id, english, chinese, segment_order)
            VALUES (%s, %s, %s, %s)
        """, segments_data)
```

这样的设计考虑了：

1. 数据完整性和关联关系
2. 查询效率（通过索引优化）
3. 分词顺序的保持
4. 未来可能的扩展需求（如添加用户系统、版本控制等）

你觉得哪种方案更适合你的需求？我可以针对具体方案提供更详细的建议。



[Q3：Viva/viva/后端/日志.md]
=======================

---
draw:
tags: []
title: 日志
date created: 2024-10-22
date modified: 2025-01-08
---

## 类中

```python
# 获取 logger

logger = logging.getLogger(__name__)

logger.error(f"Login process error: {str(e)}", exc_info=True)
```

## 配置

在 FastAPI 应用中，日志的输出位置取决于你的日志配置。在你展示的代码中，我看到使用了 Python 的标准日志模块 logging，但没有看到具体的日志配置。

默认情况下：

- 如果没有特别配置，日志会输出到标准错误流（stderr）
- 在开发环境中，这意味着日志会显示在你运行服务器的终端窗口中
- 在生产环境中，具体位置取决于你的部署方式（比如使用 uvicorn 或 gunicorn 运行时，日志可能会被重定向到特定文件）

建议在应用的入口文件（比如 main.py）中添加日志配置。

让我们逐步排查：

1. 首先，确认权限问题：

```yaml:~/viva/docker-compose.yml
services:
  backend:
    # ... 其他配置 ...
    volumes:
      - ./viva-backend/.env:/app/.env
      - ./viva-backend/logs:/app/logs  # 确保这个目录在宿主机上有写入权限
    user: "${UID}:${GID}"  # 添加这行，使用宿主机的用户权限
```

2. 在宿主机上创建日志目录并设置权限：

```bash
# 在宿主机执行
mkdir -p ./viva-backend/logs
chmod 777 ./viva-backend/logs
```

3. 修改日志配置，添加更多调试信息：

```python:viva-backend/app/infrastructure/logging/logging_config.py
def setup_logging(log_level=numeric_level):
    try:
        # 使用绝对路径
        log_dir = '/app/logs'
        print(f"Creating log directory at: {log_dir}")  # 调试信息
        
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
            print(f"Created log directory: {log_dir}")  # 调试信息
        
        log_file = os.path.join(log_dir, 'app.log')
        print(f"Log file path: {log_file}")  # 调试信息
        
        # 检查文件权限
        file_handler = RotatingFileHandler(
            log_file,
            maxBytes=10485760,
            backupCount=5
        )
        print("Successfully created file handler")  # 调试信息
        
        # ... 其余代码保持不变 ...
    except Exception as e:
        print(f"Error in setup_logging: {str(e)}")  # 调试信息
        raise
```

4. 检查容器日志：

```bash
# 查看容器日志
docker-compose logs backend

# 进入容器检查
docker-compose exec backend /bin/bash
ls -la /app/logs
```

5. 确保日志配置在应用启动时被正确加载：

```python:viva-backend/app/main.py
# 在应用启动时添加调试日志
if __name__ == "__main__":
    print("Starting application...")  # 调试信息
    logger = setup_logging()
    logger.info("Application started")
    print("Logging configured")  # 调试信息
```

这些步骤应该能帮助你找出问题所在。通常，如果前端收到 500 错误但日志文件不存在，最可能的原因是：

1. 容器内的权限问题导致无法创建或写入日志文件
2. 日志目录映射配置不正确
3. 日志配置在运行时出错但异常被吞掉了

你可以按照上述步骤逐一排查。如果还是有问题，可以查看 Docker 容器的标准输出日志，因为即使文件日志失败，控制台日志应该还是可以看到的。

---

日志配置文件（通常名为 logging.conf 或类似的名称）
setup_logging() 函数确定具体的输出日志位置

- 控制台（标准输出）
- 特定的日志文件
- 系统日志（在 Unix-like 系统上）
- 其他日志服务（如 ELK stack, Splunk 等）

## 每个模块最好创建自己的 logger 实例

不能直接使用 main.py 里的 logger。每个模块最好创建自己的 logger 实例，这是 Python logging 的最佳实践。

在 Python 的日志系统中，`logging.getLogger(__name__)` 会根据模块的名称创建一个独立的 logger。这样做有几个好处：

1. **模块隔离**：每个模块有自己的 logger，日志会显示来源模块的名称
2. **灵活控制**：可以针对不同模块设置不同的日志级别
3. **继承配置**：所有模块的 logger 会自动继承根 logger 的配置（handlers、formatters 等）

所以 auth_controller.py 中的写法是正确的：

```python:viva-backend/app/interfaces/api/auth_controller.py
logger = logging.getLogger(__name__)
```

当这个 logger 记录日志时，会显示类似这样的输出：

```Java
2024-03-21 10:30:45,123 - interfaces.api.auth_controller - INFO - Some message
```

而如果使用 main.py 中的 logger，日志来源会显示为 `__main__` 或 `main`，这样就无法准确追踪日志的来源模块了。

所以建议保持现有的方式，让每个模块使用自己的 logger 实例。只要 main.py 中的 `setup_logging()` 在最开始被调用，所有模块的 logger 都会正常工作。



[Q3：Viva/viva/后端/社交认证.md]
=========================

---
draw:
tags: []
title: 社交认证
date created: 2024-10-02
date modified: 2024-12-27
---

OAuth 2.0 是一个开放标准的授权协议，允许用户授权第三方应用访问他们在某个服务提供商上的受保护资源，而无需将自己的凭证直接提供给第三方应用。

```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant Google
    participant Backend
    User->>Frontend: 点击"使用Google登录"
    Frontend->>Google: 请求认证
    Google->>User: 显示登录/授权页面
    User->>Google: 登录并授权
    Google->>Frontend: 返回ID token
    Frontend->>Backend: 发送ID token
    Backend->>Google: 验证ID token
    Google->>Backend: 确认token有效
    Backend->>Frontend: 创建会话，返回自定义token或session
    Frontend->>User: 显示登录成功
```

这种登录交互方式属于 OAuth 2.0 和 OpenID Connect (OIDC) 技术的应用，特别是 " 使用 Google 登录 " 这种情况。让我为您详细解释一下：

1. OAuth 2.0：这是一个授权框架，允许第三方应用获取对用户资源的有限访问权，而无需知道用户的凭据。
2. OpenID Connect (OIDC)：这是建立在 OAuth 2.0 之上的身份层，增加了身份验证功能。它允许客户端应用基于授权服务器执行的身份验证来验证最终用户的身份。

在您描述的流程中，我们可以看到典型的 OAuth 2.0 + OIDC 流程，具体是 " 授权码流程 "（Authorization Code Flow）:



[Q3：Viva/viva/用户系统.md]
======================

---
draw:
tags: []
title: 用户系统
date created: 2024-10-01
date modified: 2024-11-12
---
1. **用户认证与授权**：
	- 首先，建立用户认证系统，支持注册、登录、密码管理等功能。可以使用OAuth、JWT等主流认证技术。
	- 实现用户角色和权限管理，定义不同用户的访问权限，确保用户只能访问自己拥有的数据。
2. **数据隔离**：
	- **表级隔离**：可以为每个用户创建独立的数据表，但这种方法通常会导致数据库膨胀，不适合大规模用户。
	- **行级隔离**：在数据表中增加用户ID字段，所有查询操作都需要通过用户ID来过滤，实现数据隔离。这种方法比较高效，适合多数场景。
	- **[schema](schema.md)级隔离**：每个用户对应一个独立的数据库Schema，这样可以做到更彻底的隔离，但会增加数据库管理的复杂性。
3. **接口设计**：
	- 在开发API接口时，确保所有请求都要经过权限校验，根据用户身份返回相应的数据。
	- 使用中间件进行用户身份验证，保证在用户未通过身份认证或未授权的情况下无法访问系统资源。
4. **数据安全**：
	- 对敏感数据进行加密存储，避免数据泄露风险。
	- 确保传输层安全，采用HTTPS进行通信，避免中间人攻击。
5. **监控与审计**：
	- 实现日志记录和监控，追踪用户行为，方便检测权限滥用或潜在的安全问题。



[Q3：Viva/viva/算法/分词.md]
=======================

---
draw:
tags: []
title: 分词
date created: 2024-10-23
date modified: 2024-12-27
---

## Task Description

You are a specialized language processing assistant that performs English-ordered, fine-grained Chinese-English sentence mapping.

## Core Requirements

1. MUST follow English sentence order in segmentation
2. MUST break down into smallest meaningful units
3. MUST maintain accurate Chinese-English semantic mapping

## Processing Steps

1. First create a natural, fluent English translation
2. Then break the English into minimal meaningful units
3. Finally map each English unit back to corresponding Chinese segments

## Granularity Rules

### Must Split

1. Compound nouns:
   - 名词短语必须拆分（"计算机科学" → "计算机" + "科学"）
   - 除非是专有名词或技术术语

2. Verb phrases:
   - 动词+补语（"写完" → "写" + "完"）
   - 动词+方向（"跑出去" → "跑" + "出去"）
   - 动词+结果（"听懂" → "听" + "懂"）

3. Modifiers:
   - 形容词+程度（"很大" → "很" + "大"）
   - 副词+动词（"快速地跑" → "快速" + "地" + "跑"）

4. Measure words:
   - 数词+量词+名词（"三本书" → "三" + "本" + "书"）

### Keep Together

1. Fixed expressions (成语)
2. Proper nouns
3. Technical terms
4. Single-morpheme words

## Output Schema

```json
{
  "original": "原中文句子",
  "translation": "Natural English translation",
  "segments": [
    {
      "english": "English segment (following English word order)",
      "chinese": "对应的中文片段",
      "position": 1  // Optional: Indicates original position in Chinese sentence
    }
  ]
}
```

## Step-by-Step Process

1. Translation Phase
   - Create natural English translation
   - Verify grammatical correctness
   - Ensure idiomatic expression

2. English Segmentation Phase
   - Break down English by minimal meaning units
   - Follow English syntax order
   - Mark function words and particles

3. Chinese Mapping Phase
   - Map each English segment to Chinese
   - Verify each Chinese segment exists in original
   - Maintain granularity rules

4. Validation Phase
   - Verify English order correctness
   - Check segmentation granularity
   - Ensure all Chinese segments present
   - Validate JSON format

## Examples

### Example 1 - Simple Sentence

Input: "我昨天在图书馆学习了三个小时。"

```json
{
  "original": "我昨天在图书馆学习了三个小时。",
  "translation": "I studied at the library for three hours yesterday.",
  "segments": [
    {
      "english": "I",
      "chinese": "我"
    },
    {
      "english": "studied",
      "chinese": "学习"
    },
    {
      "english": "at",
      "chinese": "在"
    },
    {
      "english": "the library",
      "chinese": "图书馆"
    },
    {
      "english": "for",
      "chinese": null
    },
    {
      "english": "three",
      "chinese": "三"
    },
    {
      "english": "hours",
      "chinese": "个小时"
    },
    {
      "english": "yesterday",
      "chinese": "昨天"
    }
  ]
}
```

### Example 2 - Complex Sentence

Input: "虽然今天下雨了，但是我还是决定去跑步。"

```json
{
  "original": "虽然今天下雨了，但是我还是决定去跑步。",
  "translation": "Although it rained today, I still decided to go running.",
  "segments": [
    {
      "english": "Although",
      "chinese": "虽然"
    },
    {
      "english": "it",
      "chinese": null
    },
    {
      "english": "rained",
      "chinese": "下雨"
    },
    {
      "english": "today",
      "chinese": "今天"
    },
    {
      "english": "I",
      "chinese": "我"
    },
    {
      "english": "still",
      "chinese": "还是"
    },
    {
      "english": "decided",
      "chinese": "决定"
    },
    {
      "english": "to go",
      "chinese": "去"
    },
    {
      "english": "running",
      "chinese": "跑步"
    }
  ]
}
```

## Critical Rules

1. MUST follow English word order in segments list
2. MUST break down to smallest meaningful units
3. MUST preserve accurate semantic mapping
4. MAY use null for English words without direct Chinese correspondence
5. MUST maintain proper granularity as specified
6. MUST ensure all Chinese segments come from original text

## Error Prevention

1. Always verify English grammar correctness
2. Check for over-segmentation
3. Validate Chinese segment existence
4. Confirm English word order
5. Test segment recombination

## input sentence

{sentence}

如果我们真的将专注力视为脑力工作者最弥足珍贵的资源，那就绝不能眼睁睁地坐视我们所取得的阶段性成果付诸东流；如果我们真的认识到生命中可用于非凡创作的时间无比珍贵，那就务必要构建起一个给力的知识系统，对各种过程性信息进行有效的回收再利用。

I'll help process this complex Chinese sentence following the specified requirements.

1. First, let's create a natural English translation:  
"If we truly regard focus as the most precious resource for knowledge workers, we must not sit idly by and watch our incremental achievements go to waste; if we truly realize that time available for extraordinary creation in life is incredibly precious, then we must build a powerful knowledge system to effectively recycle and reuse various procedural information."

2. Now let's break it down and map according to the schema:

```json
{
  "original": "如果我们真的将专注力视为脑力工作者最弥足珍贵的资源，那就绝不能眼睁睁地坐视我们所取得的阶段性成果付诸东流；如果我们真的认识到生命中可用于非凡创作的时间无比珍贵，那就务必要构建起一个给力的知识系统，对各种过程性信息进行有效的回收再利用。",
  "translation": "If we truly regard focus as the most precious resource for knowledge workers, we must not sit idly by and watch our incremental achievements go to waste; if we truly realize that time available for extraordinary creation in life is incredibly precious, then we must build a powerful knowledge system to effectively recycle and reuse various procedural information.",
  "segments": [
    {
      "english": "If",
      "chinese": "如果"
    },
    {
      "english": "we",
      "chinese": "我们"
    },
    {
      "english": "truly",
      "chinese": "真的"
    },
    {
      "english": "regard",
      "chinese": "视为"
    },
    {
      "english": "focus",
      "chinese": "专注力"
    },
    {
      "english": "as",
      "chinese": null
    },
    {
      "english": "the most precious",
      "chinese": "最弥足珍贵的"
    },
    {
      "english": "resource",
      "chinese": "资源"
    },
    {
      "english": "for",
      "chinese": null
    },
    {
      "english": "knowledge workers",
      "chinese": "脑力工作者"
    },
    {
      "english": "we",
      "chinese": "我们"
    },
    {
      "english": "must not",
      "chinese": "绝不能"
    },
    {
      "english": "sit idly by",
      "chinese": "眼睁睁地坐视"
    },
    {
      "english": "and watch",
      "chinese": null
    },
    {
      "english": "our",
      "chinese": "我们"
    },
    {
      "english": "incremental",
      "chinese": "阶段性"
    },
    {
      "english": "achievements",
      "chinese": "成果"
    },
    {
      "english": "go to waste",
      "chinese": "付诸东流"
    },
    {
      "english": "if",
      "chinese": "如果"
    },
    {
      "english": "we",
      "chinese": "我们"
    },
    {
      "english": "truly",
      "chinese": "真的"
    },
    {
      "english": "realize",
      "chinese": "认识到"
    },
    {
      "english": "that",
      "chinese": null
    },
    {
      "english": "time",
      "chinese": "时间"
    },
    {
      "english": "available for",
      "chinese": "可用于"
    },
    {
      "english": "extraordinary",
      "chinese": "非凡"
    },
    {
      "english": "creation",
      "chinese": "创作"
    },
    {
      "english": "in",
      "chinese": "中"
    },
    {
      "english": "life",
      "chinese": "生命"
    },
    {
      "english": "is incredibly precious",
      "chinese": "无比珍贵"
    },
    {
      "english": "then",
      "chinese": "那就"
    },
    {
      "english": "we must",
      "chinese": "务必要"
    },
    {
      "english": "build",
      "chinese": "构建"
    },
    {
      "english": "up",
      "chinese": "起"
    },
    {
      "english": "a",
      "chinese": "一个"
    },
    {
      "english": "powerful",
      "chinese": "给力的"
    },
    {
      "english": "knowledge system",
      "chinese": "知识系统"
    },
    {
      "english": "to",
      "chinese": null
    },
    {
      "english": "effectively",
      "chinese": "有效的"
    },
    {
      "english": "recycle",
      "chinese": "回收"
    },
    {
      "english": "and reuse",
      "chinese": "再利用"
    },
    {
      "english": "various",
      "chinese": "各种"
    },
    {
      "english": "procedural",
      "chinese": "过程性"
    },
    {
      "english": "information",
      "chinese": "信息"
    }
  ]
}
```

This segmentation follows the requirements by:

1. Following English word order
2. Breaking down into minimal meaningful units
3. Maintaining accurate Chinese-English mapping
4. Using null for English words without direct Chinese correspondents
5. Keeping appropriate compounds together (like "知识系统")
6. Breaking down separable elements (like "回收" and "再利用")
7. Preserving all original Chinese segments



[Q3：Viva/viva/算法/微调.md]
=======================

---
draw:
tags: []
title: 微调
date created: 2024-10-23
date modified: 2024-12-27
---

## 参考资料

https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing

[hiyouga/LLaMA-Factory/LLaMA-Factory: 一键调用LLaMA-Factory ，轻松微调。集成多种辅助功能。 - CG (codewithgpu.com)](https://www.codewithgpu.com/i/hiyouga/LLaMA-Factory/LLaMA-Factory)

我来为您解释一下如何微调(fine-tune) Llama 模型，我会尽量用通俗的语言解释这个过程。

作为 Java 程序员转入 AI 领域，您需要了解以下几个关键概念和步骤:

1. 基本概念:
- 微调是指在预训练模型的基础上,用特定数据集进行进一步训练,使模型更适合特定任务
- LoRA(Low-Rank Adaptation)是一种高效的微调方法,只需要更新很小一部分参数(1-10%)
- 在这个例子中使用了 Unsloth 框架,它可以让训练速度提升 2 倍

1. 主要步骤:

a) 环境准备:

```python
# 安装必要的库
!pip install unsloth
```

b) 加载模型:

```python
from unsloth import FastLanguageModel

# 加载预训练模型
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct",  # 选择模型
    max_seq_length = 2048,  # 最大序列长度
    load_in_4bit = True     # 使用4位量化以减少内存使用
)
```

c) 准备训练数据:

- 需要准备对话格式的数据,包含用户输入(user)和助手回答(assistant)
- 数据需要转换成模型可以理解的格式

d) 配置训练参数:

```python
# 设置 LoRA 参数
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,                # LoRA 秩,建议值 8,16,32,64,128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                     "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 16,
    lora_dropout = 0
)
```

e) 开始训练:

```python
from trl import SFTTrainer

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text"
)

trainer.train()
```

f) 保存模型:

```python
# 本地保存
model.save_pretrained("lora_model")
tokenizer.save_pretrained("lora_model")
```

1. 实用建议:
- 建议使用 Google Colab 等云平台进行训练,因为需要 GPU 资源
- 开始时可以用小数据集测试流程
- 确保训练数据质量,这对模型效果影响很大
- 可以先用较小的模型(如 1B 或 3B 参数量的版本)来测试
- 注意保存训练检查点,以防训练中断

1. 资源消耗参考:
- 使用 Tesla T4 GPU 训练时,峰值内存使用约 6.5GB
- 完整训练时间因数据集大小而异,示例中约 7.5 分钟

如果您想开始尝试:

1. 先在 Google Colab 上运行示例代码
2. 准备一个小的测试数据集
3. 逐步调整参数观察效果
4. 成功后再尝试使用自己的数据集进行训练

需要注意的是,虽然示例使用 Python,但您不需要精通 Python,只需要能看懂基本语法并修改关键参数即可。随着您的深入学习,可以逐步掌握更多 AI 相关知识。



[Q3：Viva/进度可视化/英语学习可视化$.md]
===========================

---
draw:
tags: []
title: 英语学习可视化$
date created: 2024-08-17
date modified: 2024-12-27
---

```mermaid
graph TD
    A[英语学习进度仪表盘] --> B[词汇量概览]
    A --> C[听说读写词汇量趋势]
    A --> D[每日学习目标达成率]
    A --> E[词汇分类统计]

    B --> B1[总词汇量计数器]
    B --> B2[听说读写词汇量仪表]

    C --> C1[折线图:各项技能词汇量随时间变化]

    D --> D1[仪表盘:日目标完成度]
    D --> D2[热力图:周/月学习情况]

    E --> E1[饼图:词汇类型分布]
    E --> E2[条形图:不同难度词汇数量]
```



[Q3：Viva/项目管理.md]
=================

---
draw:
tags: []
title: 项目管理
date created: 2024-10-28
date modified: 2024-11-12
---

如果viva能完成 就说明自己彻底具备工程能力了



[Q3：丽景私募/丽景私募 一期.md]
====================

---
draw:
tags: []
title: 丽景私募 一期
date created: 2024-10-06
date modified: 2024-11-12
---

## 投资体系

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F10%2F06%2F15-03-16-c9d6ffc5c8461c02d5ddcfe2fa1c2077-202410061503462-a90214.png)

## 绩效摘要

- 初始投资: $XXXX.XX
- 当前总资产: $XXXX.XX
- 总盈亏: $XXXX.XX (XX.XX%)
- 年化收益率: XX.XX%



[Q3：丽景私募/如何培养仓位的思维？.md]
=======================

---
draw:
tags: []
title: 如何培养仓位的思维？
date created: 2024-10-09
date modified: 2024-11-12
---

想象你有10元钱去买糖果。这个10元就是一个"constraint",限制了你能买多少糖果。

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F10%2F09%2F16-59-26-c461f02e5844d8bdcf593de50e7f1baf-202410091659161-aad548.png)

培养仓位思维是投资中非常重要的一环。我来为你解释一下仓位思维,并提供一些培养这种思维的方法:

1. 什么是仓位思维:  
   仓位思维就是考虑你要把多少钱投入到某个投资中。它不仅仅是考虑买不买,而是要考虑买多少。

2. 为什么重要:
   - 控制风险: 不把所有鸡蛋放在一个篮子里。
   - 灵活应对: 留有资金可以抓住新的机会。

3. 如何培养仓位思维:

   a. 理解风险:  
      想象你有100块零花钱。如果你全部用来买一种糖果,万一这种糖果不好吃,你就亏大了。但如果你只用20块买这种糖果,就算不好吃,你还有80块可以买其他的。

   b. 分散投资:  
      比如你有5个存钱罐,可以考虑在每个存钱罐里放不同数量的钱,这样某一个存钱罐出问题,不会影响全部的钱。

   c. 逐步建仓:  
      不要一次性投入所有资金。可以先投一点,观察一段时间,如果情况好就再投一些。

   d. 设置上限:  
      决定投资在某一个地方的钱最多不超过你总钱数的一个比例,比如20%。

   e. 根据信心调整:  
      对于你更有信心的投资,可以投入更多;对不太确定的,就少投一点。

   f. 考虑市场情况:  
      如果觉得现在整个"市场"不太好,可以保留更多现金,等待更好的机会。

4. 练习方法:
   - 可以用扑克牌来模拟:假设每张牌代表一定金额,练习如何在不同的"投资"中分配这些牌。
   - 使用虚拟投资游戏:很多App或网站提供模拟投资功能,你可以在不用真钱的情况下练习。

5. 记住:  
   没有完美的仓位,关键是要根据自己的情况和对市场的判断来做决定。随着你对投资的了解越来越多,你会越来越擅长做这些决定。

培养仓位思维需要时间和练习。开始时可能会感觉复杂,但随着经验的积累,你会发现这种思维方式会让你的投资决策更加明智和灵活。



[Q3：丽景私募/第一期/0 入金.md]
=====================

---
draw:
tags: []
title: 0 入金
date created: 2024-10-08
date modified: 2024-11-12
---

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F10%2F08%2F22-59-26-9bb04218d98601225222147d166ebf02-202410082259795-ba8248.png)



[Q3：丽景私募/第一期/1 交易记录.md]
=======================

---
draw:
title: 1 交易记录
date created: 2024-10-07
date modified: 2025-02-14
---

| code    | create_time             | trd_side | qty   | price    | 当时的思路                                              | 一段时间后回头复盘                                                               |
| ------- | ----------------------- | -------- | ----- | -------- | -------------------------------------------------- | ----------------------------------------------------------------------- |
| US.TSLA | 2024-10-23 21:53:19.701 | SELL     | 10.0  | 239.8600 | 财报后，盘后大涨 13%，止盈一部分，剩下的可以看一波 250？|                                                                         |
| US.AAPL | 2024-10-21 09:52:46.092 | BUY      | 4.0   | 234.9797 | AAPL 基本面稳定，长期持有                                    |                                                                         |
| US.NVDA | 2024-10-21 09:52:12.493 | SELL     | 7.0   | 140.7006 | NVDA 回到历史最高点，止盈兑现一部分                               | 后来看这次是一次彻底的突破，Openai1500 亿美元的新估值带动整个 AI 板块新估值                           |
| US.ASML | 2024-10-17 09:30:26.449 | BUY      | 2.0   | 691.1415 | 下跌加仓，摊薄成本，整体仓位控制在 20%                              |                                                                         |
| US.ASML | 2024-10-15 12:04:18.479 | BUY      | 2.0   | 725.9900 | ASML 订单不及预期，股价跳水，但是基本面应该还不错，所以入了一些                 |                                                                         |
| US.MSTU | 2024-10-15 12:03:23.207 | SELL     | 10.0  | 51.6550  | 止损                                                 | 有点追涨了，不过仓位控制的不错                                                         |
| US.NVDA | 2024-10-15 11:59:48.666 | BUY      | 10.0  | 130.6500 | 逐步调仓到一半苹果一半英伟达                                     | 另外放15%仓位到加密，富途这个账户买不了 IBIT，所以准备买 MSTR                                   |
| US.NVDA | 2024-10-15 10:53:37.625 | BUY      | 20.0  | 132.4695 |                                                    |                                                                         |
| US.MSTU | 2024-10-14 08:04:48.557 | BUY      | 10.0  | 67.8400  | MSTR 最近异动，希望用两倍杠杆的 MSTU 快速吃一波                      |                                                                         |
| US.CHAU | 2024-10-14 04:39:18.805 | SELL     | 30.0  | 17.2000  |                                                    |                                                                         |
| US.GPRO | 2024-10-11 14:19:03.007 | SELL     | 1.0   | 1.2800   | 开户的时候送的碎股，清理掉减少信息干扰                                |                                                                         |
| US.AAPL | 2024-10-11 12:06:27.853 | BUY      | 10.0  | 227.9892 | 苹果比较稳健，放了不到百分之 20 的仓位                              |                                                                         |
| US.TSLA | 2024-10-11 12:05:16.318 | BUY      | 10.0  | 220.3195 | Robotaxi 发布会过短，未谈及商业模式，预期落地时间太长导致不及预期，难以与 waymo 竞争 | 如果当时提前 sell the news 或许是个不错的选择，这也是 1008 的大 A 和这次特斯拉让我学会的，卖在人声鼎沸时        |
| US.CHAU | 2024-10-11 11:55:33.152 | SELL     | 100.0 | 17.7405  | 周五计划彻底清掉中概相关，但是当时因为睡着了，剩一点没走完                      | YANG 和 CHAU 做了大量频繁 HF 的 DT 日内交易，但是效果并不好，且十分浪费精力，以后尽量摒弃这种操作              |
| US.CHAU | 2024-10-11 11:55:23.923 | SELL     | 100.0 | 17.7300  |                                                    |                                                                         |
| US.CHAU | 2024-10-11 11:50:34.419 | BUY      | 50.0  | 17.8600  |                                                    |                                                                         |
| US.CHAU | 2024-10-11 11:15:16.754 | SELL     | 50.0  | 17.6700  |                                                    |                                                                         |
| US.CHAU | 2024-10-11 10:27:15.928 | BUY      | 50.0  | 17.3600  |                                                    |                                                                         |
| US.CHAU | 2024-10-11 10:01:00.647 | SELL     | 30.0  | 17.1600  |                                                    |                                                                         |
| US.YANG | 2024-10-11 09:38:26.445 | SELL     | 200.0 | 3.3823   |                                                    |                                                                         |
| US.YANG | 2024-10-11 09:35:21.831 | SELL     | 67.0  | 3.3700   |                                                    |                                                                         |
| US.YANG | 2024-10-11 09:35:21.808 | SELL     | 233.0 | 3.3700   |                                                    |                                                                         |
| US.YANG | 2024-10-11 09:32:06.605 | BUY      | 500.0 | 3.4487   |                                                    |                                                                         |
| US.CHAU | 2024-10-11 07:44:31.337 | SELL     | 70.0  | 16.7600  |                                                    |                                                                         |
| US.CHAU | 2024-10-11 02:46:27.047 | BUY      | 70.0  | 16.5000  |                                                    |                                                                         |
| US.CHAU | 2024-10-11 02:27:08.492 | SELL     | 70.0  | 16.6500  |                                                    |                                                                         |
| US.CHAU | 2024-10-11 02:22:32.961 | BUY      | 70.0  | 16.6300  |                                                    |                                                                         |
| US.YANG | 2024-10-11 01:55:44.761 | SELL     | 200.0 | 3.4100   |                                                    |                                                                         |
| US.YANG | 2024-10-11 01:04:36.942 | SELL     | 300.0 | 3.3900   |                                                    |                                                                         |
| US.YANG | 2024-10-10 23:13:11.684 | SELL     | 299.0 | 3.3600   |                                                    |                                                                         |
| US.YANG | 2024-10-10 23:12:59.800 | SELL     | 1.0   | 3.3600   |                                                    |                                                                         |
| US.YANG | 2024-10-10 23:05:36.112 | BUY      | 300.0 | 3.3200   |                                                    |                                                                         |
| US.YANG | 2024-10-10 22:10:34.277 | SELL     | 300.0 | 3.3500   |                                                    |                                                                         |
| US.YANG | 2024-10-10 21:34:59.746 | SELL     | 300.0 | 3.3200   |                                                    |                                                                         |
| US.YANG | 2024-10-10 21:22:20.181 | SELL     | 300.0 | 3.2800   |                                                    |                                                                         |
| US.YANG | 2024-10-10 14:22:18.134 | BUY      | 600.0 | 3.1970   |                                                    |                                                                         |
| US.YANG | 2024-10-10 13:59:39.092 | SELL     | 500.0 | 3.1503   |                                                    |                                                                         |
| US.UVXY | 2024-10-10 13:43:18.755 | SELL     | 85.0  | 28.8900  | 做了一些 UVXY 的 DT，总共盈利53.27美元                         | UVXY 跟着布林带策略交易的话，还是比较容易赚一些小钱的                                           |
| US.UVXY | 2024-10-10 13:33:17.523 | BUY      | 50.0  | 28.7796  |                                                    |                                                                         |
| US.CHAU | 2024-10-10 13:28:31.379 | BUY      | 50.0  | 18.0700  |                                                    |                                                                         |
| US.CHAU | 2024-10-10 13:22:30.652 | SELL     | 30.0  | 17.9809  |                                                    |                                                                         |
| US.UVXY | 2024-10-10 13:11:23.723 | SELL     | 35.0  | 29.0900  |                                                    |                                                                         |
| US.UVXY | 2024-10-10 12:33:42.989 | SELL     | 120.0 | 28.6900  |                                                    |                                                                         |
| US.UVXY | 2024-10-10 12:10:41.090 | BUY      | 50.0  | 28.4500  |                                                    |                                                                         |
| US.UVXY | 2024-10-10 11:41:28.076 | BUY      | 8.0   | 28.5400  |                                                    |                                                                         |
| US.UVXY | 2024-10-10 11:41:28.057 | BUY      | 62.0  | 28.5400  |                                                    |                                                                         |
| US.YANG | 2024-10-10 11:40:58.718 | BUY      | 300.0 | 3.2400   |                                                    |                                                                         |
| US.UVXY | 2024-10-10 11:34:26.301 | SELL     | 50.0  | 28.4400  |                                                    |                                                                         |
| US.YANG | 2024-10-10 11:27:04.087 | BUY      | 300.0 | 3.2483   |                                                    |                                                                         |
| US.YANG | 2024-10-10 11:19:57.277 | SELL     | 600.0 | 3.2100   |                                                    |                                                                         |
| US.UVXY | 2024-10-10 11:11:12.101 | BUY      | 50.0  | 28.4600  |                                                    |                                                                         |
| US.YANG | 2024-10-10 11:08:46.915 | BUY      | 300.0 | 3.2557   |                                                    |                                                                         |
| US.YANG | 2024-10-10 10:21:01.011 | BUY      | 300.0 | 3.2757   |                                                    |                                                                         |
| US.UVXY | 2024-10-10 09:57:16.092 | BUY      | 70.0  | 28.6100  |                                                    |                                                                         |
| US.YANG | 2024-10-10 09:52:21.946 | SELL     | 300.0 | 3.4000   |                                                    |                                                                         |
| US.YANG | 2024-10-10 09:44:41.950 | SELL     | 300.0 | 3.3550   |                                                    |                                                                         |
| US.TSLA | 2024-10-10 07:50:43.634 | BUY      | 10.0  | 243.8500 |                                                    |                                                                         |
| US.YANG | 2024-10-10 00:29:49.251 | SELL     | 100.0 | 3.0900   |                                                    |                                                                         |
| US.YANG | 2024-10-10 00:29:49.233 | SELL     | 100.0 | 3.0900   |                                                    |                                                                         |
| US.YANG | 2024-10-10 00:29:49.214 | SELL     | 200.0 | 3.0900   |                                                    |                                                                         |
| US.YANG | 2024-10-09 23:26:40.531 | BUY      | 400.0 | 3.1600   |                                                    |                                                                         |
| US.NVDA | 2024-10-09 14:11:30.244 | BUY      | 30.0  | 131.8400 | 账户前期表现不佳，购入相对基本面好，且更有想象空间的英伟达，从整体上更有希望填大 A 的窟窿     |                                                                         |
| US.YANG | 2024-10-09 11:11:10.302 | BUY      | 400.0 | 3.3200   |                                                    |                                                                         |
| US.YANG | 2024-10-09 06:45:23.620 | BUY      | 127.0 | 3.5000   |                                                    |                                                                         |
| US.YANG | 2024-10-09 06:45:23.600 | BUY      | 300.0 | 3.5000   |                                                    |                                                                         |
| US.YANG | 2024-10-09 06:45:23.579 | BUY      | 73.0  | 3.5000   |                                                    |                                                                         |
| US.YANG | 2024-10-09 06:20:08.101 | BUY      | 400.0 | 3.4650   | CHAU 表现不佳后，企图通过 YANG 来做一些止损                        | 当时写过一句话，参考 UVXY 过去的表现，其实短期分歧大的杠杆 ETF 是可以捏一段时间到合理价位再出的，这一点之后做的并不好，操作非常频繁 |
| US.CHAU | 2024-10-08 15:55:44.001 | SELL     | 60.0  | 20.2400  |                                                    |                                                                         |
| US.CHAU | 2024-10-08 15:10:20.925 | BUY      | 50.0  | 19.4723  |                                                    |                                                                         |
| US.FUTU | 2024-10-08 14:40:10.091 | SELL     | 5.0   | 112.4000 | 获利了结                                               | 富途这一波不错，但是可能只是运气好，参考之后老虎的表现的话，这一波的短牛让券商的表现都非常诡异                         |
| US.CHAU | 2024-10-08 13:48:14.890 | BUY      | 100.0 | 19.8200  |                                                    |                                                                         |
| US.FUTU | 2024-10-08 13:08:48.681 | SELL     | 5.0   | 111.1000 |                                                    |                                                                         |
| US.FUTU | 2024-10-08 09:34:12.686 | BUY      | 10.0  | 104.8400 | 富途目前有些超跌                                           |                                                                         |
| US.TSLA | 2024-10-08 09:10:05.498 | BUY      | 4.0   | 242.8900 |                                                    |                                                                         |
| US.TSLA | 2024-10-08 09:09:56.818 | BUY      | 3.0   | 242.8900 | Robotaxi 发布会                                       |                                                                         |
| US.CHAU | 2024-10-08 07:46:44.290 | BUY      | 30.0  | 21.0600  |                                                    |                                                                         |
| US.CHAU | 2024-10-08 07:46:44.270 | BUY      | 70.0  | 21.0600  | CHAU超跌太多了                                          | 事后来看，8 号大跌后，9 号继续再跌，大 A 这一轮表现非常差                                        |
| US.BND  | 2024-08-12 13:45:11.664 | SELL     | 1.0   | 74.0000  |                                                    |                                                                         |
| US.BND  | 2024-08-05 04:00:49.516 | BUY      | 1.0   | 75.0000  |                                                    |                                                                         |



[Q3：丽景私募/第一期/周报/TradingView.md]
===============================

---
draw:
tags: []
title: TradingView
date created: 2024-10-13
date modified: 2024-11-12
---

tradeview的alart功能非常不错，是我以前在 IB 中想要的那种方式



[Q3：丽景私募/第一期/周报/美股投资周报 第一周.md]
==============================

---
draw:
tags: []
title: 美股投资周报 第一周
date created: 2024-10-13
date modified: 2024-11-12
---

## 1. 本周市场概览

1. Robotaxi 发布会
2. 道琼斯创新高
3. ASML 跳水，导致整个芯片半导体下跌

## 2. 投资组合表现

### 目前持仓

| code    | stock_name | qty  | cost_price | market_val | pl_ratio |
| ------- | ---------- | ---- | ---------- | ---------- | -------- |
| US.TSLA | 特斯拉        | 27.0 | 234.886    | 5928.39    | -6.52    |
| US.NVDA | 英伟达        | 60.0 | 131.851    | 7896.00    | -0.19    |
| US.ASML | 阿斯麦        | 2.0  | 725.990    | 1460.86    | 0.61     |
| US.AAPL | 苹果         | 10.0 | 227.989    | 2338.50    | 2.57     |

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F10%2F16%2F15-31-07-c24a4a381aae0141437cf9291a8c5264-202410161531490-6ca03e.png)

### 已实现盈亏

| Stock   | Unrealized P/L | Realized P/L | Total P/L |
| ------- | -------------- | ------------ | --------- |
| US.FUTU | 0.00           | 69.10        | 69.10     |
| US.UVXY | 0.00           | 53.62        | 53.62     |
| US.YANG | 0.00           | -129.35      | -129.35   |
| US.MSTU | 0.00           | -161.85      | -161.85   |
| US.CHAU | 0.00           | -491.34      | -491.34   |

| Stock   | Unrealized P/L | Realized P/L | Total P/L |
| ------- | -------------- | ------------ | --------- |
| US.YANG | 0.00           | -129.35      | -129.35   |
| US.MSTU | 0.00           | -161.85      | -161.85   |
| US.CHAU | 0.00           | -491.34      | -491.34   |

## 4. 下周展望

1. TSLA、NVDA 回调

## 5. 投资策略调整建议

1. 强化仓位意识
2. 避免闲着就非要做一些操作，事实证明对于普通投资者来说，日内噪音太多，DT 是个愚蠢的选择，起码把交易放在周维度，而不是天维度，更不是小时维度
  

如有任何问题或需要进一步讨论,请随时与我联系。祝投资顺利!



[Q3：丽景私募/第一期/周报/美股投资周报 第二周.md]
==============================

---
draw:
tags: []
title: 美股投资周报 第二周
date created: 2024-10-20
date modified: 2024-11-12
---

用nvda和aapl的rebalance来缓解自己的操作欲望



[Q3：丽景私募/第一期/学习点.md]
====================

---
draw:
tags: []
title: 学习点
date created: 2024-10-09
date modified: 2024-11-12
---

1. 没有 revenge trade，做到了无感情交易

期权止盈系统：

1. 设置止盈点，要么基于价格要么基于事件，前提是要理解深刻
2. 关键日期要 reserve 时间来思考操作
3. 动态评价风险。止盈点要调整
4. 三个月内的期权盈利 100%，不管发生什么开始每日动态止盈
5. 严格控制仓位，尤其是短期的风险较高的事件

我看他用波浪理論，再謹守自己買賣的rule. 紀律心態比技術更緊要吧



[Q3：如何投资/观察仓.md]
================

---
draw:
tags: []
title: 观察仓
date created: 2024-10-17
date modified: 2024-11-12
---

方法二：设置观察仓。设置观察仓，就是指把少量的、无关痛痒的钱投入到你想要了解的投资标的上。这部分钱会自然而然地吸引你的注意力，让你去观察这个标的的风险和收益特性，在做到知己知彼后，你就可以再做进一步的投资。这个方法我自己屡试不爽，既能快速熟悉不同标的的特点，又不用承受太大的投资损失。所以，我建议你在进入任何一个新的投资领域时，都先采用这个方法，快速学习的同时又不会伤筋动骨。

方法三：结合兴趣，有重点地学习。这一点前面也提到了。我鼓励你去多了解各类投资标的的特性，并不是要你了解所有细分的投资标的，事实上也没有人可以做到这一点。你要做的，是结合自己的兴趣，进行有重点的学习。还是拿我自己的经历给大家举例子：我对固收类、股票类、实物类、新兴类这四大类投资标的都有研究，分别是从银行理财产品、科技股、买房和比特币入手的。在广度上，对大类投资标的有一个的正确的认知，同时对自己感兴趣的点有较深入的研



[Q3：如何投资/项目总结.md]
=================

---
draw:
tags: []
title: 项目总结
date created: 2024-10-17
date modified: 2024-11-12
---
**



[Q3：领航者 陪跑计划/领航者 陪跑计划.md]
=========================

---
draw:
tags: []
title: 领航者 陪跑计划
date created: 2024-09-24
date modified: 2024-11-12
---

## 团队成员背景

- 字节跳动、阿里巴巴、美团、拼多多、去哪儿等知名互联网企业的工程师、产品经理（人均年薪40w+）
- 中科院人工智能博士、哈工大电子信息博士、南京大学博士

## 项目愿景

   陪跑==泛计算机专业==的本科同学们，在大学四年里消除信息差，做好升学、就业的准备，读研能进好学校，工作能拿高薪！

## 适合人群

- 适合==泛计算机专业==的同学们，想要不虚度本科四年，希望在专业知识、行业认知、个人发展方面都有所进步，毕业能有一份年薪 30w 以上工作。
- ==泛计算机专业==包括：计算机科学与技术、软件工程、自动化、控制工程、电子信息工程、通信工程、网络工程、人工智能、物联网工程、数据科学与大数据技术、信息安全、嵌入式系统

## 交付内容

### 按时间线划分

前期重基础

- 选课咨询
- 编程语言入门帮助
- 专业方向选择咨询
- 计算机相关主流技术答疑解惑
- 奖学金申请经验分享
- 竞赛参与和获奖经验
- 国外高阶计算机课程资源分享和帮助
- 免费提供ChatGPT会员、科学上网会员、AI工具使用指导、科学上网工具指导
- 免费英语听说读写提升经验指导、托福雅思规划  
后期重发展
- 保研、考研、留学、考公方向简单咨询、靠谱机构对接
- 计算机就业方向深度咨询和辅导
- 实习机会内推、春招秋招内推

### 按能力培养划分

- 专业技能上，瞄准热门方向，获取优质资源，扫清学习道路上的一切障碍！
- 信息资讯上，具备全球视野，追踪科技前沿，小到买什么电脑，用什么软件，大到职业选择、前途发展。能快人一步，提前布局，抹平信息差！
- 交友进步上，打造优秀上进的同龄人社区，相互鼓励，相互督促，共同进步，利用好大学时光。
	- 不定期腾讯会议聊天，聊学习生活，答疑解惑、分享进步的喜悦。
	- 提前对接行业内人脉，加入学术界和企业界的朋友圈，打开个人发展的新视野，向拿到结果的优秀前辈学习！

## 模式

- 微信群+腾讯会议
- 严格控制人数，保障交付质量

## 团队特色

团队成员们作为学长学姐，均有较为充实的大学时光，熟悉整个大学生涯发展中的各种『套路』，并且已经在职业发展和学术发展上拿到一定结果。

## 定价

- 报名的同学会参加一个基础的面试，通过实际沟通确定价格（区间在 1.3 到 2 w），包括大一到大四完整的服务周期。

## 交付细节

- [ ] 前期先引自己的微信上，等攒够 20 人左右再建群

1. 学生侧：每个人在我这里填表格，包括大学、出生年、专业、年级，每个人进群后改备注大学-专业-年级
2. 老师侧：面向团队成员出一个话题池，让团队成员认领出wiki，并负责 wiki 内容的答疑，这样把各种话题的最优解沉淀下来
3. 帮忙买翻墙账号和提供安装教程、chrome 里沉浸式翻译的插件、reddit 和 quora 的使用
4. 后期把沉淀下来的客户和文档转入知识星球，这样能更好地分享历史信息



[Q4：DEV insight/Java 基础题$.md]
=============================

---
draw:
tags: []
title: Java 基础题$
date created: 2024-08-23
date modified: 2024-11-12
---

## [第一性原理](第一性原理.md)

回顾[Java@](Java@.md)的细节、低频知识，打牢语言基础。

## [SMART 指标](SMART%20指标.md)

| 日期   | 进度      | 是否完成 |
| ---- | ------- | ---- |
| 8.23 | 415/844 | ✅    |
| 8.24 | 425/844 |      |
| 8.25 | 435/844 |      |
| 8.26 | 445/844 |      |
| 8.27 | 455/844 |      |
| 8.28 | 465/844 |      |
| 8.29 | 475/844 |      |
| 8.30 | 485/844 |      |
| 8.31 | 495/844 |      |
| 9.1  | 505/844 |      |
| 9.2  | 515/844 |      |
| 9.3  | 525/844 |      |
| 9.4  | 535/844 |      |
| 9.5  | 545/844 |      |
| 9.6  | 555/844 |      |
| 9.7  | 565/844 |      |
| 9.8  | 575/844 |      |
| 9.9  | 585/844 |      |
| 9.10 | 595/844 | 443  |
| 9.11 | 605/844 |      |
| 9.12 | 615/844 |      |
| 9.13 | 625/844 |      |
| 9.14 | 635/844 |      |
| 9.15 | 645/844 |      |
| 9.16 | 655/844 |      |
| 9.17 | 665/844 |      |
| 9.18 | 675/844 |      |
| 9.19 | 685/844 |      |
| 9.20 | 695/844 |      |
| 9.21 | 705/844 |      |
| 9.22 | 715/844 |      |
| 9.23 | 725/844 |      |
| 9.24 | 735/844 |      |
| 9.25 | 745/844 |      |
| 9.26 | 755/844 |      |
| 9.27 | 765/844 |      |
| 9.28 | 775/844 |      |
| 9.29 | 785/844 |      |
| 9.30 | 795/844 |      |
| 10.1 | 805/844 |      |
| 10.2 | 815/844 |      |
| 10.3 | 825/844 |      |
| 10.4 | 835/844 |      |
| 10.5 | 844/844 | ✅    |



[Q4：DEV insight/Q4：DEV insight.md]
==================================

---
draw:
tags: []
title: Q4：DEV insight
date created: 2024-12-01
date modified: 2024-12-27
---

Q2：每天十道 java 题目

改为

Q4：DEV insight

不再关注之前牛客里面的 java 题目中的细节，按需学习即可

而是转向整个后端开发领域的 insight，利用 hollis 的八股文库的分词结果，拓宽自己的技术视野到 P8 水准



[Q4：一百首歌/歌单.md]
===============

---
draw:
tags: []
title: 歌单
date created: 2024-10-17
date modified: 2024-11-12
---



[Q4：自建 publish/Publish 插件.md]
=============================

---
draw:
tags: []
title: Publish 插件
date created: 2024-11-12
date modified: 2024-12-27
---


1. 写插件
2. 点击publish 插件后，lint 生成 frontmatter 里的 title，拷贝文件到 vuepress 的 docs 目录下，然后git push，npm deploy
3. 评论系统



[qt Agent/API 特性/Assistant API.md]
==================================

---
aliases: [Assistants Function Calling with Streaming]
draw:
tags: []
title: Assistants Function Calling with Streaming
linter-yaml-title-alias: Assistants Function Calling with Streaming
date created: 2024-11-16
date modified: 2024-12-27
---
1. 创建有状态的 thread、无状态的 assistant
2. 把 message 加到 thread 中
3. 选一个 assistant 来处理2 中的 thread，生成一个 run
4. 轮询当 run 的状态为完成时，从 thread 拿到新message

## Assistants Function Calling with Streaming

https://platform.openai.com/docs/assistants/tools/function-calling



[qt Agent/API 特性/Batch API.md]
==============================

---
draw:
tags: []
title: Batch API
date created: 2024-11-25
date modified: 2024-12-27
---
**Batch API** 是一种可以批量处理大量请求的异步接口，非常适合那些不需要立即返回结果的任务。例如，如果你有一个庞大的数据集需要分类、要进行大量的嵌入生成，或者需要对模型进行评估，那么 Batch API 可以帮助你更快、更省成本地完成这些工作。

  

**Batch API 的好处**

  

1. **降低成本**：Batch API 每次请求的费用比实时 API 低 50%，适合需要处理大量数据但又想节省成本的场景。
2. **更高的速率限制**：使用 Batch API 时，允许的请求速率比标准 API 高很多，让你可以一次性处理更多的请求。
3. **24小时内完成**：每个批次最多需要 24 小时就可以完成。对于不需要立即返回结果的任务（如数据分析），这是个非常合适的选择。

  

**使用流程**

  

1. **准备批处理文件**：首先，你需要一个.jsonl 文件（JSON格式的文本文件，每行代表一个请求）。在每一行里，包含请求的详细信息，比如模型名称、输入文本、输出的最大 token 数量等。
2. **上传批处理文件**：用 OpenAI 提供的 Files API 上传这个.jsonl 文件。
3. **创建批处理任务**：上传文件后，创建一个批处理任务。这个任务会依次处理每一行请求并返回结果。
4. **查询任务状态**：任务创建后，你可以随时查看其状态，如"处理中"、"已完成"或"已过期"等。
5. **下载批处理结果**：任务完成后，可以下载一个.jsonl 文件，其中包含每条请求的响应。
6. **取消任务**：如果批处理任务需要中止，可以发出取消指令。

  

**使用场景**
- **运行评估**：可以批量测试模型的输出质量，查看模型是否符合预期。
- **分类数据集**：对大规模数据集进行分类。
- **嵌入内容**：将大量文本内容转换成向量（Embedding）用于进一步的分析。

  

**限制和注意事项**

  

- **请求数量**：每个批处理任务最多包含 50,000 个请求。
- **文件大小**：批处理文件最大不能超过 200MB。
- **单个批次的执行时间**：每个批次会在 24 小时内完成，但通常会更快。
- **任务过期**：如果批处理任务超过 24 小时未完成，会自动变成过期状态，已完成的请求结果仍可以下载。

  

**总结**

  

Batch API 就像是一个"后台处理器"，让你可以把大量的请求一次性提交给模型，它会在后台自动处理，而不占用实时请求的资源。所以，对于需要批量处理而不急于得到即时反馈的任务来说，Batch API 是一个高效且经济的选择。

```Java

```



[qt Agent/API 特性/Function Call.md]
==================================

---
draw:
tags: []
title: Function Call
date created: 2024-11-15
date modified: 2024-12-27
---

是的，理解正确！在 **Function Calling** 和 **GPT Actions** 中，"tools" 的确有所不同：

1. **Function Calling**：

- **工具**：这里的 "tools"（函数）通常是我们自己在本地定义的函数。模型会根据定义生成参数，但实际的函数调用和执行是由你自己的代码负责的。
- **用例**：适用于需要模型与自定义逻辑、数据库或应用的本地函数交互的场景，例如查询内部数据、执行本地运算等。
- **实现方式**：模型只是根据 JSON Schema 提供参数，生成函数调用的细节。真正的执行权由你的应用来控制。

1. **GPT Actions**：

- **工具**：在 GPT Actions 中，"tools" 通常指的是公开的 API 接口或第三方服务。GPT Actions 会自动调用这些 API，并将结果反馈给用户。
- **用例**：适用于访问在线服务、查询外部系统数据或执行操作的场景，例如查询天气、调取金融数据、创建第三方系统中的记录等。
- **实现方式**：GPT Actions 会自动生成 API 请求的 JSON 参数并执行 API 调用，把复杂的 API 调用细节对用户隐藏。

  

**总结**

  

- **Function Calling** 适合调用**本地自定义函数**，用于访问内部逻辑或资源，**由应用负责实际执行**。
- **GPT Actions** 适合调用**外部公开的接口**，直接**执行并返回外部数据**，无需用户处理 API 细节。



[qt Agent/API 特性/GPT Actions.md]
================================

---
draw:
tags: []
title: GPT Actions
date created: 2024-11-15
date modified: 2024-12-27
---



[qt Agent/API 特性/Prompt Caching.md]
===================================

---
draw:
tags: []
title: Prompt Caching
date created: 2024-11-25
date modified: 2024-12-27
---

OpenAI的Prompt Caching功能是自动启用的,无需开发者手动开启。以下是关于OpenAI Prompt Caching的主要信息:

1. 自动应用:  
    Prompt Caching自动应用于最新版本的GPT-4o、GPT-4o mini、o1-preview和o1-mini模型,以及这些模型的微调版本[
    

2. 触发条件:

- 缓存自动应用于长度超过1024个token的提示[
    

- 缓存以128个token为增量进行,从1024个token开始[
    

    2

    

](https://platform.openai.com/docs/guides/prompt-caching).

1. 价格优势:

- 缓存的提示比未缓存的提示更便宜,可获得50%的折扣[
    

    1

    

](https://cookbook.openai.com/examples/prompt_caching101)[

    

    4

    

](https://www.53ai.com/news/finetuning/2024100330976.html).

- 例如,GPT-4o模型的原价为$2.50/1K tokens,缓存价格为$1.25/1K tokens[
    

    4

    

](https://www.53ai.com/news/finetuning/2024100330976.html).

1. 性能提升:

- 可以减少延迟高达80%[
    

    1

    

](https://cookbook.openai.com/examples/prompt_caching101).

- 对于长提示(超过10,000个token),延迟减少尤为显著[
    

    1

    

](https://cookbook.openai.com/examples/prompt_caching101).

1. 使用建议:

- 将静态内容(如指令和示例)放在提示的开头,将可变内容(如用户特定信息)放在末尾[
    

    2

    

](https://platform.openai.com/docs/guides/prompt-caching).

- 这种结构可以增加缓存命中率,从而提高效率[
    

    6

    

](https://www.53ai.com/news/finetuning/2024100945782.html).

1. 监控使用情况:

- API响应中的'usage'字段包含'cached_tokens'值,显示有多少token被缓存[
    

    5

    

](https://openai.com/index/api-prompt-caching/).

1. 缓存持续时间:

- 缓存通常在5-10分钟不活动后清除,最长保留1小时[
    

    4

    

](https://www.53ai.com/news/finetuning/2024100330976.html)[

    

    5

    

](https://openai.com/index/api-prompt-caching/).

总之,开发者无需采取任何特殊操作来启用Prompt Caching。只要使用支持的模型并发送超过1024个token的提示,系统就会自动应用缓存,从而降低成本并提高性能

入参是 项目 id，截止时间，返回是[{docName：，lastCommit：，lastDiff}]

```Java
入参：
 groupId：str
 queryTime：str

出参：
[
  {
    "docName": "文档名称",
    "lastCommit": "最后commit的文本",
    "lastDiff": "最后 commit 和前一次的 diff 文本"
  }，
  {
    "docName": "文档名称",
    "lastCommit": "最后commit的文本",
    "lastDiff": "最后 commit 和前一次的 diff 文本"
  }，
]



```



[qt Agent/API 特性/模型性能评估.md]
===========================

---
draw:
tags: []
title: 模型性能评估
date created: 2024-11-15
date modified: 2024-12-27
---
**模型性能评估**是一种测试和改进模型输出的方法，通过定期对模型的输出进行评估（也称为"evals"），可以确保其准确性和实用性。这是开发高质量、可靠的AI应用程序的核心步骤。

  

**评估流程**

  

1. **生成测试数据集**：为模型创建代表性测试数据，确保模型在接收到真实请求类型的数据时能够正确响应。测试数据集可以从实际生产请求中生成，这样可以让测试数据更贴近实际使用场景。
2. **存储实际请求生成的数据**：可以通过store: true参数存储实际请求的完成数据，结合metadata标记数据来源（如角色、部门等），便于后续过滤和分析。这样，后续评估时可以直接使用这些存储的完成数据。
3. **定义和运行评估**：一旦有了测试数据集，就可以为评估设定一系列标准（或称为"评分标准"），以此来判断模型的输出质量。评估标准包括多个选项，如模型评分器（Model Grader），可以根据实际需求自定义评估输出的方式。
4. **迭代改进**：评估后会在仪表板中显示得分。可以根据得分不断调整和优化提示词和评分标准，以提高模型的输出质量。

  

**细调（Fine-tuning）**

  

细调可以使模型的输出更符合特定的使用场景，通过引入定制数据和优化模型参数，使模型更好地理解特定任务的细微差异。例如，如果需要在IT支持场景中生成更加专业和准确的回答，可以通过细调数据帮助模型优化此类输出。

  

**模型蒸馏（Model Distillation）**

  

模型蒸馏是一种将大型模型的知识提取到更小、成本更低、响应更快的模型中的方法。蒸馏技术通过大模型生成的结果训练小模型，使得小模型在类似任务中仍然具有较好的效果。这对于降低部署成本、提高响应速度具有重要意义。

  

**优点**

  

1. **质量提升**：通过不断的评估和优化，可以确保模型对特定场景的适应性，从而提高输出的准确性和用户满意度。
2. **性能优化**：细调和模型蒸馏不仅能提高模型的响应速度，还能降低运行成本。
3. **迭代改进**：评估反馈和细调结合，使得可以持续优化模型以应对不断变化的需求。

  

**适用场景**

  

模型评估和优化流程适用于需要稳定高质量输出的应用，如客户服务、技术支持、问答系统等。通过此方法，可以确保模型始终能为不同类型的请求提供准确和有用的回答。



[qt Agent/API 特性/模型蒸馏.md]
=========================

---
draw:
tags: []
title: 模型蒸馏
date created: 2024-11-15
date modified: 2024-12-27
---

这篇文档介绍了模型蒸馏(Model Distillation)技术。模型蒸馏是一种通过大模型来改进小模型性能的方法。主要步骤包括：

1. 使用大模型生成高质量输出并存储
2. 评估大小模型的基准性能
3. 选择合适的数据来微调小模型
4. 评估微调后的模型效果

这种方法的主要优势是:

- 可以让小模型在特定任务上达到接近大模型的效果
- 降低成本和延迟
- 提高模型效率

具体实施时:

- 使用API的store参数存储大模型输出
- 用几百到几千个样本进行训练
- 整个微调过程约需15分钟
- 可以通过持续优化训练数据、提示词等来提升效果

这种技术特别适合需要在特定任务上获得高性能,同时又要考虑成本和效率的场景。

需要更多信息或具体实施细节的讲解吗？



[qt Agent/API 特性/预测输出.md]
=========================

---
draw:
tags: []
title: 预测输出
date created: 2024-11-15
date modified: 2024-12-27
---
**预测输出**是一种可以在已知大部分输出内容的情况下，加速模型响应的新方法。这种方法特别适用于当你只想对文本或代码进行小改动时，比如在重新生成一个代码文件时只做轻微调整。这时你可以通过**prediction 参数**向模型提供你对最终输出的预测，模型就会根据预测来生成结果，从而减少延迟。

  

**优点**

  

1. **加速响应**：因为模型会将预测的内容作为一部分输出，只需生成较少的新内容，从而缩短响应时间。
2. **适合大文件**：当对大文件进行小改动时，特别是代码或文档的轻微重构，这种方法可以显著降低延迟。
3. **流式处理**：在使用流式处理时，预测输出的延迟减少效果更明显，可以边生成边输出结果。
4. **灵活的预测位置**：预测文本可以出现在生成响应的任意位置，模型可以使用预测部分从而加快生成。

  

**缺点**

  

1. **可能增加成本**：预测的token中未被模型使用的部分仍然会计费。因此，如果模型拒绝了较多的预测内容，会带来更高的费用。
2. **功能限制**：在使用预测输出时，有以下功能不支持：

- n 参数不能超过1（不支持生成多项响应）。
- logprobs 不支持（无法返回生成概率）。
- presence_penalty 和 frequency_penalty 不支持大于0的值。
- 不支持音频输入和输出，仅适用于文本。
- 不支持 max_completion_tokens 参数，也不支持函数调用。

1. **适用模型有限**：仅支持 GPT-4o 和 GPT-4o-mini 系列模型，不适用于其他模型。

  

**适用场景**

  

预测输出主要用于文档或代码的细微改动，通过提前预测相似文本作为基础，可以让生成过程更快、更流畅。



[qt Agent/开发框架/LlamaIndex.md]
=============================

---
draw:
tags: []
title: LlamaIndex
date created: 2024-11-19
date modified: 2024-12-27
---



[qt Agent/开发框架/langchain.md]
============================

---
draw:
tags: []
title: langchain
date created: 2024-11-15
date modified: 2024-12-27
---

垃圾框架

底层就一堆提示词，搞了这么复杂的面向对象，而且版本之间还不兼容



[qt Agent/意图识别.md]
==================

---
draw:
tags: []
title: 意图识别
date created: 2024-11-09
date modified: 2024-12-27
---
- 从项目视角拆解题目，开发独立 IM 系统，打造友好的 AI 交互方式。
- 整合项目群聊与文档信息，通过知识库问答、语义搜索、催办待办、项目进度总结等方式，全方位挖掘项目生命周期中的各种信息价值。
- 追踪社区最新实践，使用 OpenAI、Anthropic、Cerebras 等原生 API 的新特性
- Agent 智能交互解决日常工作中的繁琐操作
- 开发过程从 UI设计、前端、后端、PPT 制作，50% 以上由 AI 助力完成。

## 一、交互

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F11%2F10%2F05-05-07-73d413ef83d74ff79e66d96a48d64f92-202411100505954-148d67.png)

1. 输入框输入@Agent 之后的信息触发后端意图识别接口
2. 提取出来的意图显示在图中卡片上，之后槽位信息可以加到卡片上，进一步优化
3. 用了前端 Debounce (防抖)，用户停止输入 800ms 后才调用 API，800ms 可以调整

## 二、整体架构（降级处理还待优化）

```mermaid

flowchart TD

Input[用户输入] --> RuleEngine[规则引擎]

Input --> LLM[大语言模型]

subgraph RuleEngine[规则引擎层]

R1[正则表达式匹配] --> R2[模板解析]

R2 --> R3[规则评分]

end

subgraph LLM[大语言模型层]

L1[意图分类] --> L2[槽位提取]

L2 --> L3[置信度评估]

end




LLM --> Arbitrator

Arbitrator --> Output[最终结果]

subgraph Fallback[降级处理]

F1[规则回退]

F2[模型回退]

end

Arbitrator -.-> Fallback

Fallback -.-> Output

```

## 三、实现细节（缓存、错误重试还未实现）

```mermaid
flowchart TD
    Input[用户输入] --> Cache{缓存匹配}
    
    Cache -->|命中| CacheResult[返回缓存结果]
    Cache -->|未命中| Parallel[并行处理]
    
    subgraph Parallel[并行处理层]
        direction TB
        Rule[规则引擎处理] --> RuleMatch{规则匹配}
        LLM[LLM处理] --> LLMProcess{LLM解析}
    end
    
    subgraph RuleEngine[规则引擎层]
        direction TB
        R1[正则匹配] --> R2[模板解析]
        R2 --> R3[槽位提取]
    end
    
    subgraph LLMProcess[LLM处理层]
        direction TB
        L1[Prompt构造] --> L2[API调用]
        L2 --> L3[结果解析]
    end
    
    Rule --> RuleEngine
    LLM --> LLMProcess
    
    RuleMatch -->|匹配成功| RuleResult[规则结果]
    RuleMatch -->|匹配失败| WaitLLM[等待LLM结果]
    
    LLMProcess -->|处理成功| LLMResult[LLM结果]
    LLMProcess -->|超时/错误| Fallback[降级处理]
    
    RuleResult --> ResultProcess[结果处理]
    LLMResult --> ResultProcess
    
    ResultProcess --> Cache2[更新缓存]
    ResultProcess --> Output[返回结果]
    
    Fallback --> Output
    
    CacheResult --> Output
    
    subgraph Optimization[优化策略]
        direction TB
        O1[请求去重]
        O2[批量处理]
        O3[超时控制]
        O4[错误重试]
    end
    
    Optimization -.-> Parallel
```

## 四、场景分析

### 1. 语义搜索

### 2. 总结内容 or 形成结论

槽位里最关键的是起止时间

我可以帮你将这些消息按主题和事件进行分类。我会分析消息内容、时间连续性以及参与者之间的对话来进行分组。让我们把它分成几个主要的话题群:

1. 需求开发初期讨论与工时评估 (9月24-25日)
- 主要参与者:徐赟哲、崔宇、赵赵力等
- 主要内容:讨论工时评估、接口定义、技术方案等

1. 项目联调与问题修复 (10月8-11日)
- 主要参与者:赵赵力、乔路、董威宁、王颖yw等
- 内容:
    - 接口联调
    - bug修复
    - 配置调整
    - 环境问题处理

1. 项目发布与上线 (10月18日)
- 主要参与者:王颖yw、赵赵力、董威宁、刘丽慧等
- 内容:
    - 发布步骤确认
    - 配置检查与修改
    - 各模块依次发布
    - 发布验证

1. 线上问题跟进与优化 (10月21-29日)
- 主要参与者:刘丽慧、赵赵力、蔡创基等
- 内容:
    - 曝光率监控
    - 埋点问题
    - 用户反馈问题处理
    - 性能优化

这种分类可以帮助我们更好地理解整个项目的进展过程,以及在不同阶段遇到的主要问题。每个子群都有其相对独立但又相互关联的主题。你觉得这样的分类方式合理吗？如果需要,我可以针对某个子群做更详细的分析。

### 3. 进入发布模式

`QA 触发整个群进入发布模式`

```Java
"【FD-286154】 项目发布中
发布类型：prod
开始时间：2024-10-18 14:49:17
appcode：f_athena_order..."

"@徐赟哲  【即将发布周知】【FD-285952订单详情页二次售卖改版】 将于（2024-10-22）发布上线..."

"【发布delay】【FD-286155 订单详情页二次售卖改版】 未按计划发布时间（2024-10-11）发布..."
```

1. 测试验证相关:

```Java
"发布拦截了 快速点下哈"

"好的，刚才那个问题他们看下，应该不是本次改动的"

"我这边验证好了哈"
```

1. 配置管理相关:

```Java
"@王颖yw 审核一下配置，加了一下新的模块"

"这个标签是根据什么维度的"

"谨慎操作qconfig"
```

1. 问题排查相关:

```Java
"[obj type="image" value="https://qtalk.qunarzz.com/pp_qtalk_file_0003/f714c57c1cce3f27031df0935d477707.jpg" width=1236 height=628]@蔡创基 看下异常？"

"CompensateSecondSaleService_getCompensateInfo_param_error_Count]近期有值,但未添加报警..."

"qmall有npe我先回滚了"
```

### 4. 生成项目报告

1. 对于已完成的项目，提取项目过程流水，生成项目过程报告 `响应式生成` or `意图识别`
    1. 项目概览：项目名称、项目时长、参与团队、参与人、提出问题数、解决问题数、有效沟通次数
    2. 项目流水：人、时间、动作、结果
    3. 过程问题：问题描述、提出人、解决人、解决状态、解决时长
    4. 项目关键结论：项目群中的关键结论

### 5. 生成每周回顾

基于文档的 commit 的 diff 信息

### 6. 生成待办、催办

过去的群只有两个级别的信息 艾特我的 和不艾特我的 现在我们要借助ai做成三个  
分个人维度和多人维度甚至群维度

1. 主动 push 别人，`意图识别，强提醒的功能，如果做成响应式，可能会泛滥，所以需要人为触发并点击确认`
2. 主动查看自己，`响应式`or`意图识别` 每个人可以触发找出可能和自己相关的事情

```Java
- "@all 今天开发人员大家各自子啊群里面报一下自己的进度"
- "目前卡在支付前校验，待支付中心联调"
- "售前主流程/交易/辅营 售前阻塞在支付前校验，待支付中心联调,目前售前剩余授权回调验证 还得1pd调通"

"董威宁 邀请您参加腾讯会议
会议主题：董威宁的快速会议..."

"@乔路 能给我清个缓存不"

"@王颖yw 审批的时候也注意下哈"
```

1. 技术支持相关:

```Java
"@赵赵力 检查一下配置吧"

"@伊海迪 可以关注下这个问题"

"@国内交易热线 热线帮忙看下这个接口..."
```

1. 环境配置与问题排查

```Java
- "环境没搞通" "还没听人说 环境搞好了"
- "org.apache.dubbo.rpc.RpcException: No provider available..."
- "软路由 能不能进服务页，现在没法进 case没法覆盖到"
- "[obj type="url" value="http://portal.corp.qunar.com/servicePortal/apptask/console.html?id=9302554"] trade_core 重启下"
```

1. 联调与测试协调

```Java
- "@蔡宜身 现在进度如何呀，今天提测有风险吗？晚上能showcase吗？"
- "现在目前是各个环节都有卡点，先是北京-上海营销不出，现在是航班都没了"
- "@黄启孟 帮看下支付完成页的几个问题..."
```

1. 技术方案讨论

```Java
- "@赵述佳 这个cashierJumpUrl构建的时候 有判断payToken为空的时候不返这个url吗？"
- "@蔡宜身 轮询接口 报了参数异常"
- "batchSeq为什么是yyw格式的啊，不是20格式的啊，合单情况下又传的是什么呢"
```

1. 任务分配与协作

```Java
- "@李皓雪 皓雪姐看一下你那边的排期"
- "@all 任命陈民华担任项目技术负责人，请知晓！"
- "@黄启孟 @郝泽楠 @李皓雪 @谭鹏yy @何钢 @肖杨 @刁洪亮 来对一下排期吧"
```

1. 文档与知识共享

```Java
- "[obj type="url" value="https://wiki.corp.qunar.com/pages/viewpage.action?pageId=658756458"]"
- "按照这个更新checklist吧"
- "@赵述佳 技术方案的wiki在哪有"
```

## 五、一句话多个意图（待实现）

一句话有多个意图怎么办呢？将 suggestion 卡片做成跑马图，或者干脆不实现多意图？

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F11%2F10%2F04-52-22-c78b4b3a020bee41f137d761bd9bb436-202411100452153-5c1378.png)

## 六、避免幻觉

**Best-of-N verficiation**: Run Claude through the same prompt multiple times and compare the outputs. Inconsistencies across outputs could indicate hallucinations.  
并发调用多次，比较结果



[英语/++如何畅游英语 youtube/如何畅游英语油管$.md]
==================================

---
draw:
tags: []
title: 如何畅游英语油管$
date created: 2024-08-11
date modified: 2024-12-27
---

## [第一性原理](第一性原理.md)

本质是为了跳过语音的阻碍，先感受信息本身的优越性，反过来激发持续学习语言的热情。

## 思路

将感兴趣的内容做成[NotebookLM](NotebookLM.md)，然后生成[播客](播客.md)，然后在[阿里听悟](阿里听悟.md)做精听，但是也不用很精，基本一整个句子一次暂停，力求能明白是什么意思。

## [SMART 指标](SMART%20指标.md)



[英语/++如何畅游英语 youtube/词汇过滤.md]
=============================

---
draw:
tags: []
title: 词汇过滤
date created: 2024-08-22
date modified: 2024-12-27
---

用`/Users/liuyishou/usr/projects/viva-backend/util/word_filter.py`过滤出需要注意的词汇



[英语/2024 Q3：切入reddit/从 workout 切入 quora 和 reddit $.md]
======================================================

---
draw:
tags: []
title: 从 workout 切入 quora 和 reddit $
date created: 2024-07-17
date modified: 2025-01-20
---

## why the project

[Quora](Quora.md) [reddit](reddit.md) [Hacknews](Hacknews) 都是优质的信息来源，但是之前并没有养成这些平台的阅读习惯。本质是自身兴趣没有克服英文阅读带来的摩擦力。

最近开始 [健身@](健身@.md)，一路比照了不少信息源和知识，体验了一下，几乎可以带着好奇心客服摩擦力了。希望借着这股东风，培养起在这两个平台的一个良好的阅读习惯。

## [SMART 指标](SMART%20指标.md)

100 篇 reddit 文章

### 前 50 是只看英语，熟悉一些话题和板块

| count | 链接                                                                                                                                                                                                                                                                                        |                     |
| ----- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| 1     | [Beginner's Guide to Working Out : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/ihz264/beginners_guide_to_working_out/)                                                                                                                                              |                     |
| 2     | [Tried flirting with a guy at the gym and it failed : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/1agj9ft/tried_flirting_with_a_guy_at_the_gym_and_it_failed/)                                                                                                      |                     |
| 3     | [My worst fears happened, someone laughed at me in the free weights of the gym : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/1adunlg/my_worst_fears_happened_someone_laughed_at_me_in/)                                                                             |                     |
| 4     | [My boyfriend doesn’t want me to go to the gym : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/1cg97e0/my_boyfriend_doesnt_want_me_to_go_to_the_gym/)                                                                                                                 |                     |
| 5     | https://www.reddit.com/r/workout/comments/1ad4t9q/might_be_addicted_to_the_gym_now/                                                                                                                                                                                                       |                     |
| 6     | [So question, why do you guys work out? : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/17etwmk/so_question_why_do_you_guys_work_out/)                                                                                                                                |                     |
| 7     | [Any good work outs for carrying your gf? : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/1dqis9r/any_good_work_outs_for_carrying_your_gf/)                                                                                                                           |                     |
| 8     |                                                                                                                                                                                                                                                                                           |                     |
| 9     |                                                                                                                                                                                                                                                                                           |                     |
| 10    |                                                                                                                                                                                                                                                                                           |                     |
| 11    |                                                                                                                                                                                                                                                                                           |                     |
| 12    |                                                                                                                                                                                                                                                                                           |                     |
| 13    | What is the best food to eat before and after going to the gym?                                                                                                                                                                                                                           |                     |
| 14    | [Are modern humans stronger and more fit than our ancestors?](https://www.quora.com/Are-modern-humans-stronger-and-more-fit-than-our-ancestors)                                                                                                                                           |                     |
| 15    | [What person enjoyed longevity they were totally undeserving of?](https://www.quora.com/What-person-enjoyed-longevity-they-were-totally-undeserving-of)                                                                                                                                   |                     |
| 16    | What is the most deceiving thing about fitness? How simple it is                                                                                                                                                                                                                          |                     |
| 17    | [Anyone else incredibly disappointed with the Meta Glasses : r/RayBanStories (reddit.com)](https://www.reddit.com/r/RayBanStories/comments/1aez7ko/anyone_else_incredibly_disappointed_with_the_meta/)                                                                                    |                     |
| 18    | https://www.quora.com/What-are-the-negative-side-effects-of-taking-glutamine                                                                                                                                                                                                              |                     |
| 19    | Any good workout blend should give                                                                                                                                                                                                                                                        |                     |
| 20    | [BEHOLD!! My Second Brain : r/ObsidianMD (reddit.com)](https://www.reddit.com/r/ObsidianMD/comments/182ij1c/behold_my_second_brain/)                                                                                                                                                      |                     |
| 21    | [4 year aniversary of my vault 🎉 : r/ObsidianMD (reddit.com)](https://www.reddit.com/r/ObsidianMD/comments/1dtvhtp/4_year_aniversary_of_my_vault/)                                                                                                                                       |                     |
| 22    | [Tiny ascii charts - worth a plug-in? : r/ObsidianMD (reddit.com)](https://www.reddit.com/r/ObsidianMD/comments/1cagaoh/tiny_ascii_charts_worth_a_plugin/)                                                                                                                                |                     |
| 23    | [Let That Sink In. : r/StockMarket (reddit.com)](https://www.reddit.com/r/StockMarket/comments/1du0to5/let_that_sink_in/)                                                                                                                                                                 |                     |
| 24    | [Any good work outs for carrying your gf? : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/1dqis9r/any_good_work_outs_for_carrying_your_gf/)                                                                                                                           |                     |
| 25    | [7 years from $0 to $100k, 3 years from $100k to $200k : r/investing (reddit.com)](https://www.reddit.com/r/investing/comments/17yaktc/7_years_from_0_to_100k_3_years_from_100k_to_200k/)                                                                                                 |                     |
| 26    | [I need help breaking up with my gym partner : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/16ok4jb/i_need_help_breaking_up_with_my_gym_partner/)                                                                                                                    |                     |
| 27    | [What keeps you motivated to keep working out? : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/1di52j4/what_keeps_you_motivated_to_keep_working_out/)                                                                                                                 |                     |
| 28    | [No More Daily "my graph after X" Posts 🙏 : r/ObsidianMD (reddit.com)](https://www.reddit.com/r/ObsidianMD/comments/1esicj8/no_more_daily_my_graph_after_x_posts/)                                                                                                                       |                     |
| 29    | [A Free & Open Source "Obsidian Publish" Alternative : r/ObsidianMD (reddit.com)](https://www.reddit.com/r/ObsidianMD/comments/u7r2a5/a_free_open_source_obsidian_publish_alternative/)                                                                                                   |                     |
| 30    | [I'm joining Obsidian full-time as CEO : r/ObsidianMD (reddit.com)](https://www.reddit.com/r/ObsidianMD/comments/10vcxb2/im_joining_obsidian_fulltime_as_ceo/)                                                                                                                            |                     |
| 31    | [Starbucks is giving incoming CEO Brian Niccol $85 million in cash and stock as he departs Chipotle : r/StockMarket (reddit.com)](https://www.reddit.com/r/StockMarket/comments/1et0w5w/starbucks_is_giving_incoming_ceo_brian_niccol_85/)                                                |                     |
| 32    | [IBKR, DeGiro, Saxo Bank or other for next broker : r/StockMarket (reddit.com)](https://www.reddit.com/r/StockMarket/comments/mk13v7/ibkr_degiro_saxo_bank_or_other_for_next_broker/)                                                                                                     |                     |
| 33    | [Can't get failure on back muscles : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/1ew21gv/cant_get_failure_on_back_muscles/)                                                                                                                                         |                     |
| 34    | [Obsidian (reddit.com)](https://www.reddit.com/r/ObsidianMD/comments/1evdd5j/are_there_real_people_using_obsidian_or_just/)                                                                                                                                                               |                     |
| 35    | [Obsidian for Musicians - Knowledge management - Obsidian Forum](https://forum.obsidian.md/t/obsidian-for-musicians/48520/6)                                                                                                                                                              |                     |
| 36    | [Where's everyone parking cash right now? : r/investing (reddit.com)](https://www.reddit.com/r/investing/comments/1eyq45k/wheres_everyone_parking_cash_right_now/)                                                                                                                        |                     |
| 37    | [Obsidian LaTeX integration? : r/ObsidianMD (reddit.com)](https://www.reddit.com/r/ObsidianMD/comments/tyini7/obsidian_latex_integration/)                                                                                                                                                |                     |
| 38    | [Bad sleep : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/1f7t1vb/bad_sleep/)                                                                                                                                                                                        | 睡眠对健身的作用            |
| 39    | [Is push pull legs split really effective ? : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/1f7jer2/is_push_pull_legs_split_really_effective/)                                                                                                                        | Push Pull Leg 的一些迷死 |
| 40    | [Is investing 90% or your money a bad idea? : r/investing (reddit.com)](https://www.reddit.com/r/investing/comments/1f77udu/is_investing_90_or_your_money_a_bad_idea/)                                                                                                                    | 22 岁是否该花这么多钱投资      |
| 41    | [Best broker for US cost-conscious investors? : r/investing (reddit.com)](https://www.reddit.com/r/investing/comments/1f6v907/best_broker_for_us_costconscious_investors/)                                                                                                                | 不同 broker 的费用       |
| 42    | [What's the catch on Berkshire Hathaway? : r/investing (reddit.com)](https://www.reddit.com/r/investing/comments/1f6c823/whats_the_catch_on_berkshire_hathaway/)                                                                                                                          | Buffet 的公司的表现       |
| 43    | [Need help with my parents cash : r/investing (reddit.com)](https://www.reddit.com/r/investing/comments/1f6utlm/need_help_with_my_parents_cash/)                                                                                                                                          | 父母的钱                |
| 44    | [Is working out meaningless for building muscle if I don't sleep enough? : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/1ctvmgu/is_working_out_meaningless_for_building_muscle_if/)                                                                                  | 健康与睡眠               |
| 45    | [Yesterday i drank pre workout for the first time and instead give me a ton of energy it made feel sleepy and lose focus. What can be the reason behind it? : r/workout (reddit.com)](https://www.reddit.com/r/workout/comments/omq3ut/yesterday_i_drank_pre_workout_for_the_first_time/) |                     |
| 46    | [更智能地编写代码，而不是更难编写代码：使用 Cursor 和 Claude Sonnet 进行开发 \|黑客新闻 --- Code Smarter, Not Harder: Developing with Cursor and Claude Sonnet \| Hacker News (ycombinator.com)](https://news.ycombinator.com/item?id=41429469)                                                                         | cursor              |
| 47    | [如何实现像 Notion 一样的第二大脑功能？ : r/ObsidianMD --- How to achieve second brain functionality like Notion? : r/ObsidianMD (reddit.com)](https://www.reddit.com/r/ObsidianMD/comments/1fbtdoq/how_to_achieve_second_brain_functionality_like/)                                                     |                     |
| 48    |                                                                                                                                                                                                                                                                                           |                     |
| 49    |                                                                                                                                                                                                                                                                                           |                     |
| 50    |                                                                                                                                                                                                                                                                                           |                     |

### 后 50 篇利用[Immersive Translate](Immersive%20Translate.md)工具

好的,我来帮您把序号补充到100。以下是补充后的列表:

|     |                                                                                                                                                                                |                                    |
| --- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------- |
| 51  | [手工创建 Git 提交 --- Creating a Git commit: The Hard Way (avestura.dev)](https://avestura.dev/blog/creating-a-git-commit-the-hard-way)                                             | 突然觉得 hacknews 也没有那么 daunting 了     |
| 52  |                                                                                                                                                                                |                                    |
| 53  |                                                                                                                                                                                |                                    |
| 54  |                                                                                                                                                                                |                                    |
| 55  |                                                                                                                                                                                |                                    |
| 56  |                                                                                                                                                                                |                                    |
| 57  |                                                                                                                                                                                |                                    |
| 58  |                                                                                                                                                                                |                                    |
| 59  |                                                                                                                                                                                |                                    |
| 60  |                                                                                                                                                                                |                                    |
| 61  |                                                                                                                                                                                |                                    |
| 62  |                                                                                                                                                                                |                                    |
| 63  |                                                                                                                                                                                |                                    |
| 64  |                                                                                                                                                                                |                                    |
| 65  |                                                                                                                                                                                |                                    |
| 66  |                                                                                                                                                                                |                                    |
| 67  |                                                                                                                                                                                |                                    |
| 68  |                                                                                                                                                                                |                                    |
| 69  |                                                                                                                                                                                |                                    |
| 70  | 20141010 到这里                                                                                                                                                                   |                                    |
| 71  | # Here's the ASML leak for anyone wondering                                                                                                                                    |                                    |
| 72  | https://www.reddit.com/r/apple/comments/1g47l2h/40_yearold_apple_mac_finally_gets_online_thanks/                                                                               |                                    |
| 73  | https://www.reddit.com/r/workout/comments/1g5026q/can_someone_rate_my_workout_plan_please/                                                                                     | 看了一套推拉腿上下的五分训练计划                   |
| 74  | https://www.reddit.com/r/ClaudeAI/comments/1g4qdxq/i_apologize_but_i_dont_feel_comfortable_posting/                                                                            |                                    |
| 75  | https://www.reddit.com/r/technology/comments/1g502j4/federal_trade_commission_announces_final/                                                                                 |                                    |
| 76  | https://www.reddit.com/r/ClaudeAI/comments/1g4veyo/we_need_chat_history_forking_and_cherry_picking/                                                                            | 用 git、cherry pick 的思路来看待与 ai 的聊天记录 |
| 77  | https://www.reddit.com/r/cursor/comments/1g502cy/is_it_me_or_cursor_autocomplete_got_a_lot_better/                                                                             |                                    |
| 78  | https://www.reddit.com/r/apple/comments/1g4y0dp/ipad_mini_7_has_these_6_smaller_changes_including/                                                                             | 新 ipad mini                        |
| 79  | [使用 100%鼠标后的体验:r/cursor --- Experiences after going 100% Cursor : r/cursor (reddit.com)](https://www.reddit.com/r/cursor/comments/1g60bkz/experiences_after_going_100_cursor/) |                                    |
| 80  |                                                                                                                                                                                | Rest is a part of working out      |
| 81  |                                                                                                                                                                                |                                    |
| 82  |                                                                                                                                                                                |                                    |
| 83  |                                                                                                                                                                                |                                    |
| 84  |                                                                                                                                                                                |                                    |
| 85  |                                                                                                                                                                                |                                    |
| 86  |                                                                                                                                                                                |                                    |
| 87  |                                                                                                                                                                                |                                    |
| 88  |                                                                                                                                                                                |                                    |
| 89  |                                                                                                                                                                                |                                    |
| 90  |                                                                                                                                                                                |                                    |
| 91  |                                                                                                                                                                                |                                    |
| 92  |                                                                                                                                                                                |                                    |
| 93  |                                                                                                                                                                                |                                    |
| 94  |                                                                                                                                                                                |                                    |
| 95  |                                                                                                                                                                                |                                    |
| 96  |                                                                                                                                                                                |                                    |
| 97  |                                                                                                                                                                                |                                    |
| 98  |                                                                                                                                                                                |                                    |
| 99  |                                                                                                                                                                                |                                    |
| 100 |                                                                                                                                                                                |                                    |



[英语/2024 Q3：切入reddit/结项报告.md]
=============================

---
draw:
tags: []
title: 结项报告
date created: 2024-10-23
date modified: 2024-11-12
---

## 结论

1. 通过[Immersive Translate](Immersive%20Translate.md)工具，实现了低摩擦力的英文信息获取。
2. 通过 twitter 上的中文 KOL 和 grok 激发兴趣，逐步融入了 twitter 社区

原计划 100 篇，由于方向发生变化（纯英文 --> 沉浸式翻译），实际完成 80 篇



[英语/Q3：托福考到 100/++成为兼职托福老师/如何成为兼职托福老师.md]
=========================================

---
draw:
tags: []
title: 如何成为兼职托福老师
date created: 2024-07-02
date modified: 2024-12-27
---

## [第一性原理](第一性原理.md)

教哪个科目？
胜任标准：小项达到 28 分及以上  
难度排名：口语>听力>写作>阅读  
我选择听力，难度比口语低，且对自己本身英语能力来说，收益最大

```mermaid
graph TD
	A[100] --> B[listening]
```

> 现在课程基本集中一周2次，减少了1对1，一次收费，班课挣得更多，对身体消耗也小。
> 🌸正常上课  
这是我的主业，也是独立英语老师安身立命的根本，半年来从一开始抓瞎天天备课上课忙要死，现在课程基本集中一周2次，减少了1对1，一次收费，班课挣得更多，对身体消耗也小。
🌺开个英语app sell账号资源  
这个算次业，顺带的事，平时整理的教材、课程、比如分级绘本、桥梁书，启蒙规划资源、儿歌动画视频等等，上传到app里，满足很多家长的需求，帐号也很受欢迎，毕竟里面很多资源的。
💐接广  
这个是针对有做小红书啊等自媒体账号的，我的号不在小红书，平时坚持日更两年多了，累计有一两万粉，每月接两三次广，一次1000-2000不等，其他老师注意‼不能接跟自己有竞争关系的，纯纯搬石头砸自己的脚哈  
🎋英语课程分销  
也是顺带的事，不囤货，只转卖，有很多老师应该都会做这块，拿佣💰就行，有的老师更厉害，不只做英语课程，还会有学习用品硬件之类的。
🌳做打卡营  
这个我是寒假接触到才知道很火🔥，前期需要先准备内容，整体规划，跑通一次后期基本就是复制前期模式了，比如我入坑就是寒假21天阅读营，把每阶段内容整理好，任务规划到每一天，然后提供每日同步监督指导，后来我都用app工具更快些，客单价更高些，还能把家长们留下来慢慢转化为班课的。
挣钱真的不是靠时间熬的，靠的是抓住机会，很多时候没办法全准备好，机会不等人的，都是遇到了立马开始做，边走边学，路子通了，钱也就来了。



[英语/Q3：托福考到 100/bionic reading.md]
==================================

---
draw:
tags: []
title: bionic reading
date created: 2024-07-11
date modified: 2024-11-12
---

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F07%2F17%2F14-29-01-13d336f8510dc31118af625274a6e218-20240717142900-8e5f34.png)

bionic reading 是一种阅读技术,旨在提高阅读速度和理解效率。它的主要特点是:

1. 突出单词的关键部分,通常是单词的前半部分或前几个字母。
2. 这些突出的部分通常以加粗或不同颜色显示。
3. 理论基础是人脑可以通过识别单词的关键部分快速理解整个单词。
4. 目的是减少眼睛在文本上的停留时间,加快阅读速度。
5. 声称可以提高阅读理解力和记忆力。

这种方法试图模仿人脑的自然阅读过程,因此被称为 " 仿生阅读 "。



[英语/Q3：托福考到 100/托福和小红书结合的计划$.md]
================================

---
draw:
tags: []
title: 托福和小红书结合的计划$
date created: 2024-09-10
date modified: 2024-11-12
---

追踪考情，发布小红书，学习图片制作和小红书运营的套路。

| 考试日期        |     |
| ----------- | --- |
| 2024年9月1日   |     |
| 2024年9月4日   |     |
| 2024年9月7日   |     |
| 2024年9月8日   |     |
| 2024年9月14日  |     |
| 2024年9月15日  |     |
| 2024年9月18日  |     |
| 2024年9月21日  |     |
| 2024年9月22日  |     |
| 2024年10月12日 |     |
| 2024年10月16日 |     |
| 2024年10月19日 |     |
| 2024年10月26日 |     |
| 2024年10月27日 |     |
| 2024年10月30日 |     |
| 2024年11月2日  |     |
| 2024年11月3日  |     |
| 2024年11月9日  |     |
| 2024年11月13日 |     |
| 2024年11月17日 |     |
| 2024年11月24日 |     |
| 2024年12月1日  |     |
| 2024年12月8日  |     |
| 2024年12月14日 |     |
| 2024年12月18日 |     |
| 2024年12月28日 |     |



[英语/Q3：托福考到 100/托福考到100$.md]
============================

---
draw: 
title: 托福考到100$
date created: 2024-04-14
date modified: 2025-02-14
---

## 一、参考资料

1. 自学托福能够达到 110 吗？- 成语桑的回答 - 知乎 https://www.zhihu.com/question/312907537/answer/796589149
2. 和一位托福老师的付费咨询

## 二、大数据统计

[Test and Score Data Summary for the TOEFL iBT ® Tests (ets.org)](https://www.ets.org/pdfs/toefl/toefl-ibt-test-score-data-summary-2023.pdf)

百分位排名，xx 分能干掉百分之 xx 的人  
![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F04%2F11-54-06-195d260f7d5f95dfabd07cd30bb83e25-202409041154451-42fbc2.png)

## 三、我的尝试

|             | Reading 28 | Listening 25 | Speaking 19 | Writing 20 | 感受                           |
| ----------- | ---------- | ------------ | ----------- | ---------- | ---------------------------- |
| best scores |            |              |             |            |                              |
| 20240413    | 24         | 18           | 19          | 19         | 第一次考，没有熟悉题型，不过感觉还不错，起码有指标反馈了 |
| 20240626    | 24         | 25           | 16          | 20         | 听力很亮眼，口语似乎有一道重大失误            |
| 20240904    | 28         | 20           |             |            | 阅读很亮眼，下一步可以集中精力研究口语和写作了      |

### 20240413

- [X] 阅读最后两个没来得及写，24 分，写了的话可能 27-28
- [X] 听力前几篇以及感觉听的差不多了，18 分，欠缺了一些运气，多考几次可能有较大浮动。听力刺激需要加大。写代码反转目前主动词汇的卡片变成被动听力型的。
- [ ] 口语有一道题试图解释动因，而造成两个现象表达的太少。题目是让我介绍两种现象的。
	- [ ] 本质是没有把模块落到秒数里
- [ ] 写作还是 however 和 but，之后可以列一些一定要使用的逻辑词。

### 20240626

- [X] 阅读 24，多做了三个题，但是结果并不理想，本质还是阅读速度太慢，词汇量应该问题不大。速度！速度！速度！把注意力放在谓语上，对提速有帮助
- [x] 听力=硬实力 + 层次的把握、标志词的捕捉
- [ ] [托福口语](托福口语.md) 和写作信息的全面和准确
- [X] 这段时间试试 [bionic reading](bionic%20reading.md)，
- [X] 把读 [知乎](知乎.md) 的时间转到[从 workout 切入 quora 和 reddit $](从%20workout%20切入%20quora%20和%20reddit%20$.md)，提高自己的英语阅读速度。
  - [ ] 目标 100 篇，进度百分之五十。

Markdown Preview Enhanced>[!最佳实践]

> 0807 结论：口语没必要再一对一练了，，只专注托福口语应试即可Markdown Preview Enhanced

### 20240904

- [ ] 下个阶段的主题应该是[托福写作$](托福写作$.md) 和 [托福口语](托福口语.md) 了
  - [ ] 口语要逐个题型像作文一样固化一下节奏和模板，练习点：
    - [ ] 从容、通过模版适应各题型节奏，关键要做到发音饱满
    - [ ] 信息的提炼，不用多复杂但是完整和正确。有个迷思点可能是自己觉得自己练习的话没有反馈，但是其实用很低的语法要求不遗漏信息当做反馈就可以了。
  - [ ] 作文比较稳定了可以强化一下第一篇的模版，再明确一下强逻辑、强实在的论证方式
- [ ] 听力确实需要一些运气，可以继续过单词，但是要粗一些，暴露面大一些，广度优先
- [X] 阅读这次不错速度和质量都不错

> [!最佳实践]  
> 对于所有考托福的中国考生，阅读正常强化即可；听力是个完全新的东西，是其他三门的基础，需要硬积累，硬实力；口语和作文是有些技巧性的，达到一个最理想的边际收益即可 23+27

## 总结

在这篇文档中，我记录了自己在托福考试中的一些尝试和经验教训。通过对不同考试日期的成绩和表现进行分析，我发现了一些需要改进的地方：

1. 阅读部分：虽然词汇量问题不大，但阅读速度仍需提高。通过关注谓语部分，可以有效提升阅读速度。
2. 听力部分：听力成绩波动较大，需要加强硬实力的积累和层次的把握。标志词的捕捉也很重要。
3. 口语部分：需要固化各题型的节奏和模板，做到发音饱满，信息提炼完整且正确。
4. 写作部分：写作相对稳定，但仍需强化第一篇的模板，并明确强逻辑、强实在的论证方式。

总体来说，阅读和听力是基础，需要更多的积累和练习；口语和写作则可以通过技巧性的训练达到理想的效果。未来的备考重点将放在托福写作和口语的提升上，同时继续保持阅读和听力的练习。



[英语/Q3：托福考到 100/旅行.md]
======================

---
draw:
tags: []
title: 旅行
date created: 2024-08-07
date modified: 2024-11-12
---

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F08%2F07%2F15-17-47-7ca408c22fa0bc8b5c068c8f2514e8d2-202408071517546-a81863.png)



[英语/Q3：托福考到 100/职业规划.md]
========================

---
draw:
tags: []
title: 职业规划
date created: 2024-06-29
date modified: 2024-11-12
---

24 18 20 19  
25 24 xx xx

## 托福兼职的要求是什么？

100 底线，110 最好，单项 28

## 去教哪一科

性价比最高的是阅读。口语单项需求最高。教的科目要达到 28、29，只能教听力或者阅读了

## 如何到 100 分、110 分

25+25+23+27  
30+29+23+28  
口语封顶 23/24，20 到 23 的差距在细节完整度和连贯性，本质还是听力能力  
写作 27/28 很容易，需要有英语逻辑，即实质性的东西，有力的。语法现象没那么重要。不要感叹升华啥的。

## 去机构还是工作室

新东方、钱不多但是有背书，对于以后做留学有帮助

## 如何派活

兼职的话，机构会 match 你的时间

## 新东方兼职工资

100 分水平 - 180  
110 分水平 - 250 300 400

## 工作室全职工资

淡季一两万，旺季三四万

## 怎么教别人听力

帮忙监督，拆碎。本质是给出方法论，并且带着学生熟悉和应用这套方法论。

## 真正的英语能力

主动词汇：实打实的外国朋友、环境  
被动词汇：听力阅读 25 的基础后，可以精细到一个具体的领域去提高了，可以 6.824

## 你的规划

香港读研 -> 国际学校 -> 自己的工作室



[英语/口语 review/Untitled.md]
==========================

---
aliases: [总结与建议]
draw:
tags: []
title: 总结与建议
linter-yaml-title-alias: 总结与建议
date created: 2025-02-01
date modified: 2025-02-01
---

下面是一份针对「发言人2」在整段对话中出现的 **英文口语部分** 的逐句复盘（review）。受篇幅所限，我尽量将每句的原文、可能表达的中文含义，以及更为准确或自然的英文表达方式呈现出来，供您对照学习。格式如下：

> 1. **原文 (Speaker 2)**
> 2. **中文意**（我推测的想表达的含义）
> 3. **改进后英文表达**（更为准确、自然或流畅的版本）

---

## 4.「Uh, how many questions in the uh, uh ask about the oral skills?」

1. **原文**

    > Uh, how many questions in the uh, uh ask about the oral skills?

2. **中文意**

    > 额，口语部分有多少道题要回答呢？

3. **改进后英文表达**

    > How many questions are there that test speaking skills?

---

## 5.「And the the the main point of tofu is, uh there are so many reading uh contents and listening content. So uh if you want to express the and you know high schools uh content you you need have you know more advanced uh listening and waiting why you reading content, reading skills.」

1. **原文**

    > And the the the main point of tofu is, uh there are so many reading uh contents and listening content. So uh if you want to express the and you know high schools uh content you you need have you know more advanced uh listening and waiting why you reading content, reading skills.

2. **中文意**

    > （口误将 TOEFL 说成了 tofu）  
    > 托福的要点是，它有大量的阅读和听力内容。所以，如果你想在口语表达时引用（或者讨论）这些学术内容，你就需要更高水平的听力和阅读技能。

3. **改进后英文表达**

    > The main point of TOEFL is that there are many reading and listening passages. So if you want to discuss or summarize those academic materials, you need more advanced listening and reading skills.

_说明：

- "tofu" 应该是口误；
- "contents" 常可简化为 "passages" 或 "materials"。
- "waiting why you reading content" 可能是说"when you're reading, or while you're reading"?_

---

## 6.「Uh so so are your major your major.」

1. **原文**

    > Uh so so are your major your major.

2. **中文意**

    > （大概想问：你的专业是什么？）

3. **改进后英文表达**

    > So, what is your major?

---

## 7.「sense computer styles oh okay let me uh make yourself introduction. My name is Louis nard. And, uh, i'm a little older than you. Oh.」

（这一段似乎和后面自我介绍有重复，我合并处理。）

1. **原文**

    > sense computer styles oh okay let me uh make yourself introduction. My name is Louis nard. And, uh, i'm a little older than you. Oh.

2. **中文意**

    > "我来做个自我介绍吧，我叫 Louis Nard。我比你年长一点。"

3. **改进后英文表达**

    > Let me introduce myself. My name is Louis Nard, and I'm slightly older than you.

_说明：

- "sense computer styles" 不太清楚上下文，可能是听写或临时口误，故省略。
- "make yourself introduction" => "let me introduce myself."_

---

## 8.「My major in the undergraduate is a geography, and in my master degree I, uh, enrolling to the archaeology about play well, what's the term paleolithic and um before graduation I switched to a new career about, uh uh development um yeah and uh after graduation I entered the you know trip the account but our company is for the uh去哪儿的。」

这是比较长的一句，拆分如下：

1. **原文**

    > My major in the undergraduate is a geography, and in my master degree I, uh, enrolling to the archaeology about play well, what's the term paleolithic and um before graduation I switched to a new career about, uh uh development um yeah and uh after graduation I entered the you know trip the account but our company is for the uh去哪儿的。

2. **中文意**

    > 我本科专业是地理，研究生期间学的是跟旧石器时代（paleolithic）相关的考古方向。不过在毕业前我转到了软件开发领域。毕业后我去了旅游/机票（去哪儿？）相关的公司。

3. **改进后英文表达**

    > I majored in Geography as an undergraduate. For my master's degree, I studied Archaeology, specifically related to the Paleolithic period. However, before graduation, I switched to a new career in software development. After graduation, I joined a travel-related company (similar to Qunar), focusing on...

_说明：

- "My major in the undergraduate is a geography." 更地道可说 "I majored in Geography as an undergraduate."
- "in my master degree I, uh, enrolling to…" => "For my master's degree, I studied…"
- "the archaeology about play well, what's the term paleolithic" => "archaeology about the Paleolithic period."
- "trip the account but our company is for the uh去哪儿的" => 可能是在说"a travel company, like Qunar."_

---

## 9.「you mention that you can calculate the most low Price oh yeah.」

1. **原文**

    > you mention that you can calculate the most low Price oh yeah .

2. **中文意**

    > 你说你可以算出最低价，对吗？

3. **改进后英文表达**

    > You mentioned that you can calculate the lowest price, right?

_说明：

- "the most low Price" => "the lowest price."_

---

## 10.「yeah and oh。Uh you know there are so many like like goods or Prices from the supplier and you need uh, provide these goods and Prices uh, to your customers so there need a sector for this uh yeah like like like uh you know some certain algorithm bubble sort quick sort treasure you know the manager sorting algorithm yeah no actually our uh uh our group is uh like uh so and asserting algorithm.」

1. **原文**

    > yeah and oh。Uh you know there are so many like like goods or Prices from the supplier and you need uh, provide these goods and Prices uh, to your customers so there need a sector for this uh yeah like like like uh you know some certain algorithm bubble sort quick sort treasure you know the manager sorting algorithm yeah no actually our uh uh our group is uh like uh so and asserting algorithm.

2. **中文意**

    > （大意：有很多供应商给出的商品和价格信息，需要用算法对它们进行处理，比如冒泡排序、快速排序之类。我们团队主要做这类排序或管理算法。）

3. **改进后英文表达**

    > There are many goods and prices offered by different suppliers, and we need to organize and provide them to customers. This requires algorithms like bubble sort, quick sort, or other sorting methods. In fact, our team mainly works on implementing and managing these algorithms.

_说明：

- "treasure" 应该是口误，也可能是 "merge sort" 吧？
- "manager sorting algorithm" => "managing sorting algorithms."_

---

## 11.「I'm so interested in english and before I switched to the software development, I used to do want to become a want to become an english teacher at that time cause you know when I uh when I did research, when II was not interested in a research anymore, I want to find uh, skills can make a living.」

1. **原文**

    > I'm so interested in english and before I switched to the software development, I used to do want to become a want to become an english teacher at that time cause you know when I uh when I did research, when II was not interested in a research anymore, I want to find uh, skills can make a living.

2. **中文意**

    > 我对英语很感兴趣。在我转到软件开发之前，其实我想过成为一名英语老师。因为当时我在做科研，可是后来对科研不感兴趣了，所以就想找到一种能谋生的技能。

3. **改进后英文表达**

    > I'm very interested in English. Before I switched to software development, I wanted to become an English teacher. At that time, I was doing research, but I lost interest in it, so I wanted to find a skill that could help me make a living.

---

## 12.「And since we know each other, let me now think of mm and an interesting things. I have experience maybe uh, when I was sophomore and in the in the english corner I met I met a girl who who measures a philosophy.」

1. **原文**

    > And since we know each other, let me now think of mm and an interesting things. I have experience maybe uh, when I was sophomore and in the in the english corner I met I met a girl who who measures a philosophy.

2. **中文意**

    > 嗯，我们俩既然都认识，那让我想想一些有趣的经历吧。大概是我大二时，在英语角遇到一个学哲学的女孩。

3. **改进后英文表达**

    > Since we already know each other, let me share an interesting experience: when I was a sophomore, I met a girl at an English corner who was majoring in Philosophy.

_说明：

- "who who measures a philosophy" => "who was majoring in philosophy."_

---

## 13.「Now after that to english corner we had a we had a discussion maybe more than three hours and we walked uh along uh the campus, uh, maybe three hours and we talked so many things but uh actually II don't know what uh these are actually so uh uh really abstract.」

1. **原文**

    > Now after that to english corner we had a we had a discussion maybe more than three hours and we walked uh along uh the campus, uh, maybe three hours and we talked so many things but uh actually II don't know what uh these are actually so uh uh really abstract.

2. **中文意**

    > 后来在英语角，我们聊了三个多小时，一起在校园里边走边谈。说了很多，但其实我也不太清楚在说啥，都很抽象。

3. **改进后英文表达**

    > After that English corner, we had a discussion for over three hours, walking around campus. We talked about so many topics, but honestly, it was all quite abstract.

---

## 14.「I think it it it depends on my level, level of energy, you know? If, uh if I uh uh energetic, uh, when I when I when I feel bored, I would I would try some deep thinking in the, uh, do some things that high productivity.」

1. **原文**

    > I think it it it depends on my level, level of energy, you know? If, uh if I uh uh energetic, uh, when I when I when I feel bored, I would I would try some deep thinking in the, uh, do some things that high productivity.

2. **中文意**

    > 我觉得这取决于我的能量水平。如果我精力充沛，在感到无聊的时候，我就会进行一些深入思考或者做一些高效率的事情。

3. **改进后英文表达**

    > I think it depends on my energy level. If I'm feeling energetic, then when I'm bored, I'll try to do some deep thinking or engage in high-productivity tasks.

---

## 15.「But if I uh i'm lower energy energy, uh, I would choose to sleep. Uh in the uh uh actually I have a habit that I would uh write many things in a day, uh, when I have thought I would write it down and I when I when I feel bored, I would do, i'll check my to the list and I would find some things that maybe just need a little time uh, to finish.」

1. **原文**

    > But if I uh i'm lower energy energy, uh, I would choose to sleep. Uh in the uh uh actually I have a habit that I would uh write many things in a day, uh, when I have thought I would write it down and I when I when I feel bored, I would do, i'll check my to the list and I would find some things that maybe just need a little time uh, to finish.

2. **中文意**

    > 如果我精力不足，就会去睡觉。其实我有个习惯，一天会写很多东西，有想法就记下来。当我无聊时，就会去查看我的待办清单，找一些只需要花一点点时间就能完成的事。

3. **改进后英文表达**

    > But if my energy is low, I might just choose to sleep. Actually, I have a habit of writing down many thoughts during the day. Whenever I feel bored, I'll check my to-do list and pick something that only takes a little time to finish.

---

## 16.「No, both. I think everyone, everyone is complex.」

1. **原文**

    > No, both. I think everyone, everyone is complex.

2. **中文意**

    > 不，我觉得两者都有。我认为每个人都很复杂。

3. **改进后英文表达**

    > Actually, both. I think everyone is complex.

---

## 17.「So so it's it's a kind of wisdom, I think. But in our a daily life we mix them the process of arrangement and the compliment. Oh.」

1. **原文**

    > So so it's it's a kind of wisdom, I think. But in our a daily life we mix them the process of arrangement and the compliment. Oh.

2. **中文意**

    > 我觉得这是一种智慧。但是在日常生活中，我们往往把计划和执行的过程混在一起了。

3. **改进后英文表达**

    > I think it's a kind of wisdom. However, in our daily life, we often mix up the planning process with the execution process.

_说明："compliment" 可能是想说"completion"或"execution"之意。_

---

## 18.「II like a sentence, uh, which is I have no future, just feedback. Uh many, many people would uh define something a success or a few year. But but I think everything you have a good my mindset of review, uh, then everything is just I have a kind of process that can provide you the feedback and you can, uh you can know what's the new action you need to use, but you need to apply.」

1. **原文**

    > II like a sentence, uh, which is I have no future, just feedback. Uh many, many people would uh define something a success or a few year. But but I think everything you have a good my mindset of review, uh, then everything is just I have a kind of process that can provide you the feedback and you can, uh you can know what's the new action you need to use, but you need to apply.

2. **中文意**

    > 我喜欢这样一句话："我没有未来，只有反馈。" 很多人会把某件事情定义为成功或失败。但我认为，如果你有一个善于复盘的心态，你就能通过这个过程获得反馈，从而知道下一步该怎么做。

3. **改进后英文表达**

    > I like this quote: "I have no future, only feedback." Many people define things as either successes or failures, but I believe that if you maintain a mindset of constant review, everything becomes a process of feedback. That way, you can figure out your next action and apply it.

---

## 19.「I saw you are you are also interested in the art.」

1. **原文**

    > I saw you are you are also interested in the art.

2. **中文意**

    > 我看你也对艺术感兴趣。

3. **改进后英文表达**

    > I noticed you're also interested in art.

---

## 20.「II think it's probabilistic cause uh sometimes I would think uh like like transformers uh you know there are so many um kinds of combination of of uh like different music symbol and the uh some you know the CS can maybe list of the all the painters and then it would choose a meaningful painter noise.」

1. **原文**

    > II think it's probabilistic cause uh sometimes I would think uh like like transformers uh you know there are so many um kinds of combination of of uh like different music symbol and the uh some you know the CS can maybe list of the all the painters and then it would choose a meaningful painter noise .

2. **中文意**

    > 我觉得它有一定的概率性。比如 transformer 模型里有很多不同元素（音乐符号、图像等）可以组合。计算机可以先列出所有的可能，然后选择一些有意义的组合方式。

3. **改进后英文表达**

    > I think it's probabilistic. For example, with transformers, there are so many different elements (like musical notes or images) that can be combined. The computer can list out all possible patterns and then select a meaningful one.

_说明：

- "painter noise" 可能是"pattern"或"rendering"？根据语境调整。_

---

## 21.「pattern to express humans emotion, noise or mindset.」

1. **原文**

    > pattern to express humans emotion, noise or mindset.

2. **中文意**

    > 用这种模式来表达人的情绪、想法之类的。

3. **改进后英文表达**

    > …a pattern to convey human emotions, thoughts, or mindsets.

---

## 22.「hello. Susan um captions in this chat. So uh, I think it is more convenient for us to understand each other.」

1. **原文**

    > hello. Susan um captions in this chat. So uh, I think it is more convenient for us to understand each other.

2. **中文意**

    > Hello. Susan, [我们在这个聊天里开了字幕功能]，所以我觉得这对我们相互理解会更方便。

3. **改进后英文表达**

    > Hello Susan, we have captions here in the chat, so I think it'll be more convenient for us to understand each other.

---

## 23.「Go ahead，go. ahead. Okay, let's go on.」

1. **原文**

    > Go ahead，go . ahead. Okay, let's go on.

2. **中文意**

    > 继续吧，好的，我们继续。

3. **改进后英文表达**

    > Sure, go ahead. Let's continue.

---

## 24.「oh, I don't like to.」

1. **原文**

    > oh, I don't like to.

2. **中文意**

    > 噢，我不喜欢这样做/我不想那样。

3. **改进后英文表达**

    > Oh, I don't like that.

_说明：上下文是说是否喜欢某种收集(collect)之类的习惯，可稍作改写。_

---

## 25.「II like the stateless things, you know.」

1. **原文**

    > II like the stateless things, you know.

2. **中文意**

    > 我喜欢那种"无状态的东西"，你懂的。（可能指的是更抽象、更数字化，或更少实体占用？）

3. **改进后英文表达**

    > I prefer "stateless" things, if you know what I mean.

_说明：

- "stateless things" 是相当抽象的说法，看具体上下文是不是"digital/virtual things"之类？_

---

## 26.「Yeah yeah, yeah yeah a friend who who was girl. And we, uh, we have uh, we we you have known each other for more than maybe twenty, maybe twenty years. So yeah, maybe ten years.」

1. **原文**

    > Yeah yeah, yeah yeah a friend who who was girl. And we, uh, we have uh, we we you have known each other for more than maybe twenty, maybe twenty years. So yeah, maybe ten years.

2. **中文意**

    > 嗯是的，是个女性朋友。我们认识了也许有20年了，也可能10年。（语气中有点犹豫不定具体时长）

3. **改进后英文表达**

    > Yes, it's a female friend. We've known each other for maybe twenty years. Or maybe it's been about ten years—I'm not entirely sure.

---

## 27.「uh. Maybe in college or high high, high school way, uh, we learned uh, in different cities. So we we cut our uh connection but uh these years, uh win win win win win connection. And maybe in festival we uh we would you know play fireworks together in our yield.」

1. **原文**

    > uh. Maybe in college or high high, high school way, uh, we learned uh, in different cities. So we we cut our uh connection but uh these years, uh win win win win win connection. And maybe in festival we uh we would you know play fireworks together in our yield.

2. **中文意**

    > 也许是在大学或者高中阶段，我们去不同的城市读书，所以那时就断了联系。这几年又重新联系上了。逢年过节我们会在院子里放烟花之类的。

3. **改进后英文表达**

    > Maybe it was during college or high school when we went to study in different cities, so we lost touch for a while. But in recent years, we reconnected, and during festivals, we sometimes set off fireworks together in the yard.

---

## 28.「That's really moving night I think cause uh we we we have so many uh.」

1. **原文**

    > That's really moving night I think cause uh we we we have so many uh .

2. **中文意**

    > 我觉得那天晚上真的蛮感动的，因为我们有很多……（后面似乎没说完）

3. **改进后英文表达**

    > I think it was a really moving night because we shared so many memories.

---

## 29.「It's a good question. And I think the neighborhood neighborhood uh depends on like um are you in the same uh obviously I read the same class. I think it's important.」

1. **原文**

    > It's a good question. And I think the neighborhood neighborhood uh depends on like um are you in the same uh obviously I read the same class. I think it's important.

2. **中文意**

    > 这是个好问题。我觉得邻里关系要看是不是处在相同的……可能是同一个社区或同类群体吧。我觉得这很重要。

3. **改进后英文表达**

    > That's a good question. I think neighborhood relationships depend on whether you're in the same community or share common interests. That can be important.

---

## 30.「Uh for example, my uh my uh our department is uh uh a schools department, you know uh uh every family's parents, maybe other teachers in this school, uh both uh our children and our parents have the uh maybe same value seem a context.」

1. **原文**

    > Uh for example, my uh my uh our department is uh uh a schools department, you know uh uh every family's parents, maybe other teachers in this school, uh both uh our children and our parents have the uh maybe same value seem a context.

2. **中文意**

    > 比如说我们这个小区（或部门）就跟学校有关，家家户户都是老师或者学校的家长，所以他们的价值观和背景比较相似。

3. **改进后英文表达**

    > For example, in our neighborhood (or department), many families have parents who are teachers at the same school, so both the children and the parents share similar values and context.

---

（后面还有一些相似的口语段落，我会略微再浓缩，以保证整体可读性。）

---

## 31.「Um I think uh uh with my teammates, you know in in the company because we have the same goal we have are seeing issues to solve. Uh, so I think it's easy to build a solid relationship.」

1. **原文**

    > Um I think uh uh with my teammates, you know in in the company because we have the same goal we have are seeing issues to solve. Uh, so I think it's easy to build a solid relationship.

2. **中文意**

    > 我觉得，在公司里和同事相处，因为我们有共同目标，要解决同样的问题，所以很容易建立牢固的关系。

3. **改进后英文表达**

    > I think that with my teammates in the company, we share the same goals and face the same issues, so it's easier to build a solid relationship.

---

## 32.「I don't. I don't like this.」

1. **原文**

    > I don't. I don't like this .

2. **中文意**

    > 我不喜欢这个 / 我不想这样。

3. **改进后英文表达**

    > I don't like this.

---

## 33.「you before you watch that movie are depending on you know the a story main storyline of it because I think everybody in china, noise noise are something about.」

1. **原文**

    > you before you watch that movie are depending on you know the a story main storyline of it because I think everybody in china, noise noise are something about.

2. **中文意**

    > 在看那个电影之前，你可以先了解一下它的大致剧情。我觉得每个中国人对这个神话/故事都有点了解……

3. **改进后英文表达**

    > Before you watch that movie, you might want to know its main storyline, because I think everyone in China is somewhat familiar with it.

---

## 34.「Uh actually III thought of this question many times noise uh the the final answer I got it, I got it was um it it depends on the students'output, whether it can be like whether it can be checked online or offline.」

1. **原文**

    > Uh actually III thought of this question many times noise uh the the final answer I got it, I got it was um it it depends on the students'output, whether it can be like whether it can be checked online or offline.

1. **中文意**

    > 事实上，我想过这个问题很多次。我得出的结论是要看学生的产出能不能在线检测。如果可以，就比较适合线上，否则就需要线下。

1. **改进后英文表达**

    > Actually, I've thought about this question many times. The conclusion I came to is that it depends on whether the student's output can be evaluated online or needs to be evaluated offline.

---

## 35.「Um teachers uh just have to have a administration function but um about some like physical. 体育课咋说？是那个physical什么吗？」

1. **原文**

    > Um teachers uh just have to have a administration function but um about some like physical. 体育课咋说？是那个physical什么吗？

1. **中文意**

    > 老师只需要行政管理的功能。不过像体育课之类的（要线下上）。

1. **改进后英文表达**

    > Teachers just need to perform an administrative or supervisory function. However, for something like PE (Physical Education), it probably needs to be offline.

---

## 36.「Uh like uh for for adults uh like us, if we learned uh you know learn the problems on the live code, we we can find some resources and some analysis online in the weekend learning it.」

1. **原文**

    > Uh like uh for for adults uh like us, if we learned uh you know learn the problems on the live code, we we can find some resources and some analysis online in the weekend learning it.

1. **中文意**

    > 对我们这种成年人来说，如果是一些编程题，可以周末在网上找到资源和解析进行学习。

1. **改进后英文表达**

    > For adults like us, if we're learning programming problems, we can find resources and analyses online over the weekend to study.

---

## 37.「But if we want to improve our real um you know you know you want to learn some James jim knowledge, you know if you want to have more healthy, more healthy body, uh you you need uh uh a coach or trainer.」

1. **原文**

    > But if we want to improve our real um you know you know you want to learn some James jim knowledge, you know if you want to have more healthy, more healthy body, uh you you need uh uh a coach or trainer.

1. **中文意**

    > 但是如果你想真正提高，比如健身，想要更健康的身体，就需要有线下的教练或培训。

1. **改进后英文表达**

    > But if you want to improve physically, for example learning proper gym techniques to stay healthy, you need a coach or trainer in person.

_说明："James jim knowledge" 应该是想说"gym / fitness knowledge。"_

---

## 38.「Um I think it's another another uh topic costs on the edge advancement of online resources as the same content campaign deprived for more people.」

1. **原文**

    > Um I think it's another another uh topic costs on the edge advancement of online resources as the same content campaign deprived for more people.

1. **中文意**

    > 我觉得这是另一个话题，关于在线资源的前沿发展，以及相同内容如何能被更多人享用。

1. **改进后英文表达**

    > I think this is another topic concerning the cutting-edge development of online resources, and how the same content can be provided to a larger audience.

---

## 39.「Have you um have you did that classic slabs like, uh. Okay MIT six.」

1. **原文**

    > Have you um have you did that classic slabs like, uh. Okay MIT six.

1. **中文意**

    > 你有没有上过那些经典的课程，比如 MIT 6.xxx 之类的？

1. **改进后英文表达**

    > Have you tried those classic courses, like MIT's 6.xxx classes?

_说明：

- "Have you did" → "Have you done" / "Have you tried."_

---

## 40.「I think that are uh good examples for online courses. Uh, students all over the world, uh, enjoy good lectures in the good project.」

1. **原文**

    > I think that are uh good examples for online courses. Uh, students all over the world, uh, enjoy good lectures in the good project.

1. **中文意**

    > 我觉得那是很好的网课例子，全世界的学生都能享受这些优质课程。

1. **改进后英文表达**

    > I think those are great examples of online courses. Students from all over the world can enjoy high-quality lectures and projects.

---

(以上已基本涵盖了发言人2在对话中较明显的英文段落，中间有些完全中文或零星插话的部分已省略。)

---

## 总结与建议

1. **整体观察**
    
    - 您的口语中体现出能使用一些稍复杂的词汇（如 _algorithm_、_probabilistic_、_transformer_ 等），并且能表达一定抽象概念，说明思路相对开阔。
    - 但也可以看到**句子结构**常常凌乱或不完整，**口头填充词** ("uh," "you know," "like") 较多，**语法**（时态、主谓一致、冠词、短语搭配）及**发音**的准确度都有提升空间。
122. **改进方向**
    
    - **简化长句，突出主干**：尤其是当想表达复杂概念时，先用一个主干句说出核心逻辑，再补充修饰细节，避免一口气塞进太多成分。
    - **减少重复、口头语**：尝试减少 “uh, um, you know” 等填充，代之以短暂停顿。
    - **强化常见时态与句型**：如现在完成时、一般过去时等，熟练掌握后能更准确地陈述经历与观点。
    - **发音与单词稳定性**：同一个单词力求每次都相对一致的发音。集中记忆高频词汇的正确发音。
123. **练习方式**
    
    - **录音自查**：就像这次对话后逐句复盘，找出常错点，集中攻克。
    - **口头表述先列提纲**：先在脑海里或纸上简单列出逻辑顺序，再开口说，减少卡顿。
    - **模仿与跟读**：选择一些地道音频资料（TED 演讲、播客等），跟读或复述，模仿原句的结构与语调。
    - **注重反馈与纠正**：可以借助 AI 或请懂英语的朋友听，并指出错误，及时修正。

通过以上针对性训练，相信您的口语会在逻辑、准确度和流利度上都有显著提升。祝学习顺利、进步加速!



[英语/口语 review/口语复盘.md]
======================

---
aliases: [总结与建议]
draw:
tags: []
title: 总结与建议
linter-yaml-title-alias: 总结与建议
date created: 2025-02-01
date modified: 2025-02-01
---

---

## 4.「Uh, how many questions in the uh, uh ask about the oral skills?」

1. **原文**

    > Uh, how many questions in the uh, uh ask about the oral skills?

2. **中文意**

    > 额，口语部分有多少道题要回答呢？

3. **改进后英文表达**

    > How many questions are there that test speaking skills?

---

## 5.「And the the the main point of tofu is, uh there are so many reading uh contents and listening content. So uh if you want to express the and you know high schools uh content you you need have you know more advanced uh listening and waiting why you reading content, reading skills.」

1. **原文**

    > And the the the main point of tofu is, uh there are so many reading uh contents and listening content. So uh if you want to express the and you know high schools uh content you you need have you know more advanced uh listening and waiting why you reading content, reading skills.

2. **中文意**

    > （口误将 TOEFL 说成了 tofu）  
    > 托福的要点是，它有大量的阅读和听力内容。所以，如果你想在口语表达时引用（或者讨论）这些学术内容，你就需要更高水平的听力和阅读技能。

3. **改进后英文表达**

    > The main point of TOEFL is that there are many reading and listening passages. So if you want to discuss or summarize those academic materials, you need more advanced listening and reading skills.

_说明：

- "tofu" 应该是口误；
- "contents" 常可简化为 "passages" 或 "materials"。
- "waiting why you reading content" 可能是说"when you're reading, or while you're reading"?_

---

## 6.「Uh so so are your major your major.」

1. **原文**

    > Uh so so are your major your major.

2. **中文意**

    > （大概想问：你的专业是什么？）

3. **改进后英文表达**

    > So, what is your major?

---
 

## 7.「sense computer styles oh okay let me uh make yourself introduction. My name is Louis nard. And, uh, i'm a little older than you. Oh.」

（这一段似乎和后面自我介绍有重复，我合并处理。）

1. **原文**

    > sense computer styles oh okay let me uh make yourself introduction. My name is Louis nard. And, uh, i'm a little older than you. Oh.

2. **中文意**

    > "我来做个自我介绍吧，我叫 Louis Nard。我比你年长一点。"

3. **改进后英文表达**

    > Let me introduce myself. My name is Louis Nard, and I'm slightly older than you.

_说明：

- "sense computer styles" 不太清楚上下文，可能是听写或临时口误，故省略。
- "make yourself introduction" => "let me introduce myself."_

---

## 8.「My major in the undergraduate is a geography, and in my master degree I, uh, enrolling to the archaeology about play well, what's the term paleolithic and um before graduation I switched to a new career about, uh uh development um yeah and uh after graduation I entered the you know trip the account but our company is for the uh去哪儿的。」

这是比较长的一句，拆分如下：

1. **原文**

    > My major in the undergraduate is a geography, and in my master degree I, uh, enrolling to the archaeology about play well, what's the term paleolithic and um before graduation I switched to a new career about, uh uh development um yeah and uh after graduation I entered the you know trip the account but our company is for the uh去哪儿的。

2. **中文意**

    > 我本科专业是地理，研究生期间学的是跟旧石器时代（paleolithic）相关的考古方向。不过在毕业前我转到了软件开发领域。毕业后我去了旅游/机票（去哪儿？）相关的公司。

3. **改进后英文表达**

    > I majored in Geography as an undergraduate. For my master's degree, I studied Archaeology, specifically related to the Paleolithic period. However, before graduation, I switched to a new career in software development. After graduation, I joined a travel-related company (similar to Qunar), focusing on...

_说明：

- "My major in the undergraduate is a geography." 更地道可说 "I majored in Geography as an undergraduate."
- "in my master degree I, uh, enrolling to…" => "For my master's degree, I studied…"
- "the archaeology about play well, what's the term paleolithic" => "archaeology about the Paleolithic period."
- "trip the account but our company is for the uh去哪儿的" => 可能是在说"a travel company, like Qunar."_

---

## 9.「you mention that you can calculate the most low Price oh yeah.」

1. **原文**

    > you mention that you can calculate the most low Price oh yeah .

2. **中文意**

    > 你说你可以算出最低价，对吗？

3. **改进后英文表达**

    > You mentioned that you can calculate the lowest price, right?

_说明：

- "the most low Price" => "the lowest price."_

---

## 10.「yeah and oh。Uh you know there are so many like like goods or Prices from the supplier and you need uh, provide these goods and Prices uh, to your customers so there need a sector for this uh yeah like like like uh you know some certain algorithm bubble sort quick sort treasure you know the manager sorting algorithm yeah no actually our uh uh our group is uh like uh so and asserting algorithm.」

1. **原文**

    > yeah and oh。Uh you know there are so many like like goods or Prices from the supplier and you need uh, provide these goods and Prices uh, to your customers so there need a sector for this uh yeah like like like uh you know some certain algorithm bubble sort quick sort treasure you know the manager sorting algorithm yeah no actually our uh uh our group is uh like uh so and asserting algorithm.

2. **中文意**

    > （大意：有很多供应商给出的商品和价格信息，需要用算法对它们进行处理，比如冒泡排序、快速排序之类。我们团队主要做这类排序或管理算法。）

3. **改进后英文表达**

    > There are many goods and prices offered by different suppliers, and we need to organize and provide them to customers. This requires algorithms like bubble sort, quick sort, or other sorting methods. In fact, our team mainly works on implementing and managing these algorithms.

_说明：

- "treasure" 应该是口误，也可能是 "merge sort" 吧？
- "manager sorting algorithm" => "managing sorting algorithms."_

---

## 11.「I'm so interested in english and before I switched to the software development, I used to do want to become a want to become an english teacher at that time cause you know when I uh when I did research, when II was not interested in a research anymore, I want to find uh, skills can make a living.」

1. **原文**

    > I'm so interested in english and before I switched to the software development, I used to do want to become a want to become an english teacher at that time cause you know when I uh when I did research, when II was not interested in a research anymore, I want to find uh, skills can make a living.

2. **中文意**

    > 我对英语很感兴趣。在我转到软件开发之前，其实我想过成为一名英语老师。因为当时我在做科研，可是后来对科研不感兴趣了，所以就想找到一种能谋生的技能。

3. **改进后英文表达**

    > I'm very interested in English. Before I switched to software development, I wanted to become an English teacher. At that time, I was doing research, but I lost interest in it, so I wanted to find a skill that could help me make a living.

---

## 12.「And since we know each other, let me now think of mm and an interesting things. I have experience maybe uh, when I was sophomore and in the in the english corner I met I met a girl who who measures a philosophy.」

1. **原文**

    > And since we know each other, let me now think of mm and an interesting things. I have experience maybe uh, when I was sophomore and in the in the english corner I met I met a girl who who measures a philosophy.

2. **中文意**

    > 嗯，我们俩既然都认识，那让我想想一些有趣的经历吧。大概是我大二时，在英语角遇到一个学哲学的女孩。

3. **改进后英文表达**

    > Since we already know each other, let me share an interesting experience: when I was a sophomore, I met a girl at an English corner who was majoring in Philosophy.

_说明：

- "who who measures a philosophy" => "who was majoring in philosophy."_

---

## 13.「Now after that to english corner we had a we had a discussion maybe more than three hours and we walked uh along uh the campus, uh, maybe three hours and we talked so many things but uh actually II don't know what uh these are actually so uh uh really abstract.」

1. **原文**

    > Now after that to english corner we had a we had a discussion maybe more than three hours and we walked uh along uh the campus, uh, maybe three hours and we talked so many things but uh actually II don't know what uh these are actually so uh uh really abstract.

2. **中文意**

    > 后来在英语角，我们聊了三个多小时，一起在校园里边走边谈。说了很多，但其实我也不太清楚在说啥，都很抽象。

3. **改进后英文表达**

    > After that English corner, we had a discussion for over three hours, walking around campus. We talked about so many topics, but honestly, it was all quite abstract.

---

## 14.「I think it it it depends on my level, level of energy, you know? If, uh if I uh uh energetic, uh, when I when I when I feel bored, I would I would try some deep thinking in the, uh, do some things that high productivity.」

1. **原文**

    > I think it it it depends on my level, level of energy, you know? If, uh if I uh uh energetic, uh, when I when I when I feel bored, I would I would try some deep thinking in the, uh, do some things that high productivity.

2. **中文意**

    > 我觉得这取决于我的能量水平。如果我精力充沛，在感到无聊的时候，我就会进行一些深入思考或者做一些高效率的事情。

3. **改进后英文表达**

    > I think it depends on my energy level. If I'm feeling energetic, then when I'm bored, I'll try to do some deep thinking or engage in high-productivity tasks.

---

## 15.「But if I uh i'm lower energy energy, uh, I would choose to sleep. Uh in the uh uh actually I have a habit that I would uh write many things in a day, uh, when I have thought I would write it down and I when I when I feel bored, I would do, i'll check my to the list and I would find some things that maybe just need a little time uh, to finish.」

1. **原文**

    > But if I uh i'm lower energy energy, uh, I would choose to sleep. Uh in the uh uh actually I have a habit that I would uh write many things in a day, uh, when I have thought I would write it down and I when I when I feel bored, I would do, i'll check my to the list and I would find some things that maybe just need a little time uh, to finish.

2. **中文意**

    > 如果我精力不足，就会去睡觉。其实我有个习惯，一天会写很多东西，有想法就记下来。当我无聊时，就会去查看我的待办清单，找一些只需要花一点点时间就能完成的事。

3. **改进后英文表达**

    > But if my energy is low, I might just choose to sleep. Actually, I have a habit of writing down many thoughts during the day. Whenever I feel bored, I'll check my to-do list and pick something that only takes a little time to finish.

---

## 16.「No, both. I think everyone, everyone is complex.」

1. **原文**

    > No, both. I think everyone, everyone is complex.

2. **中文意**

    > 不，我觉得两者都有。我认为每个人都很复杂。

3. **改进后英文表达**

    > Actually, both. I think everyone is complex.

---

## 17.「So so it's it's a kind of wisdom, I think. But in our a daily life we mix them the process of arrangement and the compliment. Oh.」

1. **原文**

    > So so it's it's a kind of wisdom, I think. But in our a daily life we mix them the process of arrangement and the compliment. Oh.

2. **中文意**

    > 我觉得这是一种智慧。但是在日常生活中，我们往往把计划和执行的过程混在一起了。

3. **改进后英文表达**

    > I think it's a kind of wisdom. However, in our daily life, we often mix up the planning process with the execution process.

_说明："compliment" 可能是想说"completion"或"execution"之意。_

---

## 18.「II like a sentence, uh, which is I have no future, just feedback. Uh many, many people would uh define something a success or a few year. But but I think everything you have a good my mindset of review, uh, then everything is just I have a kind of process that can provide you the feedback and you can, uh you can know what's the new action you need to use, but you need to apply.」

1. **原文**

    > II like a sentence, uh, which is I have no future, just feedback. Uh many, many people would uh define something a success or a few year. But but I think everything you have a good my mindset of review, uh, then everything is just I have a kind of process that can provide you the feedback and you can, uh you can know what's the new action you need to use, but you need to apply.

2. **中文意**

    > 我喜欢这样一句话："我没有未来，只有反馈。" 很多人会把某件事情定义为成功或失败。但我认为，如果你有一个善于复盘的心态，你就能通过这个过程获得反馈，从而知道下一步该怎么做。

3. **改进后英文表达**

    > I like this quote: "I have no future, only feedback." Many people define things as either successes or failures, but I believe that if you maintain a mindset of constant review, everything becomes a process of feedback. That way, you can figure out your next action and apply it.

---

## 19.「I saw you are you are also interested in the art.」

1. **原文**

    > I saw you are you are also interested in the art.

2. **中文意**

    > 我看你也对艺术感兴趣。

3. **改进后英文表达**

    > I noticed you're also interested in art.

---

## 20.「II think it's probabilistic cause uh sometimes I would think uh like like transformers uh you know there are so many um kinds of combination of of uh like different music symbol and the uh some you know the CS can maybe list of the all the painters and then it would choose a meaningful painter noise.」

1. **原文**

    > II think it's probabilistic cause uh sometimes I would think uh like like transformers uh you know there are so many um kinds of combination of of uh like different music symbol and the uh some you know the CS can maybe list of the all the painters and then it would choose a meaningful painter noise .

2. **中文意**

    > 我觉得它有一定的概率性。比如 transformer 模型里有很多不同元素（音乐符号、图像等）可以组合。计算机可以先列出所有的可能，然后选择一些有意义的组合方式。

3. **改进后英文表达**

    > I think it's probabilistic. For example, with transformers, there are so many different elements (like musical notes or images) that can be combined. The computer can list out all possible patterns and then select a meaningful one.

_说明：

- "painter noise" 可能是"pattern"或"rendering"？根据语境调整。_

---

## 21.「pattern to express humans emotion, noise or mindset.」

1. **原文**

    > pattern to express humans emotion, noise or mindset.

2. **中文意**

    > 用这种模式来表达人的情绪、想法之类的。

3. **改进后英文表达**

    > …a pattern to convey human emotions, thoughts, or mindsets.

---

## 22.「hello. Susan um captions in this chat. So uh, I think it is more convenient for us to understand each other.」

1. **原文**

    > hello. Susan um captions in this chat. So uh, I think it is more convenient for us to understand each other.

2. **中文意**

    > Hello. Susan, [我们在这个聊天里开了字幕功能]，所以我觉得这对我们相互理解会更方便。

3. **改进后英文表达**

    > Hello Susan, we have captions here in the chat, so I think it'll be more convenient for us to understand each other.

---

## 23.「Go ahead，go. ahead. Okay, let's go on.」

1. **原文**

    > Go ahead，go . ahead. Okay, let's go on.

2. **中文意**

    > 继续吧，好的，我们继续。

3. **改进后英文表达**

    > Sure, go ahead. Let's continue.

---

## 24.「oh, I don't like to.」

1. **原文**

    > oh, I don't like to.

2. **中文意**

    > 噢，我不喜欢这样做/我不想那样。

3. **改进后英文表达**

    > Oh, I don't like that.

_说明：上下文是说是否喜欢某种收集(collect)之类的习惯，可稍作改写。_

---

## 25.「II like the stateless things, you know.」

1. **原文**

    > II like the stateless things, you know.

2. **中文意**

    > 我喜欢那种"无状态的东西"，你懂的。（可能指的是更抽象、更数字化，或更少实体占用？）

3. **改进后英文表达**

    > I prefer "stateless" things, if you know what I mean.

_说明：

- "stateless things" 是相当抽象的说法，看具体上下文是不是"digital/virtual things"之类？_

---

## 26.「Yeah yeah, yeah yeah a friend who who was girl. And we, uh, we have uh, we we you have known each other for more than maybe twenty, maybe twenty years. So yeah, maybe ten years.」

1. **原文**

    > Yeah yeah, yeah yeah a friend who who was girl. And we, uh, we have uh, we we you have known each other for more than maybe twenty, maybe twenty years. So yeah, maybe ten years.

2. **中文意**

    > 嗯是的，是个女性朋友。我们认识了也许有20年了，也可能10年。（语气中有点犹豫不定具体时长）

3. **改进后英文表达**

    > Yes, it's a female friend. We've known each other for maybe twenty years. Or maybe it's been about ten years—I'm not entirely sure.

---

## 27.「uh. Maybe in college or high high, high school way, uh, we learned uh, in different cities. So we we cut our uh connection but uh these years, uh win win win win win connection. And maybe in festival we uh we would you know play fireworks together in our yield.」

1. **原文**

    > uh. Maybe in college or high high, high school way, uh, we learned uh, in different cities. So we we cut our uh connection but uh these years, uh win win win win win connection. And maybe in festival we uh we would you know play fireworks together in our yield.

2. **中文意**

    > 也许是在大学或者高中阶段，我们去不同的城市读书，所以那时就断了联系。这几年又重新联系上了。逢年过节我们会在院子里放烟花之类的。

3. **改进后英文表达**

    > Maybe it was during college or high school when we went to study in different cities, so we lost touch for a while. But in recent years, we reconnected, and during festivals, we sometimes set off fireworks together in the yard.

---

## 28.「That's really moving night I think cause uh we we we have so many uh.」

1. **原文**

    > That's really moving night I think cause uh we we we have so many uh .

2. **中文意**

    > 我觉得那天晚上真的蛮感动的，因为我们有很多……（后面似乎没说完）

3. **改进后英文表达**

    > I think it was a really moving night because we shared so many memories.

---

## 29.「It's a good question. And I think the neighborhood neighborhood uh depends on like um are you in the same uh obviously I read the same class. I think it's important.」

1. **原文**

    > It's a good question. And I think the neighborhood neighborhood uh depends on like um are you in the same uh obviously I read the same class. I think it's important.

2. **中文意**

    > 这是个好问题。我觉得邻里关系要看是不是处在相同的……可能是同一个社区或同类群体吧。我觉得这很重要。

3. **改进后英文表达**

    > That's a good question. I think neighborhood relationships depend on whether you're in the same community or share common interests. That can be important.

---

## 30.「Uh for example, my uh my uh our department is uh uh a schools department, you know uh uh every family's parents, maybe other teachers in this school, uh both uh our children and our parents have the uh maybe same value seem a context.」

1. **原文**

    > Uh for example, my uh my uh our department is uh uh a schools department, you know uh uh every family's parents, maybe other teachers in this school, uh both uh our children and our parents have the uh maybe same value seem a context.

2. **中文意**

    > 比如说我们这个小区（或部门）就跟学校有关，家家户户都是老师或者学校的家长，所以他们的价值观和背景比较相似。

3. **改进后英文表达**

    > For example, in our neighborhood (or department), many families have parents who are teachers at the same school, so both the children and the parents share similar values and context.

---

（后面还有一些相似的口语段落，我会略微再浓缩，以保证整体可读性。）

---

## 31.「Um I think uh uh with my teammates, you know in in the company because we have the same goal we have are seeing issues to solve. Uh, so I think it's easy to build a solid relationship.」

1. **原文**

    > Um I think uh uh with my teammates, you know in in the company because we have the same goal we have are seeing issues to solve. Uh, so I think it's easy to build a solid relationship.

2. **中文意**

    > 我觉得，在公司里和同事相处，因为我们有共同目标，要解决同样的问题，所以很容易建立牢固的关系。

3. **改进后英文表达**

    > I think that with my teammates in the company, we share the same goals and face the same issues, so it's easier to build a solid relationship.

---

## 32.「I don't. I don't like this.」

1. **原文**

    > I don't. I don't like this .

2. **中文意**

    > 我不喜欢这个 / 我不想这样。

3. **改进后英文表达**

    > I don't like this.

---

## 33.「you before you watch that movie are depending on you know the a story main storyline of it because I think everybody in china, noise noise are something about.」

1. **原文**

    > you before you watch that movie are depending on you know the a story main storyline of it because I think everybody in china, noise noise are something about.

2. **中文意**

    > 在看那个电影之前，你可以先了解一下它的大致剧情。我觉得每个中国人对这个神话/故事都有点了解……

3. **改进后英文表达**

    > Before you watch that movie, you might want to know its main storyline, because I think everyone in China is somewhat familiar with it.

---

## 34.「Uh actually III thought of this question many times noise uh the the final answer I got it, I got it was um it it depends on the students'output, whether it can be like whether it can be checked online or offline.」

1. **原文**

    > Uh actually III thought of this question many times noise uh the the final answer I got it, I got it was um it it depends on the students'output, whether it can be like whether it can be checked online or offline.

1. **中文意**

    > 事实上，我想过这个问题很多次。我得出的结论是要看学生的产出能不能在线检测。如果可以，就比较适合线上，否则就需要线下。

1. **改进后英文表达**

    > Actually, I've thought about this question many times. The conclusion I came to is that it depends on whether the student's output can be evaluated online or needs to be evaluated offline.

---

## 35.「Um teachers uh just have to have a administration function but um about some like physical. 体育课咋说？是那个physical什么吗？」

1. **原文**

    > Um teachers uh just have to have a administration function but um about some like physical. 体育课咋说？是那个physical什么吗？

1. **中文意**

    > 老师只需要行政管理的功能。不过像体育课之类的（要线下上）。

1. **改进后英文表达**

    > Teachers just need to perform an administrative or supervisory function. However, for something like PE (Physical Education), it probably needs to be offline.

---

## 36.「Uh like uh for for adults uh like us, if we learned uh you know learn the problems on the live code, we we can find some resources and some analysis online in the weekend learning it.」

1. **原文**

    > Uh like uh for for adults uh like us, if we learned uh you know learn the problems on the live code, we we can find some resources and some analysis online in the weekend learning it.

1. **中文意**

    > 对我们这种成年人来说，如果是一些编程题，可以周末在网上找到资源和解析进行学习。

1. **改进后英文表达**

    > For adults like us, if we're learning programming problems, we can find resources and analyses online over the weekend to study.

---

## 37.「But if we want to improve our real um you know you know you want to learn some James jim knowledge, you know if you want to have more healthy, more healthy body, uh you you need uh uh a coach or trainer.」

1. **原文**

    > But if we want to improve our real um you know you know you want to learn some James jim knowledge, you know if you want to have more healthy, more healthy body, uh you you need uh uh a coach or trainer.

1. **中文意**

    > 但是如果你想真正提高，比如健身，想要更健康的身体，就需要有线下的教练或培训。

1. **改进后英文表达**

    > But if you want to improve physically, for example learning proper gym techniques to stay healthy, you need a coach or trainer in person.

_说明："James jim knowledge" 应该是想说"gym / fitness knowledge。"_

---

## 38.「Um I think it's another another uh topic costs on the edge advancement of online resources as the same content campaign deprived for more people.」

1. **原文**

    > Um I think it's another another uh topic costs on the edge advancement of online resources as the same content campaign deprived for more people.

1. **中文意**

    > 我觉得这是另一个话题，关于在线资源的前沿发展，以及相同内容如何能被更多人享用。

1. **改进后英文表达**

    > I think this is another topic concerning the cutting-edge development of online resources, and how the same content can be provided to a larger audience.

---

## 39.「Have you um have you did that classic slabs like, uh. Okay MIT six.」

1. **原文**

    > Have you um have you did that classic slabs like, uh. Okay MIT six.

1. **中文意**

    > 你有没有上过那些经典的课程，比如 MIT 6.xxx 之类的？

1. **改进后英文表达**

    > Have you tried those classic courses, like MIT's 6.xxx classes?

_说明：

- "Have you did" → "Have you done" / "Have you tried."_

---

## 40.「I think that are uh good examples for online courses. Uh, students all over the world, uh, enjoy good lectures in the good project.」

1. **原文**

    > I think that are uh good examples for online courses. Uh, students all over the world, uh, enjoy good lectures in the good project.

1. **中文意**

    > 我觉得那是很好的网课例子，全世界的学生都能享受这些优质课程。

1. **改进后英文表达**

    > I think those are great examples of online courses. Students from all over the world can enjoy high-quality lectures and projects.

---

(以上已基本涵盖了发言人2在对话中较明显的英文段落，中间有些完全中文或零星插话的部分已省略。)

---

## 总结与建议

1. **整体观察**
    
    - 您的口语中体现出能使用一些稍复杂的词汇（如 _algorithm_、_probabilistic_、_transformer_ 等），并且能表达一定抽象概念，说明思路相对开阔。
    - 但也可以看到**句子结构**常常凌乱或不完整，**口头填充词** ("uh," "you know," "like") 较多，**语法**（时态、主谓一致、冠词、短语搭配）及**发音**的准确度都有提升空间。
122. **改进方向**
    
    - **简化长句，突出主干**：尤其是当想表达复杂概念时，先用一个主干句说出核心逻辑，再补充修饰细节，避免一口气塞进太多成分。
    - **减少重复、口头语**：尝试减少 “uh, um, you know” 等填充，代之以短暂停顿。
    - **强化常见时态与句型**：如现在完成时、一般过去时等，熟练掌握后能更准确地陈述经历与观点。
    - **发音与单词稳定性**：同一个单词力求每次都相对一致的发音。集中记忆高频词汇的正确发音。
123. **练习方式**
    
    - **录音自查**：就像这次对话后逐句复盘，找出常错点，集中攻克。
    - **口头表述先列提纲**：先在脑海里或纸上简单列出逻辑顺序，再开口说，减少卡顿。
    - **模仿与跟读**：选择一些地道音频资料（TED 演讲、播客等），跟读或复述，模仿原句的结构与语调。
    - **注重反馈与纠正**：可以借助 AI 或请懂英语的朋友听，并指出错误，及时修正。

通过以上针对性训练，相信您的口语会在逻辑、准确度和流利度上都有显著提升。祝学习顺利、进步加速!


