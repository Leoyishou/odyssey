---
draw:
tags: []
title: 注意力机制
date created: 2025-01-29
date modified: 2025-01-29
---

Seq2Seq and Attention Models

Thought for 3m 35s

下面是一份分步骤、通俗易懂的讲解，帮助你理解 CS224n 第六部分（"神经机器翻译、Seq2Seq 与注意力机制"）的主要内容。我会结合一些例子，尽量让概念更加清晰。

---

## 一、背景与动机

### 1. 为什么要研究"序列到序列"（Seq2Seq）模型？

在之前的 NLP 任务中（如命名实体识别、语言模型预测下一个词等），我们大多只需要输出一个单独的标签或词。然而在某些任务中，我们的输出本身也是一个长度不固定的序列，比如：

- **机器翻译**：输入是某种语言的句子，输出是另一种语言的句子。
- **对话系统**：根据上一句话，输出一个回答或回应。
- **文本摘要**：输入可能是长篇文章，输出是简短摘要。

这些任务需要"从一个序列映射到另一个序列"，因此就有了 Seq2Seq 模型。

### 2. 传统机器翻译的历史方法

在深度学习方法普及之前，机器翻译多用**统计机器翻译（SMT）**，比如基于 n-gram 语言模型、词/短语对齐、翻译规则等。然而它们常常无法很好地捕捉长距离依赖或复杂的句子结构，开发和维护也比较复杂。2014 年，基于神经网络的**Seq2Seq** 翻译模型崛起，迅速成为主流。

---

## 二、Seq2Seq 基本原理

### 1. 编码器（Encoder）

Seq2Seq 的编码器主要将输入序列（例如英文句子）"编码"成一个固定长度的向量（也叫"上下文向量" context vector）。这个向量可以被视为对整个输入序列含义的压缩表达。

- **操作方式**：通常使用 RNN（如 LSTM/GRU）把输入序列 {x1,x2,…,xn}\{x_1, x_2, \dots, x_n\}{x1​,x2​,…,xn​} 依次读进去，得到一系列隐藏状态 {h1,h2,…,hn}\{h_1, h_2, \dots, h_n\}{h1​,h2​,…,hn​}，最后一个隐藏状态（或最后一层的隐藏状态）就是我们所说的上下文向量 CCC。
- **前后双向（Bidirectional）**：为了更好地捕捉上下文，通常会用双向 LSTM，让编码器能同时读"正向"和"反向"的句子，再把它们的隐藏状态拼接起来，以获得更丰富的表征。
- **反转输入**：有些论文（如最初的 Seq2Seq）会把输入序列反转后再喂给编码器，目的是让编码器最后一个看到的词，大致对应目标句子开头需要翻译的地方，从而帮助解码器"更方便地开始翻译"（但在实际实现中，也可用双向 RNN，不一定非要反转输入）。

简单示例：

- 英文句子 "What is your name?"
    - 先把每个词转换为词向量（Embedding），再依次输入 LSTM。
    - 最后得到一个隐藏状态向量 CCC，可被视为"整个句子的大意"。

### 2. 解码器（Decoder）

解码器的任务就是根据这个上下文向量 CCC，一步步生成目标序列（比如翻译成法语句子）。

- **初始化**：用编码器最终的隐藏状态来初始化解码器的第一个隐藏状态。这样"输入句子的语义信息"就能传到解码器里。
- **生成序列**：解码器通常也是一个 RNN（LSTM/GRU）。它会把上一步的输出词（或者在第一步时用特殊的 ⟨GO⟩\langle GO\rangle⟨GO⟩ 起始符）作为输入，再结合当前隐藏状态，输出下一个词的概率分布。选择概率最高的那个词（或者在训练时用教师强制），作为输出。
- **迭代**：将刚输出的词再次送入解码器，持续进行，直到生成结束符 ⟨EOS⟩\langle EOS\rangle⟨EOS⟩ 或达到设定的最大长度。

例如：

- 已得到的译文前半句是 "Comment t'". 下一步解码器会把 "t'" 作为输入，加上隐状态来预测下一个法语单词是 "appelles"，然后继续下一步，直到生成完整句子 "Comment t'appelles tu?"

---

## 三、注意力机制（Attention）

### 1. 为什么需要注意力？

让编码器把整个输入句子"全部信息"都压缩到一个定长向量，其实很难，尤其当句子很长时，信息可能会丢失或难以利用。**注意力机制（Attention）** 的核心思想就是：当解码器要生成目标序列的第 iii 个词时，它可以"动态地去看"输入句子的所有隐藏状态，根据需要"关注"不同部分，而不是只依赖一个全局向量。

### 2. Bahdanau 注意力（举例说明原理）

- **计算对齐（alignment）分数**：对于解码器当前时刻 iii 的隐藏状态 si−1s_{i-1}si−1​，以及输入句子中每个位置 jjj 的编码器隐藏状态 hjh_jhj​，我们定义一个打分函数 ei,j=a(si−1,hj)e_{i,j} = a(s_{i-1}, h_j)ei,j​=a(si−1​,hj​) 它可以是一个小的前馈神经网络输出一个标量分数。
- **归一化得到注意力分布**：对每个 jjj 做 Softmax，得到 αi,j\alpha_{i,j}αi,j​，就像"对齐概率"或"注意力权重"。
- **计算上下文向量**：把输入序列所有隐藏状态加权求和：ci=∑jαi,jhjc_i = \sum_j \alpha_{i,j} h_jci​=j∑​αi,j​hj​
- **将上下文和解码器隐藏状态结合**：最后得到新的解码器隐藏向量，用来预测第 iii 个输出词。

这样一来，解码器在生成每个词时，都能对源句子的不同位置进行"聚焦"，比如在翻译英语"your name"时，注意力更集中在输入的"your name"相关的隐藏状态上。

### 3. Attention 带来的好处

- **更好地处理长句子**：不必把所有信息塞到一个向量里，模型在解码时可以灵活地"查看"输入句子的各个部分。
- **翻译质量更高**：网络能学到类似"对齐"关系。例如英语单词"name"往往对应法语单词"nom"等。
- **可视化可解释性**：我们可以把 αi,j\alpha_{i,j}αi,j​ 的矩阵用热力图画出来，就能看出翻译过程中的对齐关系（对研究很有帮助）。

3blue的transformer视频

Transformer 模型 Q 由 Vaswani 等人在 2017 年提出，最初用于机器翻译任务。与传统的 RNN（循环神经网络）和 LSTM（长短期记忆网络）不同，Transformer 完全基于自注意力机制（Self-Attention Mechanism）实现，并行处理能力更强，训练速度更快。Transformer 的出现彻底改变了 NLP 领域，使得任务的性能显著提升。

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F22%2F03-55-58-dd276138c27da251f8022778663c33e9-202409220355685-852643.png)

## 自注意力机制

以 d_model=1 为例

```Java
## Q K V
Q 是问题，K 是 key，V 是 value
三者相乘得到 Q 的回答 A
自注意力就是 Q、K、V 是相同的

## 输入
我: [1]
爱: [2]
你: [3]

## 注意力分数(Q × K^T)

[我对我的关注度, 我对爱的关注度, 我对你的关注度]
[爱对我的关注度, 爱对爱的关注度, 爱对你的关注度]
[你对我的关注度, 你对爱的关注度, 你对你的关注度]
    softmax处理
[0.6, 0.2, 0.2]
[0.2, 0.6, 0.2]
[0.2, 0.2, 0.6]


## 与V相乘得到输出

[0.6, 0.2, 0.2]   [1]     [1.4]  我视角里的我爱你
[0.2, 0.6, 0.2] × [2] =   [1.8]  爱视角里的我爱你
[0.2, 0.2, 0.6]   [3]     [2.2]  你视角里的我爱你

通过这种方式映射以后，每个词从自我为中心的基础上稍微关注窗口内其他词，用每个词的视角去解读了一遍整个句子，输入是n个词，那么输出就是n个视角里的这句话

```

上面的部分『我爱你』只是变成了序列无关的embedding，但是现实中「我爱你」和「你爱我」其实是不同的，所以在 embedding 的时候还要加入位置编码以反应单词出现顺序的信息

知名大学教授

[@ProfTomYeh](https://x.com/ProfTomYeh)  
 并重新计算查看变化！

千万不要搁那研究 k 是建值，q 是查询，v 是值，如果你看到这种讲解，基本就别看了，那作者自己也没搞明白。

信我一句，把 transformer 和 [GNN](GNN.md)，[GCN](GCN) 放在一起学，你会看到更加本质的东西。

这样你就能理解位置嵌入，不管是正弦还是可学习的嵌入，不管是时间嵌入还是其他先验嵌入。
进而理解什么 [autoformer](https://www.zhihu.com/search?q=autoformer&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，ltransformer，[itransformer](https://www.zhihu.com/search?q=itransformer&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，graphformer，这样你就会看到 [transformer](https://www.zhihu.com/search?q=transformer&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 在多元时序和图上的应用（二者本就一样）

然后你就能明白只要改动注意力计算的方式就能造一个新的 transformer，至于 [多头](https://www.zhihu.com/search?q=%E5%A4%9A%E5%A4%B4&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 和单头，就非常容易理解。而至于什么 [多模态cross attention](https://www.zhihu.com/search?q=%E5%A4%9A%E6%A8%A1%E6%80%81cross%20attention&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，那也就更加显而易见了。
而 [残差](https://www.zhihu.com/search?q=%E6%AE%8B%E5%B7%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 和 norm 只是模型的一种技巧，虽然是小技巧，但实际很有用。
那个 [ffn](https://www.zhihu.com/search?q=ffn&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，则更是不值一提。你就算用 [CNN](https://www.zhihu.com/search?q=CNN&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 去平替，在小问题上也毫无压力。
而至于在 [cv](https://www.zhihu.com/search?q=cv&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 上的使用，其实就是变着法把图像信息变成 [token序列](https://www.zhihu.com/search?q=token%E5%BA%8F%E5%88%97&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)。

后续的自监督方式，则是另一个内容了。
从可解释性上可以试着考虑一下流形，信息论，
encoder 和 decoder 看着玄学，实则简单，建议多往 [流形](https://www.zhihu.com/search?q=%E6%B5%81%E5%BD%A2&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 和信息论上靠一靠。
如果发顶会了记得带我一个名字。

区别于 [Transformers](Transformers.md)
