---
draw:
title: CS224n Natural Language Processing
date created: 2025-01-23
date modified: 2025-05-13
---

| 课程编号  | 主题                                                  | 主要知识点                                                                                                              |                   |
| ----- | --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ | ----------------- |
| 1 & 2 | [[词向量、词嵌入]] (Word Vectors)                              | - 词义表示：one-hot 向量、词义相似度<br>- Word2vec (skip-gram, CBOW)：目标函数、梯度推导、负采样<br>- GloVe：共现矩阵、目标函数<br>- 词向量的评估方法：内在评估、外在评估 | 输入是句子，输出是词向量      |
| 3     | [[神经网络]] (Neural Networks)                          | - 神经网络基础：神经元、激活函数、前向传播、反向传播<br>- 梯度下降：随机梯度下降、小批量梯度下降<br>- 神经网络的训练技巧：正则化、dropout<br>- 神经网络的应用：情感分析                  | 用线性函数+非线性函数拟合一切规律 |
| 4     | [[依存句法分析]] (Dependency Parsing)                     | - 依存句法：依存树、依存关系<br>- 基于转移的依存句法分析：arc-standard 转移系统<br>- 神经网络在依存句法分析中的应用                                            |                   |
| 5     | 循环神经网络[RNN](RNN.md) (Recurrent Neural Networks)     | - RNN 的基本结构：隐藏状态、时间步<br>- RNN 的变体：LSTM、GRU<br>- 语言模型：RNN 语言模型、困惑度<br>- RNN 的应用：命名实体识别                              |                   |
| 6     | 神经机器翻译 ([[Neural Machine Translation]])             | - 序列到序列模型 (Seq2Seq)：[编码器、解码器](编码器、解码器.md)<br>- [[自注意力机制]] (Attention)：点积注意力、缩放点积注意力<br>- Beam search：提高翻译质量的方法     |                   |
| 7     | [[问答系统]] (Question Answering)                       | - 阅读理解式问答：给定文章和问题，找到答案<br>- BiDAF 模型：双向注意力流<br>- 其他问答模型：Match-LSTM、QANet                                           |                   |
| 8     | 卷积神经网络[CNN](CNN.md) (Convolutional Neural Networks) | - CNN 的基本结构：卷积层、池化层<br>- CNN 在 NLP 中的应用：文本分类、句子建模                                                                  |                   |

## 整体思路

这门课的整体设计思路，可以理解为**一步步搭建从语言表示到复杂语言任务的系统化能力**。换句话说，课程是从最基本的"词语如何表示"开始，一路到如何"让模型在复杂任务中理解并生成语言"。最终目标是**帮助学生构建出对自然语言有深层次理解和处理能力的模型**，能够在各种下游 NLP 场景中取得好表现，比如问答、机器翻译、文本分类等。

下面从整体脉络上说明课程为什么要这么安排、以及它想要解决的核心问题：

 
1. **词向量 (课程 1 & 2)**
    - **为什么要学**：语言中最小的可学习单元就是"词"。我们需要一种方法，把"词"映射到向量空间中，让后续神经网络可以处理。
    - **要解决的核心问题**：如何让机器知道单词之间"相似"或"不同"？如何让机器具备最基础的"词义"感知？
    - **关键概念**：one-hot 向量、word2vec、GloVe、评估方法。
    - **作用**：这是 NLP 的**最底层**表示环节，几乎所有后续任务都依赖高质量的词向量。
2. **神经网络基础 (课程 3)**
    - **为什么要学**：有了词向量，下一步就要用模型去"学习、拟合"更复杂的语言规律。神经网络是目前最常用、最有效的处理方法。
    - **要解决的核心问题**：怎么让神经网络学会从数据中更新权重（梯度下降、反向传播），如何防止过拟合（正则化、dropout）？
    - **关键概念**：前向传播、反向传播、训练技巧、激活函数。
    - **作用**：这是现代 NLP 的"通用计算引擎"，为后续任何深度模型打下基础。
3. **依存句法分析 (课程 4)**
    - **为什么要学**：句法结构是语言理解的重要一环。只知道单词含义往往不够，还需要了解它们之间的"句法关系"。
    - **要解决的核心问题**：如何快速、准确地从一个句子里分析出依存树？进而帮助机器理解"谁是主语、谁是宾语、修饰关系如何"等。
    - **关键概念**：转移系统 (arc-standard)、神经网络在句法分析中的使用。
    - **作用**：让模型不仅能"读懂词"，也能"读懂句子结构"。
4. **循环神经网络 RNN (课程 5)**
    - **为什么要学**：语言本质上是序列数据，句子中的词是按时间步一个个出现的。RNN 能捕捉序列信息，尤其在长文本、上下文关联里非常常用。
    - **要解决的核心问题**：如何在一段变长的序列中"保留"上下文信息？如何解决梯度消失和梯度爆炸（LSTM、GRU）？
    - **关键概念**：RNN 结构、LSTM、GRU、语言模型、困惑度。
    - **作用**：为**序列建模**任务奠定基础，包括文本生成、序列标注（如命名实体识别）、语言模型等。
5. **神经机器翻译 (课程 6)**
    - **为什么要学**：机器翻译是 NLP 中最经典、复杂度高的核心任务之一。它需要**序列到序列 (Seq2Seq)** 思想，并引入**注意力机制**来更好地处理长句。
    - **要解决的核心问题**：如何把一句话（源语言）准确地映射为另一种语言的句子？怎样避免信息丢失？如何提升翻译的流畅度和准确度？
    - **关键概念**：编码器-解码器框架、注意力机制 (Attention)、Beam Search。
    - **作用**：通过翻译任务，学生能更全面地理解序列到序列模型和注意力机制，这些思想也可推广到其他生成式或序列映射任务。
6. **问答系统 (课程 7)**
    - **为什么要学**：问答系统是让模型真正"理解文本并作出回答"的典型任务；比翻译更贴近"内容理解"本身。
    - **要解决的核心问题**：给定一段文章和一个问题，怎么准确找到或生成答案？
    - **关键概念**：阅读理解式问答 (machine reading comprehension)、BiDAF、Match-LSTM、QANet 等。
    - **作用**：体现神经网络如何利用注意力、如何多次交互信息来回答问题，是 NLP 实际应用中的关键场景之一。
7. **卷积神经网络 CNN (课程 8)**
    - **为什么要学**：CNN 在图像处理里非常流行，但在文本上同样有用。它可以捕捉**局部 n-gram 特征**，在文本分类、情感分析等场景表现优异。
    - **要解决的核心问题**：如何在 NLP 场景中将卷积操作运用于变长句子？如何通过池化固定输出大小？
    - **关键概念**：卷积核、池化 (max-pooling, k-max)、多通道词向量。
    - **作用**：与 RNN 形成互补。CNN 更适合并行，能快速处理文本，也能学习到局部组合特征，是构建 NLP 模型的另一大主流方案。

---

## 为什么说这些模块共同解决的是"语言理解与生成"这个大问题？

- **多角度的语言表示**：从词向量、依存树到序列模型，课程提供了多种刻画语言的方法。
- **多任务驱动**：依存句法、机器翻译、问答系统等都是 NLP 里最常见、也最能体现"理解与生成"能力的任务。
- **多模型与技巧**：课程围绕神经网络（包括 CNN、RNN、Seq2Seq、注意力）一步步升级，让学生掌握如何从基础结构到复杂应用做端到端的模型设计与实现。

最终，这些**知识点与方法**的组合，正是当下构建**通用 NLP 系统**所需的核心模块。从词向量到深层神经网络，从句法到语义，从理解到生成，为的是让机器不仅能"背单词"，更能"看懂、翻译、回答问题"，即实现对文本的**深度理解和处理**。这正是这门课整体设计的根本目的。

## 相关资料

https://csdiy.wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CS224n/#_1  
官网  
https://web.stanford.edu/class/cs224n/index.html#schedule  
代码  
https://github.com/PKUFlyingPig/CS144-Computer-Network  
[3Blue1Brown](3Blue1Brown.md)的视频  
https://www.youtube.com/watch?v=LPZh9BOjkQs&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5  
课堂笔记  
https://cenleiding.github.io/cs224n%E7%AC%94%E8%AE%B0.html

## Assignment

[[Assignment 1 Exploring Word Vectors]]  
[[Assignment 2 word2vec]]  
[[Assignment 3]]  
[[Assignment 4]]  
[[Assignment 5]]

## 知识点

[[Word2vec]]

### 损失函数

让我来解释这段内容的核心意思，这里在讲神经网络的训练目标：

1. 首先明确两种输入：
- 真实样本："Museums in Paris are amazing"（标记为s）
- 错误样本："Not all museums in Paris"（标记为sc）

1. 初始的训练目标：
- 最大化(s - sc)或最小化(sc - s)
- 也就是希望真实样本的分数比错误样本的分数高

1. 第一次改进：

```python
minimize J = max(sc - s, 0)
```

这样改进的原因是：

- 只有当sc > s时（即错误样本分数高于真实样本）才计算损失
- 当s > sc时（即真实样本分数已经更高）就不需要再训练了
- 这符合我们的直觉：只要真实样本分数更高就行，不需要高太多

1. 第二次改进：

```python
minimize J = max(Δ + sc - s, 0)
```

这次改进增加了一个边界Δ：

- 不只是要求s > sc
- 而是要求s > sc + Δ
- Δ通常取1
- 这就是所谓的"安全边界"（margin of safety）

这种设计类似于SVM（支持向量机）中的间隔概念：

- 不仅要能正确分类
- 还要在正确和错误之间保持一定的"安全距离"
- 这样的模型通常泛化能力更强，更稳定

这就是为什么后面提到这个概念在SVM（支持向量机）研究中经常出现。
