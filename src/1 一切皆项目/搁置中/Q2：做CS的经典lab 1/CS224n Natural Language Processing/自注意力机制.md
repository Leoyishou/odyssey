---
date created: 2025-02-03
date modified: 2025-07-10
uid: 94051886-deab-49b4-9122-4fcb82f99d85
---

#第一性原理

自注意力机制让每个词根据与其他词的相关性灵活聚合信息，是 Transformer 成功的秘诀之一。

`Q=K=V=x` **不是一个简化的假设，而是定义了一种非常重要且常用的注意力类型，叫做“自注意力”（Self-Attention）**。

这里的含义是：

- **查询 (Query, Q)**: 序列中的每个元素（词）都要发出查询：“我应该关注序列中的哪些部分（包括我自己）来更好地理解我？” 这个查询基于元素自身 `x`。
- **键 (Key, K)**: 序列中的每个元素都要提供一个“键”，用来和查询进行匹配。这个键也基于元素自身 `x`。
- **值 (Value, V)**: 序列中的每个元素都要提供一个“值”，这是该元素的实际表示。当一个元素（通过查询 Q）决定要关注另一个元素（通过匹配键 K）时，它实际上会聚合那个元素的“值 V”。这个值也基于元素自身 `x`。

**当你让 Q, K, V 都等于同一个输入 `x` 时，就意味着序列中的每一个元素都在关注（Attend to）这个序列本身的所有元素（包括自己），以计算出自己的新表示。这就是自注意力的核心思想。**

## 步骤3：自注意力机制介绍

理论：自注意力的核心机制。它使每个词能够根据其他词的重要性来更新自身表示。具体来说，对于序列中的每个位置，模型学习三个向量：查询（Query）、键（Key）和值（Value）。注意力的计算分两步：首先计算查询与所有键的相似度（通常用点积表示），然后对值加权求和。通过softmax函数，这些注意力权重会归一化为概率，表示当前词对其他各词的“关注”程度。自注意力可以在O(n²)时间内建立序列中任意两词之间的依赖关系，同时方便并行计算，这是Transformer高效建模长序列的关键。

简单来说，自注意力输出是其他词对当前词的加权影响。Transformer编码器使用自注意力输入自身序列（因此称“自”注意力），而解码器还有类似机制用于关注编码器输出（交叉注意力）。

编程任务：实现缩放点积注意力（Scaled Dot-Product Attention），它是自注意力的基础。公式如下：

$$\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V$$

其中 $Q$ 是查询矩阵，$K$ 是键矩阵，$V$ 是值矩阵，$d_k$ 是键向量维度的缩放因子。我们将使用小矩阵演示注意力计算，并验证注意力权重的性质。

```python
import torch
import math

# 定义一个简化的注意力函数
def scaled_dot_product_attention(Q, K, V):
    # Q, K, V 形状: (batch, seq_len, d_k)
    d_k = Q.size(-1)
    # 计算注意力得分: QK^T / sqrt(d_k)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    # 对最后一个维度应用softmax得到注意力权重
    weights = torch.softmax(scores, dim=-1)  # 形状: (batch, seq_len, seq_len)
    # 加权求和值
    output = torch.matmul(weights, V)  # 形状: (batch, seq_len, d_v), 通常 d_v = d_k
    return output, weights

# 准备模拟输入 (batch=1, seq_len=3, d_k=d_v=4)
Q = torch.tensor([[[1.0, 0.0, 1.0, 0.0],
                   [0.0, 1.0, 0.0, 1.0],
                   [1.0, 1.0, 1.0, 1.0]]])  # 查询向量
K = Q.clone()  # 为简单起见，这里令 K = Q
V = torch.tensor([[[ 1.0,  2.0,  3.0,  4.0],
                   [10.0, 20.0, 30.0, 40.0],
                   [50.0, 60.0, 70.0, 80.0]]])  # 值向量

output, attn_weights = scaled_dot_product_attention(Q, K, V)
print("注意力权重矩阵:\n", attn_weights)
print("注意力输出:\n", output)
```

结果验证：对于上述设置，我们特别选择了简单的 $Q$=$K$ 情况以方便理解。注意力权重

他词（包括自身）的注意力分配概率。由于我们使用softmax，对每一行而言，权重和应为1：

```python
# 验证每个查询位置的注意力权重和为1
assert torch.allclose(attn_weights.sum(dim=-1), torch.tensor([[1.0, 1.0, 1.0]])), "每个位置的注意力权重之和应为1"
```

打印的注意力权重矩阵可能类似：

```Java
注意力权重矩阵:
 tensor([[[0.3199, 0.3199, 0.3602],
          [0.3199, 0.3199, 0.3602],
          [0.3199, 0.3199, 0.3602]]])
```

（这是因为我们示例中三行的 $Q$ 向量恰好都相同，从而产生相同的权重分布；实际情况下不同查询会有不同权重。）

输出 `output` 是注意力加权后的值。例如：

```Java
注意力输出:
 tensor([[[19.0602, 22.0602, 25.0602, 28.0602],
          [19.0602, 22.0602, 25.0602, 28.0602],
          [19.0602, 22.0602, 25.0602, 28.0602]]])
```

可以看到三个输出行也相同。这一示例表明，当查询相同时，注意力产生的输出也会相同，因为它们以相同方式“看待”值。您可以尝试修改 Q 或 V 来观察注意力输出的变化。总之，自注意力机制让每个词根据与其他词的相关性灵活聚合信息，是 Transformer 成功的秘诀之一。

要想更形象、深刻地理解 Transformer，可以先从它的核心思想和整体结构入手，然后再去体会它为什么要用这种结构、它是如何将之前的 RNN/CNN 等结构加以改进的。下面提供一些帮助理解的思路和类比，供你参考：

  

一、核心思想：自注意力（Self-Attention）

Transformer 最突出的创新点就是基于自注意力机制（Self-Attention），它可以让网络在处理序列时，根据内容本身来决定"谁应该更加关注谁"。

你可以把自注意力想象成一个"**同学互相评分**"的场景：在一段文本中，每个"词"就像一位同学，每位同学需要给班里其他同学打分，打分越高表示自己越应该关注那位同学的信息。最后每位同学会结合别人的信息来丰富自己。

- 每个词会计算对所有词的注意力权重，并将这些权重加权地融合进来。
- 注意力权重越大，就表示这个词在当前词的理解里越重要。
- 对比 RNN/CNN：
	- 传统 RNN 是一种顺序阅读文本的方式，当前时刻会依赖前面时刻的隐藏状态，不能很好地捕捉远距离依赖。
	- CNN 也要通过卷积核在局部滑动并多次叠加卷积层，才能逐渐把远距离信息捕捉起来。
- Transformer 直接通过自注意力机制，让每个位置"一次就能看到"序列中所有其他位置的信息（可以并行处理），从而让捕捉全局依赖关系变得高效灵活。

  

二、整体结构：编码器-解码器（Encoder-Decoder）框架

  

最初的 Transformer（如论文《Attention is All You Need》）是为了机器翻译而设计的，使用了Encoder-Decoder的结构。但在很多下游任务中，我们也常常只用 Encoder（例如 BERT）或只用 Decoder（例如 GPT）部分。

- Encoder：把输入序列编码成一个更高级的表示，即把输入序列中的各个位置（词）"翻译"成更能被模型理解的向量。
- Decoder：基于 Encoder 的输出和部分目标序列信息，逐步生成目标序列（翻译、文本生成等）。

  

Encoder 内部的结构：

1. 输入嵌入 + 位置编码：
	- 把每个词或 token 映射到一个向量（Embedding），这是你常见的 " ->[向量]" 的操作。
	- 因为自注意力不再显式依赖 "顺序结构"，所以还要加入"位置编码" 来告诉模型输入序列中每个词的相对或绝对位置。

2. 自注意力层（Multi-Head Self-Attention）：
	- 每个词对整个序列进行"打分"，从而获取上下文信息。
	- 这里的 "Multi-Head" 表示模型会并行使用多个注意力头，每个注意力头都可以"关注"文本里的不同方面（比如某些头专注于语法关系，另一些头专注于语义关联），然后将它们的结果拼接或加和在一起，形成更丰富的表示。

3. 前向反馈网络（Feed-Forward Network, FFN）：
	- 在每个注意力层后面，还会有一个简单的多层感知机（MLP），对每个位置的向量单独地进行非线性变换。这就像是 "对每个词做进一步处理"。

4. 残差连接 + Layer Norm：
	- 跳跃连接（residual）用来缓解深层网络训练困难；Layer Normalization 用来稳定和加快训练收敛。

  

Decoder 内部的结构：

Decoder 的每个层也包含自注意力机制，但分成两个部分：

1. Masked Self-Attention：保证解码时只能看到已生成的部分，防止"剧透"；
2. Encoder-Decoder Attention：向 Encoder 的输出"查询"信息，以便生成当前词时可以参考原文或条件信息；
3. 前向反馈网络：同样用于对解码中的向量做进一步的特征变换。

  

三、分步骤的形象理解

可以把 Transformer 分解为几个"动作"来理解：

1. 输入词向量化：
	- 每个词先变成一个向量，你可以把它理解成描述这个词"语义特征"的一个数字向量。再加上位置编码，让模型知道这是第 1 个词、还是第 2 个词、还是第 10 个词。

2. 自注意力：谁该关注谁？
	- 对于序列中的每个词，生成三个向量：
	- Query（查询），Key（键），Value（值）。
	- 用 Query 和 Key 计算相似度（点积），相似度越大说明词 A 更应该关注词 B。
	- 将相似度（再经过 Softmax）当成加权系数，去对 Value 做加权求和，得到融合了全局信息的新表示。
	- 多个注意力头（Multi-Head）同时进行，这样每个头就像"不同的观察角度"，最后把它们的输出综合起来。

3. 前向网络：深挖信息
	- 把注意力层输出的结果，再经过一个全连接/非线性的网络，就像对每个词的位置做"进一步处理"，萃取更抽象的特征。

4. 层与层之间的残差和归一化
	- 保持上下文梯度流动通畅，避免训练中梯度消失或爆炸，提升稳定性。

5. 不断堆叠：
	- 将若干个这种 "自注意力 + 前向网络 + 残差 + 归一化" 的模块堆叠，就构成了编码器或解码器的大部分主体。

6. （如果是 Decoder）再和 Encoder 交互
	- Decoder 里面专门有一个"Encoder-Decoder Attention"，把 Encoder 的输出当做 Key/Value，把 Decoder 当前层的中间表示当做 Query，让生成端能参考输入信息。

  

四、结合数学公式与直觉

你提到已经理解 MLP 里的  并接激活函数来拟合，这里 Transformer 的 Feed Forward 部分和常规的 MLP 很相似。而它的神来之笔在于自注意力：

1. 自注意力的公式（以单头为例）：

  

其中：

- 是输入序列的向量表示；是可学习的参数矩阵；
- 是缩放因子，防止分母过小或过大导致梯度不稳定；
- 让所有的注意力权重加和为 1。

1. Multi-Head：

把上面这套操作同时做 8 次或更多次，然后再把这些结果拼接起来，通过一个线性变换。这样可以保留多方面的注意力信息。

  

从直觉上来讲，可以理解为"每个位置"在做信息整合之前，先用不同的方式去"看别的位置"、"决定关注度"，然后再把不同视角下的关注结果都合并起来。

  

五、为什么这样做有效？

- 并行性：自注意力让整个序列的处理可以在一个层级并行进行，而不像 RNN 那样顺序依赖，从而极大加快了训练速度和效率。
- 全局关联：每个位置随时都能查看到其他位置的信息，远距离依赖也能够被轻松捕捉。
- 多头机制：让模型可以在不同子空间中学习到不同类型的特征或依赖模式，呈现出比单一注意力头更强的表达能力。

  

六、在脑海里建模 Transformer

  

如果想让这个概念更形象，可以尝试以下几种想象方式：

1. "讨论组"比喻：

- 序列中的每个词是一位组员，每个人都有自己携带的信息（Value），也都有自己能发问（Query）和被询问（Key）的方式。
- 当"讨论"开始时，每个人先根据自己的问题（Query）去看所有其他人的特征（Key），算出相似度来决定"要听谁说话"。
- 最后通过这种"注意力加权"把他人带来的信息"听"进自己这里，更新自己的状态。

1. "看世界"比喻：

- 每个注意力头都像一副特殊的眼镜，它能捕捉到句子/序列中的某些关系（比如语法结构或核心语义）。每个词都会戴上这些眼镜，看看整句话谁最重要，然后加权吸收信息。多头注意力就像戴上了很多副不同风格的眼镜。

1. "全局扫描"对比 "局部扫描"：

- RNN/CNN 更多是"一步步或者局部"去提取信息；
- Transformer 在一个层就可以做"一次全局扫描"。想象你手里有一束光，可以一次就照在所有词上，然后根据他们与当前词之间的重要程度来融合信息。

  

七、学习建议

- 循序渐进地实现小示例：

可以先尝试用一个非常小的 Transformer，甚至手写一个 Self-Attention 层，来看看计算过程。

- 可视化工具：

有一些可视化 Demo（比如 The Illustrated Transformer），能让你直观地看到注意力分布。

- 调试与观察：

在调试过程里打印出注意力矩阵，看看模型到底"关注"了哪些位置。

- 常见的改进版：

比如 BERT、GPT、T5 以及它们的各种变体，都在 Transformer 的基础上进行一些改动或针对性增强，理解它们也有助于更好地加深对 Transformer 原理的感受。

  

总结

  

要形象深刻地理解 Transformer，最关键的是抓住"自注意力机制"这条主线，然后再结合多头注意力 + 前向网络 + 残差连接这几个重要模块去形成完整的心智模型。自注意力让每个位置都能够"看到"整个序列，并且在不同的头中以不同方式关注别的词，从而一次就能得到全局的关联信息。

  

希望这些类比和思路能让你在脑中更轻松地描绘出 Transformer 的工作过程。如果能配合简单的实现、可视化注意力权重、再结合一些小任务来实验，理解会更加深刻。祝学习顺利！

Seq2Seq and Attention Models

Thought for 3m 35s

下面是一份分步骤、通俗易懂的讲解，帮助你理解 CS224n 第六部分（"神经机器翻译、Seq2Seq 与注意力机制"）的主要内容。我会结合一些例子，尽量让概念更加清晰。

---

## 一、背景与动机

### 1. 为什么要研究"序列到序列"（Seq2Seq）模型？

在之前的 NLP 任务中（如命名实体识别、语言模型预测下一个词等），我们大多只需要输出一个单独的标签或词。然而在某些任务中，我们的输出本身也是一个长度不固定的序列，比如：

- **机器翻译**：输入是某种语言的句子，输出是另一种语言的句子。
- **对话系统**：根据上一句话，输出一个回答或回应。
- **文本摘要**：输入可能是长篇文章，输出是简短摘要。

这些任务需要"从一个序列映射到另一个序列"，因此就有了 Seq2Seq 模型。

### 2. 传统机器翻译的历史方法

在深度学习方法普及之前，机器翻译多用**统计机器翻译（SMT）**，比如基于 n-gram 语言模型、词/短语对齐、翻译规则等。然而它们常常无法很好地捕捉长距离依赖或复杂的句子结构，开发和维护也比较复杂。2014 年，基于神经网络的**Seq2Seq** 翻译模型崛起，迅速成为主流。

---

## 二、Seq2Seq 基本原理

### 1. 编码器（Encoder）

Seq2Seq 的编码器主要将输入序列（例如英文句子）"编码"成一个固定长度的向量（也叫"上下文向量" context vector）。这个向量可以被视为对整个输入序列含义的压缩表达。

- **操作方式**：通常使用 RNN（如 LSTM/GRU）把输入序列 {x1,x2,…,xn}\{x_1, x_2, \dots, x_n\}{x1​,x2​,…,xn​} 依次读进去，得到一系列隐藏状态 {h1,h2,…,hn}\{h_1, h_2, \dots, h_n\}{h1​,h2​,…,hn​}，最后一个隐藏状态（或最后一层的隐藏状态）就是我们所说的上下文向量 CCC。
- **前后双向（Bidirectional）**：为了更好地捕捉上下文，通常会用双向 LSTM，让编码器能同时读"正向"和"反向"的句子，再把它们的隐藏状态拼接起来，以获得更丰富的表征。
- **反转输入**：有些论文（如最初的 Seq2Seq）会把输入序列反转后再喂给编码器，目的是让编码器最后一个看到的词，大致对应目标句子开头需要翻译的地方，从而帮助解码器"更方便地开始翻译"（但在实际实现中，也可用双向 RNN，不一定非要反转输入）。

简单示例：

- 英文句子 "What is your name?"
    - 先把每个词转换为词向量（Embedding），再依次输入 LSTM。
    - 最后得到一个隐藏状态向量 CCC，可被视为"整个句子的大意"。

### 2. 解码器（Decoder）

解码器的任务就是根据这个上下文向量 CCC，一步步生成目标序列（比如翻译成法语句子）。

- **初始化**：用编码器最终的隐藏状态来初始化解码器的第一个隐藏状态。这样"输入句子的语义信息"就能传到解码器里。
- **生成序列**：解码器通常也是一个 RNN（LSTM/GRU）。它会把上一步的输出词（或者在第一步时用特殊的 ⟨GO⟩\langle GO\rangle⟨GO⟩ 起始符）作为输入，再结合当前隐藏状态，输出下一个词的概率分布。选择概率最高的那个词（或者在训练时用教师强制），作为输出。
- **迭代**：将刚输出的词再次送入解码器，持续进行，直到生成结束符 ⟨EOS⟩\langle EOS\rangle⟨EOS⟩ 或达到设定的最大长度。

例如：

- 已得到的译文前半句是 "Comment t'". 下一步解码器会把 "t'" 作为输入，加上隐状态来预测下一个法语单词是 "appelles"，然后继续下一步，直到生成完整句子 "Comment t'appelles tu?"

---

## 三、注意力机制（Attention）

### 1. 为什么需要注意力？

让编码器把整个输入句子"全部信息"都压缩到一个定长向量，其实很难，尤其当句子很长时，信息可能会丢失或难以利用。**注意力机制（Attention）** 的核心思想就是：当解码器要生成目标序列的第 iii 个词时，它可以"动态地去看"输入句子的所有隐藏状态，根据需要"关注"不同部分，而不是只依赖一个全局向量。

### 2. Bahdanau 注意力（举例说明原理）

- **计算对齐（alignment）分数**：对于解码器当前时刻 iii 的隐藏状态 si−1s_{i-1}si−1​，以及输入句子中每个位置 jjj 的编码器隐藏状态 hjh_jhj​，我们定义一个打分函数 ei,j=a(si−1,hj)e_{i,j} = a(s_{i-1}, h_j)ei,j​=a(si−1​,hj​) 它可以是一个小的前馈神经网络输出一个标量分数。
- **归一化得到注意力分布**：对每个 jjj 做 Softmax，得到 αi,j\alpha_{i,j}αi,j​，就像"对齐概率"或"注意力权重"。
- **计算上下文向量**：把输入序列所有隐藏状态加权求和：ci=∑jαi,jhjc_i = \sum_j \alpha_{i,j} h_jci​=j∑​αi,j​hj​
- **将上下文和解码器隐藏状态结合**：最后得到新的解码器隐藏向量，用来预测第 iii 个输出词。

这样一来，解码器在生成每个词时，都能对源句子的不同位置进行"聚焦"，比如在翻译英语"your name"时，注意力更集中在输入的"your name"相关的隐藏状态上。

### 3. Attention 带来的好处

- **更好地处理长句子**：不必把所有信息塞到一个向量里，模型在解码时可以灵活地"查看"输入句子的各个部分。
- **翻译质量更高**：网络能学到类似"对齐"关系。例如英语单词"name"往往对应法语单词"nom"等。
- **可视化可解释性**：我们可以把 αi,j\alpha_{i,j}αi,j​ 的矩阵用热力图画出来，就能看出翻译过程中的对齐关系（对研究很有帮助）。

3blue的transformer视频

Transformer 模型 Q 由 Vaswani 等人在 2017 年提出，最初用于机器翻译任务。与传统的 RNN（循环神经网络）和 LSTM（长短期记忆网络）不同，Transformer 完全基于自注意力机制（Self-Attention Mechanism）实现，并行处理能力更强，训练速度更快。Transformer 的出现彻底改变了 NLP 领域，使得任务的性能显著提升。

![image.png|1000](https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F22%2F03-55-58-dd276138c27da251f8022778663c33e9-202409220355685-852643.png)

## 自注意力机制

以 d_model=1 为例

```Java
## Q K V
Q 是问题，K 是 key，V 是 value
三者相乘得到 Q 的回答 A
自注意力就是 Q、K、V 是相同的

## 输入
我: [1]
爱: [2]
你: [3]

## 注意力分数(Q × K^T)

[我对我的关注度, 我对爱的关注度, 我对你的关注度]
[爱对我的关注度, 爱对爱的关注度, 爱对你的关注度]
[你对我的关注度, 你对爱的关注度, 你对你的关注度]
    softmax处理
[0.6, 0.2, 0.2]
[0.2, 0.6, 0.2]
[0.2, 0.2, 0.6]


## 与V相乘得到输出

[0.6, 0.2, 0.2]   [1]     [1.4]  我视角里的我爱你
[0.2, 0.6, 0.2] × [2] =   [1.8]  爱视角里的我爱你
[0.2, 0.2, 0.6]   [3]     [2.2]  你视角里的我爱你

通过这种方式映射以后，每个词从自我为中心的基础上稍微关注窗口内其他词，用每个词的视角去解读了一遍整个句子，输入是n个词，那么输出就是n个视角里的这句话

```

上面的部分『我爱你』只是变成了序列无关的embedding，但是现实中「我爱你」和「你爱我」其实是不同的，所以在 embedding 的时候还要加入位置编码以反应单词出现顺序的信息

知名大学教授

[@ProfTomYeh](https://x.com/ProfTomYeh)  
 并重新计算查看变化！

千万不要搁那研究 k 是建值，q 是查询，v 是值，如果你看到这种讲解，基本就别看了，那作者自己也没搞明白。

信我一句，把 transformer 和 [GNN](GNN.md)，[GCN](GCN) 放在一起学，你会看到更加本质的东西。

这样你就能理解位置嵌入，不管是正弦还是可学习的嵌入，不管是时间嵌入还是其他先验嵌入。

进而理解什么 [autoformer](https://www.zhihu.com/search?q=autoformer&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，ltransformer，[itransformer](https://www.zhihu.com/search?q=itransformer&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，graphformer，这样你就会看到 [transformer](https://www.zhihu.com/search?q=transformer&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 在多元时序和图上的应用（二者本就一样）

然后你就能明白只要改动注意力计算的方式就能造一个新的 transformer，至于 [多头](https://www.zhihu.com/search?q=%E5%A4%9A%E5%A4%B4&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 和单头，就非常容易理解。而至于什么 [多模态cross attention](https://www.zhihu.com/search?q=%E5%A4%9A%E6%A8%A1%E6%80%81cross%20attention&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，那也就更加显而易见了。

而 [残差](https://www.zhihu.com/search?q=%E6%AE%8B%E5%B7%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 和 norm 只是模型的一种技巧，虽然是小技巧，但实际很有用。

那个 [ffn](https://www.zhihu.com/search?q=ffn&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)，则更是不值一提。你就算用 [CNN](https://www.zhihu.com/search?q=CNN&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 去平替，在小问题上也毫无压力。

而至于在 [cv](https://www.zhihu.com/search?q=cv&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 上的使用，其实就是变着法把图像信息变成 [token序列](https://www.zhihu.com/search?q=token%E5%BA%8F%E5%88%97&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D)。

后续的自监督方式，则是另一个内容了。

从可解释性上可以试着考虑一下流形，信息论，

encoder 和 decoder 看着玄学，实则简单，建议多往 [流形](https://www.zhihu.com/search?q=%E6%B5%81%E5%BD%A2&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D) 和信息论上靠一靠。

如果发顶会了记得带我一个名字。

区别于 [Transformers](Transformers.md)
