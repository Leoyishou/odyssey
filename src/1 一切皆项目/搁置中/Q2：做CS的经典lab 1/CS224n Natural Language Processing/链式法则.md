---
comment_id: 310eff08
date created: 2025-01-25
date modified: 2025-01-25
draw: null
tags: []
title: 链式法则
---
从数学的角度来说，**反向传播用到的"链式法则"主要来源于微积分中的多变量求导**，是对**复合函数求导**的一种系统运用。如果你回想一下传统的单变量微积分，可能在学到**"如果 $y = f(g(x))$，那么 $\frac{dy}{dx} = f'(g(x)) \cdot g'(x)$"**这类公式时，就已经接触到了最基础的**链式法则**。但在神经网络中，**我们有多个函数嵌套、且往往是多元函数**，所以会用到多重"链式"乘法。

下面做个简要梳理，帮你回忆和理解：

---

## 1. 单变量链式法则（高中或大学初级微积分）

最简单的形式：如果

$y = f\bigl(u(x)\bigr)$,

那么

$\frac{dy}{dx} = f'(u) \cdot \frac{du}{dx}$

这就是**单变量**的"链式法则"。在高等数学或者 AP Calculus 里，这一部分通常比较早就会学到。

---

## 2. 多变量版本（多元微积分）

神经网络的"前向传播"可以看作一个多层复合函数。例如，一个两层网络可以写成：

$\hat{y} = a^{(2)} = f^{(2)} \Bigl(W^{(2)} \underbrace{f^{(1)}(W^{(1)}x + b^{(1)})}_{\text{隐藏层输出}} + b^{(2)}\Bigr)$

如果损失函数是

$J = \frac{1}{2} \bigl(y - \hat{y}\bigr)^2$,

那么你可以将 $\hat{y}$ 又替换回上面那串复合函数，就得到

$J = \frac{1}{2} \Bigl(y - f^{(2)}\bigl(\dots f^{(1)}(\dots) + b^{(2)}\bigr)\Bigr)^2$

**要对这么一个"大复合函数"求各个参数的导数（梯度），就必须反复运用链式法则**。每一层都可以看成某种"内函数"的组合。所以我们往往写成分步：

$\frac{\partial J}{\partial \theta} = \frac{\partial J}{\partial a^{(2)}} \cdot \frac{\partial a^{(2)}}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial a^{(1)}} \cdot \frac{\partial a^{(1)}}{\partial z^{(1)}} \cdot \dots \cdot \frac{\partial z^{(1)}}{\partial \theta}$

这就是链式法则在多元、多层场景下的**扩展**。

---

## 3. 在神经网络中是怎么体现的？

在神经网络训练时，我们有很多参数（权重和偏置），每个参数对最终损失 $J$ 都有"间接影响"——它并不是直接出现在损失函数里，而是**先影响某一层的激活值，再逐层传递**，最终影响输出 $\hat{y}$，进而影响损失 $J$。

要知道某个权重 $w$ 的微调对损失 $J$ 的影响，就要**一路"倒推"**：

- $w$ 怎么影响本层输入 $z$；
- $z$ 怎么影响本层激活 $a$；
- 本层激活怎么影响下一层输入；
- 直至输出层，再从输出层怎么影响损失。

把这些局部影响（导数）**乘起来**，就得到了 $\frac{\partial J}{\partial w}$。这是典型的"链式法则"思路。

---

## 4. 建议的学习路径

1. **先把单变量微积分的链式法则复习好**：
    - 如果 $y=f(g(x))$，那么 $\frac{dy}{dx}=f'(g(x))\cdot g'(x)$。
    - 这是最简单的情形，先确保你对这条公式背后的推导和直觉非常熟悉。

2. **看看多元函数的导数和梯度概念**：
    - 比如 $z = f(x_1, x_2, \dots)$ 时，$\frac{\partial z}{\partial x_i}$ 是如何定义的。
    - 感受一下向量形式和矩阵形式的写法。其实只要你知道"对每个变量分别求偏导，再用链式法则"，就足够应付神经网络的推导了。

3. **结合神经网络的层结构去体会"复合函数"的层级"**：
    - 任何一层都是前一层输出的函数；
    - 激活函数、线性层(矩阵乘法)、偏置加法……都可以拆分成一小段、一小段，你只要对每一小段的导数公式熟悉，再把它们乘起来，就可以得到最终梯度。

---

## 5. 小结

- 你在神经网络中看到的**误差信号** $\delta$ 以及**梯度**的计算，**本质**就是在做多次"链式法则"的乘积。
- 所以如果你觉得不太理解，可以先回到**大学（或高中）微积分"[[复合函数求导]]"那一块，把**链式法则**重新捋一遍。
- 然后再看神经网络的例子时，就能把每一步都看作："先求对这一步输出的偏导，再乘以对上一步输入的偏导……"。于是，**一点点地把整个传播路线上的微小影响都链起来**，就知道一个参数变化对最终损失的影响了。

简单总结：**反向传播算法 = 多元微积分 + 链式法则** (在多层结构上的系统应用)。这就是它的数学来源与本质所在。
