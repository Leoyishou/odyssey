---
draw:
title: 词向量、词嵌入
date created: 2025-01-29
date modified: 2025-04-07
---

```shell
jupyter notebook exploring_word_vectors.ipynb
```

#思考

低维稠密只是相对于独热编码那种高维稀疏的一个对立，类似于把高纬强行映射成低纬，为了学习起来方便。这种“映射”**不仅仅是为了压缩维度以求方便**，它的**核心目标是通过“可学习”的方式，在这个低维空间中对词语的含义和关系进行编码**，从而让后续的神经网络能够更好地理解和处理语言。方便性是巨大的优势，但**语义表达能力**才是它取代独热编码的根本原因。

## 词向量与词嵌入

这两个概念**非常紧密相关**，在很多情况下可以**互换使用**，但严格来说，“词向量”是一个更广泛的概念。

1. **词向量 (Word Vector)**:
    - 这是一个**通用术语**，指任何用来表示一个单词的**数字向量**。
    - 它可以是任何形式的向量，包括：
        - **独热向量 (One-hot vector)**：维度等于词汇表大小，只有一个 1，其余为 0。
        - **计数向量 (Count vector)**：例如在 Bag-of-Words 模型中，表示词语在文档中出现的次数。
        - **TF-IDF 向量**：考虑词频和逆文档频率的向量。
        - **稠密的、低维的、学习到的向量**：这就是我们通常在深度学习中所说的类型。
2. **词嵌入 (Word Embedding)**:
    - 这通常**特指**那种**稠密的 (dense)、低维的 (low-dimensional)、通过模型从数据中学习到的 (learned)** 词向量。
    - 这种向量的主要目的是**捕捉词语的语义信息**，使得意思相近的词在向量空间中的距离也相近。
    - Word2Vec, GloVe, FastText 以及深度学习模型（如 Transformer 中的 `nn.Embedding` 层）学习到的向量都属于词嵌入。
    - "Embedding" 这个词本身也有“嵌入”或“映射”的意思，强调将离散的词语符号映射到一个连续的向量空间中。

**总结：**

- **所有的词嵌入都是词向量。**
- 但**不是所有的词向量都是（现代意义上的）词嵌入**（比如独热向量就不是）。
- 在现代自然语言处理和深度学习的语境下，当人们提到 "词向量" 时，他们**通常指的就是 "词嵌入"** 这种能够捕捉语义的稠密低维向量。

所以，在日常讨论和实践中，将两者视为**几乎同义**的概念通常是可以的，但要明白 "词向量" 本身是个范围更广的词。

## 可学习 + 稠密向量

1. **克服高维度缺点**：
    - 你理解得很对。如果词汇表有 1 万个字（单词），用独热编码 (one-hot) 表示，每个字就需要一个 1 万维的向量，其中只有一个位置是 1，其他全是 0。这确实维度非常高，计算和存储效率低下。
    - "低维" 指的就是，我们不再使用 1 万维，而是选择一个**人为指定的、小得多**的维度，比如 100维、300维 或 768维（在 Transformer 模型中常见）。这就大大降低了维度。
2. **克服无法表达词语关系缺点**：
    - 独热编码的另一个大问题是，任何两个不同的词对应的向量都是**正交**的（它们的点积为 0）。这意味着从向量本身看不出 "猫" 和 "狗" 的关系比 "猫" 和 "桌子" 的关系更近。
    - "稠密向量 (dense vector)" 指的是，这个低维向量中的**大部分元素都不是 0**。
    - 最关键的是 **"可学习 (learnable)"**：这**不是**指我们对 1 万维的独热向量做 PCA 降维。虽然 PCA 也是一种降维方法，但它通常是无监督的，并且直接对独热向量做 PCA 效果并不好，因为它丢失了信息且难以捕捉语义。

**“可学习”的真正含义是：**
- 我们创建的嵌入层 (`nn.Embedding`) 内部的那个**低维权重矩阵**（比如 10000 行 x 300 列）本身就是模型的**参数**。
- 在**模型训练**的过程中（比如训练一个语言模型、文本分类器等），这些嵌入向量的值会通过**反向传播**算法，根据模型在任务上的表现（损失函数）**不断地被调整和优化**。
- 模型会**自动学习**到如何为每个词分配一个好的低维向量，使得**意思相近、用法相似**的词（比如 "国王" 和 "女王"，或者 "跑" 和 "走"）在向量空间中的**距离也比较近**。而意思无关的词（比如 "国王" 和 "香蕉"）向量距离会比较远。

**总结：**

- "低维稠密向量" 指的是用维度小得多（如 300维）且大部分值非零的向量来表示单词，解决了独热编码维度过高和稀疏的问题。
- "可学习" 指的是这些低维向量的值不是通过固定的降维算法（如 PCA）直接从独热编码转换来的，而是作为模型参数，在训练过程中根据任务目标**自动学习和优化**得到的，目的是让这些向量能够**蕴含词语的语义信息**。

所以，它更像是模型在训练中自己学会了一个从 "单词索引" 到 "有意义的低维向量" 的**智能映射**，而不是简单地对原始高维表示做压缩。

## 词如何变成向量？

### 方法一：[[共现矩阵]]

1. 收集所有的文本成为 `List<word>`
2. 然后去重成 `Set<word>`并排序
3. 然后把 `Set<word>` 作为共生矩阵的横轴和纵轴，设定窗口大小后生成共生矩阵的内容，
4. 然后做降维，然后画图

### 方法二：预测式词向量

[[GloVe]]

## 多义词与同音词

多义词 Polysemes 的不同含义之间有逻辑关联，可以追溯到相同的语源

  1. "head" (头)
     - 身体的头部："He nodded his head"
     - 领导者："She is the head of the department"
     - 物体的顶部："the head of the mountain"
  2. "paper"（纸）
     - 材料："a sheet of paper"
     - 报纸："the morning paper"
     - 学术论文："published a paper"
     

同音词 Homonyms 的不同含义之间没有关联，通常来自不同的语源，只是碰巧拼写或发音相同

- 举例：
  1. "bank"
     - 银行："I went to the bank to deposit money"
     - 河岸："sitting on the river bank"
  2. "rose"
     - 玫瑰花："a red rose"
     - rise的过去式："The sun rose in the east"

## 类比

这段代码是在演示如何使用词向量（word vectors）来解决类比问题。具体来说：

1. 问题设置：
- 给定类比 "man: king:: woman: x"
- 意思是："男人之于国王，如同女人之于 x"
- 目标是找出 x 应该是什么词

1. 解决方法：
- 使用 `most_similar` 函数
- 设置 positive=['woman', 'king']和 negative=['man']
- 这相当于在计算：king - man + woman = x
- 函数返回与计算结果最相似的词及其相似度分数

1. 结果解读：
- 'queen'（女王）得分最高（0.697）
- 这符合直觉：男人对应国王，女人对应女王
- 其他相关词如 'princess'（公主）、'monarch'（君主）等也都在列表中

这个例子展示了词向量能够捕捉词语之间的语义关系，不仅仅是简单的相似性，还包括更复杂的类比关系。这是词向量一个很有趣的特性，说明它们在某种程度上编码了语言中的语义结构。

这种技术在自然语言处理中很有用，可以用于：

- 自动问答系统
- 语义搜索
- 机器翻译
- 文本理解等任务

## 词向量中的 bias

**（1）数据本身带有偏见**  
词向量（无论是 GloVe、word2vec、还是更先进的模型）都需要在大规模语料上进行训练。如果我们所收集的海量文本（如维基百科、新闻、社交网络等）本身就反映了人类社会的刻板印象或不平衡现象，比如"nurse（护士）和女性"同时出现频率高，而"nurse（护士）和男性"一起出现频率较低，这些"共现频率"的差异最终会在词向量里留下烙印。因此，即使训练算法是中立的，一旦语料本身偏向某种群体、或呈现不平等的语言分布，就会将偏见嵌入模型。

**（2）模型优化目标强化了数据趋势**  
训练词向量的目标通常是让模型能"预测上下文"或"重构共现概率"等。换句话说，只要某些词之间在语料里经常一起出现，模型就会把它们向量拉近；如果不常一起出现，就会拉远。这样一来，社会惯用说法、媒体报道习惯等，会不断被放大并固化在词向量空间里。比如，如果语料中"boss（老板）"多半出现于"he" 的上下文，而较少出现在 "she" 的上下文，那么最终的向量空间也会将 boss 与男性词之间的距离拉近。

在下游任务里，如果直接拿这些词向量来做例如"招聘推荐""自动生成文本""情感分析"等时，模型可能会继承甚至放大这些偏见，从而导致歧视性结果。如：

- 自动翻译可能将中性的"医生"翻译成"He is a doctor"，而"护士"翻译成"She is a nurse"；
- 职位推荐系统更容易把管理岗职位推荐给男性用户，而不是女性。

因此，不仅仅是学术研究，也有越来越多的工业应用场景里需要意识到并处理这种偏见。

1. **对它的存在保持警觉**
    
    - 首先要承认：词向量学到的并不是"纯粹的语言知识"，而是人类社会使用语言过程中所存在的方方面面，包括历史和现实的刻板印象。
    - 一旦要在实际项目中用到这些向量，就需要仔细审视是否会引发潜在的歧视或不公平问题。
2. **数据采样与多样性**
    
    - 如果我们的语料过于单一，或过分偏向某个特定群体，那么偏见就更容易被放大。
    - 尝试从更多元化的文本来源获取数据，可以在一定程度上减少偏见（但并不能完全消除）。
3. **偏见检测与去偏（Debiasing）算法**
    
    - 对于"gender bias"，学术界已有一些方法来探测并"移除"向量中最显著的性别维度，从而减少对性别二分的过度关联（如 [Bolukbasi 等人在 2016 年提出的去偏方法](https://arxiv.org/abs/1607.06520)）。
    - 除了传统的词向量，现在对 BERT、GPT 等大型预训练语言模型，社区里也有一系列研究在探讨如何评测和缓解模型中的歧视与偏见。
4. **透明度与责任追究**
    
    - 在上线应用之前，需要让使用方或决策者清楚了解模型可能带来的歧视风险，并提供必要的审计与反馈渠道。
    - 在研究层面，也需要在论文或报告中披露训练数据来源、可能存在的偏见，从而让同行与公众更好地评估和应对。

在 CS224N 或相关 NLP 课程里，看到词向量的偏见并不是一个"偶然的 bug"，而是提醒我们：**自然语言处理算法背后的数据与社会现状息息相关**。通过这些实验与可视化，可以帮助学生意识到语言模型绝不是中性的。对这种 bias 要**积极地洞察、测试、审查和改进**，而不是盲目将模型当作"纯粹客观"的工具。这是当前 NLP 和 AI 领域重要的研究方向之一，也关乎社会公正与技术伦理。
