---
draw:
tags: []
title: 词向量
date created: 2025-01-29
date modified: 2025-01-29
---

```shell
jupyter notebook exploring_word_vectors.ipynb
```

## 词如何变成向量？

### 方法一：[[共现矩阵]]

1. 收集所有的文本成为 `List<word>`
2. 然后去重成 `Set<word>`并排序
3. 然后把 `Set<word>` 作为共生矩阵的横轴和纵轴，设定窗口大小后生成共生矩阵的内容，
4. 然后做降维，然后画图

### 方法二：预测式词向量

[[GloVe]]

## 多义词与同音词

多义词 Polysemes 的不同含义之间有逻辑关联，可以追溯到相同的语源

  1. "head" (头)
     - 身体的头部："He nodded his head"
     - 领导者："She is the head of the department"
     - 物体的顶部："the head of the mountain"
  2. "paper"（纸）
     - 材料："a sheet of paper"
     - 报纸："the morning paper"
     - 学术论文："published a paper"
     

同音词 Homonyms 的不同含义之间没有关联，通常来自不同的语源，只是碰巧拼写或发音相同

- 举例：
  1. "bank"
     - 银行："I went to the bank to deposit money"
     - 河岸："sitting on the river bank"
  2. "rose"
     - 玫瑰花："a red rose"
     - rise的过去式："The sun rose in the east"

## 类比

这段代码是在演示如何使用词向量（word vectors）来解决类比问题。具体来说：

1. 问题设置：
- 给定类比 "man: king:: woman: x"
- 意思是："男人之于国王，如同女人之于 x"
- 目标是找出 x 应该是什么词

1. 解决方法：
- 使用 `most_similar` 函数
- 设置 positive=['woman', 'king']和 negative=['man']
- 这相当于在计算：king - man + woman = x
- 函数返回与计算结果最相似的词及其相似度分数

1. 结果解读：
- 'queen'（女王）得分最高（0.697）
- 这符合直觉：男人对应国王，女人对应女王
- 其他相关词如 'princess'（公主）、'monarch'（君主）等也都在列表中

这个例子展示了词向量能够捕捉词语之间的语义关系，不仅仅是简单的相似性，还包括更复杂的类比关系。这是词向量一个很有趣的特性，说明它们在某种程度上编码了语言中的语义结构。

这种技术在自然语言处理中很有用，可以用于：

- 自动问答系统
- 语义搜索
- 机器翻译
- 文本理解等任务

## 词向量中的 bias

**（1）数据本身带有偏见**  
词向量（无论是 GloVe、word2vec、还是更先进的模型）都需要在大规模语料上进行训练。如果我们所收集的海量文本（如维基百科、新闻、社交网络等）本身就反映了人类社会的刻板印象或不平衡现象，比如"nurse（护士）和女性"同时出现频率高，而"nurse（护士）和男性"一起出现频率较低，这些"共现频率"的差异最终会在词向量里留下烙印。因此，即使训练算法是中立的，一旦语料本身偏向某种群体、或呈现不平等的语言分布，就会将偏见嵌入模型。

**（2）模型优化目标强化了数据趋势**  
训练词向量的目标通常是让模型能"预测上下文"或"重构共现概率"等。换句话说，只要某些词之间在语料里经常一起出现，模型就会把它们向量拉近；如果不常一起出现，就会拉远。这样一来，社会惯用说法、媒体报道习惯等，会不断被放大并固化在词向量空间里。比如，如果语料中"boss（老板）"多半出现于"he" 的上下文，而较少出现在 "she" 的上下文，那么最终的向量空间也会将 boss 与男性词之间的距离拉近。

在下游任务里，如果直接拿这些词向量来做例如"招聘推荐""自动生成文本""情感分析"等时，模型可能会继承甚至放大这些偏见，从而导致歧视性结果。如：

- 自动翻译可能将中性的"医生"翻译成"He is a doctor"，而"护士"翻译成"She is a nurse"；
- 职位推荐系统更容易把管理岗职位推荐给男性用户，而不是女性。

因此，不仅仅是学术研究，也有越来越多的工业应用场景里需要意识到并处理这种偏见。

1. **对它的存在保持警觉**
    
    - 首先要承认：词向量学到的并不是"纯粹的语言知识"，而是人类社会使用语言过程中所存在的方方面面，包括历史和现实的刻板印象。
    - 一旦要在实际项目中用到这些向量，就需要仔细审视是否会引发潜在的歧视或不公平问题。
2. **数据采样与多样性**
    
    - 如果我们的语料过于单一，或过分偏向某个特定群体，那么偏见就更容易被放大。
    - 尝试从更多元化的文本来源获取数据，可以在一定程度上减少偏见（但并不能完全消除）。
3. **偏见检测与去偏（Debiasing）算法**
    
    - 对于"gender bias"，学术界已有一些方法来探测并"移除"向量中最显著的性别维度，从而减少对性别二分的过度关联（如 [Bolukbasi 等人在 2016 年提出的去偏方法](https://arxiv.org/abs/1607.06520)）。
    - 除了传统的词向量，现在对 BERT、GPT 等大型预训练语言模型，社区里也有一系列研究在探讨如何评测和缓解模型中的歧视与偏见。
4. **透明度与责任追究**
    
    - 在上线应用之前，需要让使用方或决策者清楚了解模型可能带来的歧视风险，并提供必要的审计与反馈渠道。
    - 在研究层面，也需要在论文或报告中披露训练数据来源、可能存在的偏见，从而让同行与公众更好地评估和应对。

在 CS224N 或相关 NLP 课程里，看到词向量的偏见并不是一个"偶然的 bug"，而是提醒我们：**自然语言处理算法背后的数据与社会现状息息相关**。通过这些实验与可视化，可以帮助学生意识到语言模型绝不是中性的。对这种 bias 要**积极地洞察、测试、审查和改进**，而不是盲目将模型当作"纯粹客观"的工具。这是当前 NLP 和 AI 领域重要的研究方向之一，也关乎社会公正与技术伦理。
