---
draw:
title: 微调
date created: 2024-10-23
date modified: 2025-02-24
---

两者在基本思路上有相似之处，都是在预训练模型的基础上进行微调，使模型更适合特定任务。但细节上有明显差异：

| 路线              |                                                                                                                                                                                                         |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [SFT](SFT.md)   | - **OpenAI 的微调模块**：<br>    通常采用监督学习的方式，你上传标注好的 JSONL 数据，然后选择一个基座模型，系统会利用这些标注数据对模型进行微调，调整模型的参数使其更适应你提供的训练数据。<br>                                                                                         |
| [[GRPOTrainer]] | - **[[GRPOTrainer]]-DeepseekR1 的方法**：<br>    这里使用的是强化学习策略优化（Group Relative Policy Optimization，GRPO）的方法。训练过程中不仅依赖监督信号，还引入了奖励函数（例如格式奖励、准确率奖励和编辑距离奖励）来评估生成结果，然后通过策略优化来调整模型。这种方法在生成任务中可以更灵活地控制输出格式和内容。<br> |

总结来说，虽然两者都是基于预训练模型的微调，但 OpenAI 的方法更偏向于传统的监督微调，而 GRPOTrainer 则结合了强化学习的策略优化，使得微调过程中可以直接用生成质量的反馈来调整模型。

___

https://github.com/huggingface/open-r1?tab=readme-ov-file

这个项目的目标是公开复现 DeepSeek-R1 模型，也就是说，它提供了一整套代码和工具，让大家可以按照公开的流程来微调和评估这个模型。简单来说：

- **复现 DeepSeek-R1**：项目重现了 DeepSeek-R1 的整个训练和微调过程，包括用监督学习（SFT）和基于强化学习的 GRPO 方法来改进模型表现。
- **数据处理与生成**：它包含了数据预处理、数据生成（用来扩充训练数据）以及评估脚本，方便大家验证模型在数学推理、代码生成等任务上的表现。
- **开放与可扩展**：整个项目是开源的，目的是让更多人可以在这个基础上进行改进和二次开发，推动相关研究和应用。

换句话说，这个项目不是在从零开始训练一个模型，而是复现并改进已有的 DeepSeek-R1，通过公开的代码让大家都能用相同的方法来微调和评估模型。

___

## 参考资料

https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing

[hiyouga/LLaMA-Factory/LLaMA-Factory: 一键调用LLaMA-Factory ，轻松微调。集成多种辅助功能。 - CG (codewithgpu.com)](https://www.codewithgpu.com/i/hiyouga/LLaMA-Factory/LLaMA-Factory)

我来为您解释一下如何微调(fine-tune) Llama 模型，我会尽量用通俗的语言解释这个过程。

作为 Java 程序员转入 AI 领域，您需要了解以下几个关键概念和步骤:

1. 基本概念:
- 微调是指在预训练模型的基础上,用特定数据集进行进一步训练,使模型更适合特定任务
- LoRA(Low-Rank Adaptation)是一种高效的微调方法,只需要更新很小一部分参数(1-10%)
- 在这个例子中使用了 Unsloth 框架,它可以让训练速度提升 2 倍

1. 主要步骤:

a) 环境准备:

```python
# 安装必要的库
!pip install unsloth
```

b) 加载模型:

```python
from unsloth import FastLanguageModel

# 加载预训练模型
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct",  # 选择模型
    max_seq_length = 2048,  # 最大序列长度
    load_in_4bit = True     # 使用4位量化以减少内存使用
)
```

c) 准备训练数据:

- 需要准备对话格式的数据,包含用户输入(user)和助手回答(assistant)
- 数据需要转换成模型可以理解的格式

d) 配置训练参数:

```python
# 设置 LoRA 参数
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,                # LoRA 秩,建议值 8,16,32,64,128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                     "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 16,
    lora_dropout = 0
)
```

e) 开始训练:

```python
from trl import SFTTrainer

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text"
)

trainer.train()
```

f) 保存模型:

```python
# 本地保存
model.save_pretrained("lora_model")
tokenizer.save_pretrained("lora_model")
```

1. 实用建议:
- 建议使用 Google Colab 等云平台进行训练,因为需要 GPU 资源
- 开始时可以用小数据集测试流程
- 确保训练数据质量,这对模型效果影响很大
- 可以先用较小的模型(如 1B 或 3B 参数量的版本)来测试
- 注意保存训练检查点,以防训练中断

1. 资源消耗参考:
- 使用 Tesla T4 GPU 训练时,峰值内存使用约 6.5GB
- 完整训练时间因数据集大小而异,示例中约 7.5 分钟

如果您想开始尝试:

1. 先在 Google Colab 上运行示例代码
2. 准备一个小的测试数据集
3. 逐步调整参数观察效果
4. 成功后再尝试使用自己的数据集进行训练

需要注意的是,虽然示例使用 Python,但您不需要精通 Python,只需要能看懂基本语法并修改关键参数即可。随着您的深入学习,可以逐步掌握更多 AI 相关知识。
