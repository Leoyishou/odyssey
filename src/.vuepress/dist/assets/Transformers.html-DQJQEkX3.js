import{_ as l}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,a as i,g as r,b as n,d as t,e as p,f as h,r as o,o as k}from"./app-DokaGNO4.js";const d={};function m(A,s){const a=o("RouteLink");return k(),e("div",null,[s[8]||(s[8]=i("p",null,"架构与实现：",-1)),s[9]||(s[9]=i("ul",null,[i("li",null,"Transformer 是一种架构设计。"),i("li",null,"Transformers 库提供了这种架构（及其变体）的实现，以及基于此架构的多种模型。")],-1)),r(" more "),s[10]||(s[10]=i("p",null,"Transformer 和 Transformers 看起来非常相似，但它们实际上指的是不同的概念。让我为您解释它们的关系：",-1)),i("ol",null,[i("li",null,[s[6]||(s[6]=i("p",null,"Transformer:",-1)),i("ul",null,[s[3]||(s[3]=i("li",null,"这是一种特定的神经网络架构。",-1)),s[4]||(s[4]=i("li",null,'由 Google 在 2017 年的论文 "Attention Is All You Need" 中提出。',-1)),i("li",null,[s[1]||(s[1]=n("主要特点是使用 ")),t(a,{to:"/2%20%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/1%20%E8%8A%82%E7%82%B9/CS/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.html"},{default:p(()=>s[0]||(s[0]=[n("自注意力机制")])),_:1}),s[2]||(s[2]=n("（self-attention）来处理序列数据。"))]),s[5]||(s[5]=i("li",null,"原始设计用于自然语言处理任务，特别是机器翻译。",-1))])]),s[7]||(s[7]=i("li",null,[i("p",null,"Transformers:"),i("ul",null,[i("li",null,"这是一个由 Hugging Face 开发的流行 Python 库。"),i("li",null,"库名是 Transformer 的复数形式。"),i("li",null,"提供了许多预训练的模型实现，包括但不限于基于 Transformer 架构的模型。"),i("li",null,"包括 BERT, GPT, RoBERTa 等多种模型的实现。")])],-1))]),s[11]||(s[11]=h(`<p>关系：</p><ol><li><p>架构与实现：</p><ul><li>Transformer 是一种架构设计。</li><li>Transformers 库提供了这种架构（及其变体）的实现，以及基于此架构的多种模型。</li></ul></li><li><p>范围：</p><ul><li>Transformer 指的是特定的模型架构。</li><li>Transformers 库包含了更广泛的模型和工具。</li></ul></li><li><p>使用方式：</p><ul><li>要直接使用 Transformer 架构，你需要自己实现或使用低级库。</li><li>Transformers 库提供了高级 API，使得使用和微调预训练模型变得简单。</li></ul></li><li><p>发展：</p><ul><li>Transformer 是一个固定的概念，虽然有一些变体。</li><li>Transformers 库不断更新，增加新的模型和功能。</li></ul></li><li><p>命名由来：</p><ul><li>Transformers 库之所以用复数形式，是因为它包含多个基于 Transformer 架构的模型。</li></ul></li></ol><p>示例代码对比：</p><ol><li><p>使用原始 Transformer 架构（简化版）：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.nn </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">as</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> nn</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">class</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> Transformer</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">nn</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">Module</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">)</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    def</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> __init__</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E5C07B;--shiki-dark-font-style:italic;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#ABB2BF;"> ...</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        super</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">().</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">__init__</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # 实现Transformer架构</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">        ...</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> Transformer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">...</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>使用 Hugging Face 的 Transformers 库：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> transformers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> AutoModel, AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_name </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;bert-base-uncased&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> AutoModel.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">tokenizer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> AutoTokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol><p>总结：Transformer 是一种模型架构，而 Transformers 是一个提供了多种基于 Transformer（及其他）架构的预训练模型的库。这个库大大简化了使用和微调这些强大模型的过程。</p>`,5))])}const u=l(d,[["render",m],["__file","Transformers.html.vue"]]),y=JSON.parse('{"path":"/2%20%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/1%20%E8%8A%82%E7%82%B9/CS/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Transformers.html","title":"Transformers","lang":"zh-CN","frontmatter":{"draw":null,"tags":[],"title":"Transformers","date created":"2024-07-18T00:00:00.000Z","date modified":"2024-11-12T00:00:00.000Z","description":"架构与实现： Transformer 是一种架构设计。 Transformers 库提供了这种架构（及其变体）的实现，以及基于此架构的多种模型。 Transformer 和 Transformers 看起来非常相似，但它们实际上指的是不同的概念。让我为您解释它们的关系： Transformer: 这是一种特定的神经网络架构。 由 Google 在 20...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/2%20%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/1%20%E8%8A%82%E7%82%B9/CS/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Transformers.html"}],["meta",{"property":"og:site_name","content":"转了码的刘公子"}],["meta",{"property":"og:title","content":"Transformers"}],["meta",{"property":"og:description","content":"架构与实现： Transformer 是一种架构设计。 Transformers 库提供了这种架构（及其变体）的实现，以及基于此架构的多种模型。 Transformer 和 Transformers 看起来非常相似，但它们实际上指的是不同的概念。让我为您解释它们的关系： Transformer: 这是一种特定的神经网络架构。 由 Google 在 20..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-11-24T16:17:22.000Z"}],["meta",{"property":"article:modified_time","content":"2024-11-24T16:17:22.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Transformers\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-11-24T16:17:22.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"转了码的刘公子\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[],"git":{"createdTime":1732465042000,"updatedTime":1732465042000,"contributors":[{"name":"Luis","email":"liuysh20@gmail.com","commits":1}]},"readingTime":{"minutes":1.83,"words":548},"filePathRelative":"2 第二大脑/1 节点/CS/人工智能/Transformers.md","localizedDate":"2024年11月25日","autoDesc":true}');export{u as comp,y as data};
