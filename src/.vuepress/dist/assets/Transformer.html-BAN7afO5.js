import{_ as h}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as p,f as n,a,b as i,d as r,e as t,r as l,o as k}from"./app-DokaGNO4.js";const d={};function o(c,s){const e=l("RouteLink");return k(),p("div",null,[s[7]||(s[7]=n(`<p>3blue的transformer视频</p><p>Transformer 模型 Q 由 Vaswani 等人在 2017 年提出，最初用于机器翻译任务。与传统的 RNN（循环神经网络）和 LSTM（长短期记忆网络）不同，Transformer 完全基于自注意力机制（Self-Attention Mechanism）实现，并行处理能力更强，训练速度更快。Transformer 的出现彻底改变了 NLP 领域，使得任务的性能显著提升。</p><figure><img src="https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F22%2F03-55-58-dd276138c27da251f8022778663c33e9-202409220355685-852643.png" alt="image.png|1000" tabindex="0" loading="lazy"><figcaption>image.png|1000</figcaption></figure><h2 id="自注意力机制" tabindex="-1"><a class="header-anchor" href="#自注意力机制"><span>自注意力机制</span></a></h2><p>以 d_model=1 为例</p><div class="language-java line-numbers-mode" data-highlighter="shiki" data-ext="java" data-title="java" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">## </span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">Q</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> K</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> V</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">Q 是问题，K 是 key，V 是 value</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">三者相乘得到 Q 的回答 </span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">A</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">自注意力就是 Q、K、V 是相同的</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">## 输入</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">我</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">:</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">爱</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">:</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">你</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">:</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">## 注意力分数(Q × K</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">^</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">T)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">[我对我的关注度</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> 我对爱的关注度</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> 我对你的关注度]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">[爱对我的关注度</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> 爱对爱的关注度</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> 爱对你的关注度]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">[你对我的关注度</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> 你对爱的关注度</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> 你对你的关注度]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">    softmax处理</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.6</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.6</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.6</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">## 与V相乘得到输出</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.6</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]   [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]     [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1.4</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]  我视角里的我爱你</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.6</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">] × [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">   [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1.8</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]  爱视角里的我爱你</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.6</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]   [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]     [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2.2</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">]  你视角里的我爱你</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">通过这种方式映射以后，每个词从自我为中心的基础上稍微关注窗口内其他词，用每个词的视角去解读了一遍整个句子，输入是n个词，那么输出就是n个视角里的这句话</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>上面的部分『我爱你』只是变成了序列无关的embedding，但是现实中「我爱你」和「你爱我」其实是不同的，所以在 embedding 的时候还要加入位置编码以反应单词出现顺序的信息</p><p>知名大学教授</p><p><a href="https://x.com/ProfTomYeh" target="_blank" rel="noopener noreferrer">@ProfTomYeh</a></p><p>使用 Google 表格的可编程能力从零实现了一个「Transformer」模型！<img src="https://abs-0.twimg.com/emoji/v2/svg/1f92f.svg" alt="🤯" title="Exploding head" loading="lazy"> <a href="https://t.co/1ILGSnbVM1" target="_blank" rel="noopener noreferrer">https://by-hand.ai/sp/tfmr</a> 此电子表格引发了几万次副本拷贝，还有配套视频教程，支持调参、了解注意力、矩阵变换、配套练习！<img src="https://abs-0.twimg.com/emoji/v2/svg/1f525.svg" alt="🔥" title="Fire" loading="lazy"> 并重新计算查看变化！</p><p>千万不要搁那研究 k 是建值，q 是查询，v 是值，如果你看到这种讲解，基本就别看了，那作者自己也没搞明白。</p>`,11)),a("p",null,[s[1]||(s[1]=i("信我一句，把 transformer 和 ")),r(e,{to:"/2%20%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/1%20%E8%8A%82%E7%82%B9/CS/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/GNN.html"},{default:t(()=>s[0]||(s[0]=[i("GNN")])),_:1}),s[2]||(s[2]=i("，")),s[3]||(s[3]=a("a",{href:"GCN"},"GCN",-1)),s[4]||(s[4]=i(" 放在一起学，你会看到更加本质的东西。"))]),s[8]||(s[8]=n('<p>这样你就能理解位置嵌入，不管是正弦还是可学习的嵌入，不管是时间嵌入还是其他先验嵌入。 进而理解什么 <a href="https://www.zhihu.com/search?q=autoformer&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D" target="_blank" rel="noopener noreferrer">autoformer</a>，ltransformer，<a href="https://www.zhihu.com/search?q=itransformer&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D" target="_blank" rel="noopener noreferrer">itransformer</a>，graphformer，这样你就会看到 <a href="https://www.zhihu.com/search?q=transformer&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D" target="_blank" rel="noopener noreferrer">transformer</a> 在多元时序和图上的应用（二者本就一样）</p><p>然后你就能明白只要改动注意力计算的方式就能造一个新的 transformer，至于 <a href="https://www.zhihu.com/search?q=%E5%A4%9A%E5%A4%B4&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D" target="_blank" rel="noopener noreferrer">多头</a> 和单头，就非常容易理解。而至于什么 <a href="https://www.zhihu.com/search?q=%E5%A4%9A%E6%A8%A1%E6%80%81cross%20attention&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D" target="_blank" rel="noopener noreferrer">多模态cross attention</a>，那也就更加显而易见了。 而 <a href="https://www.zhihu.com/search?q=%E6%AE%8B%E5%B7%AE&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D" target="_blank" rel="noopener noreferrer">残差</a> 和 norm 只是模型的一种技巧，虽然是小技巧，但实际很有用。 那个 <a href="https://www.zhihu.com/search?q=ffn&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D" target="_blank" rel="noopener noreferrer">ffn</a>，则更是不值一提。你就算用 <a href="https://www.zhihu.com/search?q=CNN&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D" target="_blank" rel="noopener noreferrer">CNN</a> 去平替，在小问题上也毫无压力。 而至于在 <a href="https://www.zhihu.com/search?q=cv&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D" target="_blank" rel="noopener noreferrer">cv</a> 上的使用，其实就是变着法把图像信息变成 <a href="https://www.zhihu.com/search?q=token%E5%BA%8F%E5%88%97&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D" target="_blank" rel="noopener noreferrer">token序列</a>。</p><p>后续的自监督方式，则是另一个内容了。 从可解释性上可以试着考虑一下流形，信息论， encoder 和 decoder 看着玄学，实则简单，建议多往 <a href="https://www.zhihu.com/search?q=%E6%B5%81%E5%BD%A2&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3272070260%7D" target="_blank" rel="noopener noreferrer">流形</a> 和信息论上靠一靠。 如果发顶会了记得带我一个名字。</p>',3)),a("p",null,[s[6]||(s[6]=i("区别于 ")),r(e,{to:"/2%20%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/1%20%E8%8A%82%E7%82%B9/CS/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Transformers.html"},{default:t(()=>s[5]||(s[5]=[i("Transformers")])),_:1})])])}const m=h(d,[["render",o],["__file","Transformer.html.vue"]]),y=JSON.parse('{"path":"/2%20%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/1%20%E8%8A%82%E7%82%B9/CS/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Transformer.html","title":"Transformer","lang":"zh-CN","frontmatter":{"draw":null,"tags":[],"title":"Transformer","date created":"2024-07-17T00:00:00.000Z","date modified":"2024-11-12T00:00:00.000Z","description":"3blue的transformer视频 Transformer 模型 Q 由 Vaswani 等人在 2017 年提出，最初用于机器翻译任务。与传统的 RNN（循环神经网络）和 LSTM（长短期记忆网络）不同，Transformer 完全基于自注意力机制（Self-Attention Mechanism）实现，并行处理能力更强，训练速度更快。Trans...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/2%20%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/1%20%E8%8A%82%E7%82%B9/CS/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Transformer.html"}],["meta",{"property":"og:site_name","content":"转了码的刘公子"}],["meta",{"property":"og:title","content":"Transformer"}],["meta",{"property":"og:description","content":"3blue的transformer视频 Transformer 模型 Q 由 Vaswani 等人在 2017 年提出，最初用于机器翻译任务。与传统的 RNN（循环神经网络）和 LSTM（长短期记忆网络）不同，Transformer 完全基于自注意力机制（Self-Attention Mechanism）实现，并行处理能力更强，训练速度更快。Trans..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F22%2F03-55-58-dd276138c27da251f8022778663c33e9-202409220355685-852643.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-12-27T06:52:25.000Z"}],["meta",{"property":"article:modified_time","content":"2024-12-27T06:52:25.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Transformer\\",\\"image\\":[\\"https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F09%2F22%2F03-55-58-dd276138c27da251f8022778663c33e9-202409220355685-852643.png\\",\\"https://abs-0.twimg.com/emoji/v2/svg/1f92f.svg \\\\\\"Exploding head\\\\\\"\\",\\"https://abs-0.twimg.com/emoji/v2/svg/1f525.svg \\\\\\"Fire\\\\\\"\\"],\\"dateModified\\":\\"2024-12-27T06:52:25.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"转了码的刘公子\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"自注意力机制","slug":"自注意力机制","link":"#自注意力机制","children":[]}],"git":{"createdTime":1732465042000,"updatedTime":1735282345000,"contributors":[{"name":"Luis","email":"liuysh20@gmail.com","commits":2}]},"readingTime":{"minutes":3.9,"words":1169},"filePathRelative":"2 第二大脑/1 节点/CS/人工智能/Transformer.md","localizedDate":"2024年11月25日","autoDesc":true}');export{m as comp,y as data};
