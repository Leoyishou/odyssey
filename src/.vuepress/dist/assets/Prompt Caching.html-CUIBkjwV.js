import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as p,f as e,o as s}from"./app-DokaGNO4.js";const t={};function a(o,n){return s(),p("div",null,n[0]||(n[0]=[e(`<p>OpenAI的Prompt Caching功能是自动启用的,无需开发者手动开启。以下是关于OpenAI Prompt Caching的主要信息:</p><ol><li><p>自动应用:<br> Prompt Caching自动应用于最新版本的GPT-4o、GPT-4o mini、o1-preview和o1-mini模型,以及这些模型的微调版本[</p></li><li><p>触发条件:</p></li></ol><ul><li><p>缓存自动应用于长度超过1024个token的提示[</p></li><li><p>缓存以128个token为增量进行,从1024个token开始[</p><p>2</p><p>](https://platform.openai.com/docs/guides/prompt-caching).</p></li></ul><ol start="3"><li>价格优势:</li></ol><ul><li><p>缓存的提示比未缓存的提示更便宜,可获得50%的折扣[</p><p>1</p><p>](https://cookbook.openai.com/examples/prompt_caching101)[</p><p>4</p><p>](https://www.53ai.com/news/finetuning/2024100330976.html).</p></li><li><p>例如,GPT-4o模型的原价为$2.50/1K tokens,缓存价格为$1.25/1K tokens[</p><p>4</p><p>](https://www.53ai.com/news/finetuning/2024100330976.html).</p></li></ul><ol start="4"><li>性能提升:</li></ol><ul><li><p>可以减少延迟高达80%[</p><p>1</p><p>](https://cookbook.openai.com/examples/prompt_caching101).</p></li><li><p>对于长提示(超过10,000个token),延迟减少尤为显著[</p><p>1</p><p>](https://cookbook.openai.com/examples/prompt_caching101).</p></li></ul><ol start="5"><li>使用建议:</li></ol><ul><li><p>将静态内容(如指令和示例)放在提示的开头,将可变内容(如用户特定信息)放在末尾[</p><p>2</p><p>](https://platform.openai.com/docs/guides/prompt-caching).</p></li><li><p>这种结构可以增加缓存命中率,从而提高效率[</p><p>6</p><p>](https://www.53ai.com/news/finetuning/2024100945782.html).</p></li></ul><ol start="6"><li>监控使用情况:</li></ol><ul><li><p>API响应中的&#39;usage&#39;字段包含&#39;cached_tokens&#39;值,显示有多少token被缓存[</p><p>5</p><p>](https://openai.com/index/api-prompt-caching/).</p></li></ul><ol start="7"><li>缓存持续时间:</li></ol><ul><li><p>缓存通常在5-10分钟不活动后清除,最长保留1小时[</p><p>4</p><p>](https://www.53ai.com/news/finetuning/2024100330976.html)[</p><p>5</p><p>](https://openai.com/index/api-prompt-caching/).</p></li></ul><p>总之,开发者无需采取任何特殊操作来启用Prompt Caching。只要使用支持的模型并发送超过1024个token的提示,系统就会自动应用缓存,从而降低成本并提高性能</p><p>入参是 项目 id，截止时间，返回是 [{docName：，lastCommit：，lastDiff}]</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>入参：</span></span>
<span class="line"><span> groupId：str</span></span>
<span class="line"><span> queryTime：str</span></span>
<span class="line"><span></span></span>
<span class="line"><span>出参：</span></span>
<span class="line"><span>[</span></span>
<span class="line"><span>  {</span></span>
<span class="line"><span>    &quot;docName&quot;: &quot;文档名称&quot;,</span></span>
<span class="line"><span>    &quot;lastCommit&quot;: &quot;最后commit的文本&quot;,</span></span>
<span class="line"><span>    &quot;lastDiff&quot;: &quot;最后 commit 和前一次的 diff 文本&quot;</span></span>
<span class="line"><span>  }，</span></span>
<span class="line"><span>  {</span></span>
<span class="line"><span>    &quot;docName&quot;: &quot;文档名称&quot;,</span></span>
<span class="line"><span>    &quot;lastCommit&quot;: &quot;最后commit的文本&quot;,</span></span>
<span class="line"><span>    &quot;lastDiff&quot;: &quot;最后 commit 和前一次的 diff 文本&quot;</span></span>
<span class="line"><span>  }，</span></span>
<span class="line"><span>]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,16)]))}const m=i(t,[["render",a],["__file","Prompt Caching.html.vue"]]),r=JSON.parse('{"path":"/1%20%E4%B8%80%E5%88%87%E7%9A%86%E9%A1%B9%E7%9B%AE/qt%20Agent/API%20%E7%89%B9%E6%80%A7/Prompt%20Caching.html","title":"","lang":"zh-CN","frontmatter":{"description":"OpenAI的Prompt Caching功能是自动启用的,无需开发者手动开启。以下是关于OpenAI Prompt Caching的主要信息: 自动应用: Prompt Caching自动应用于最新版本的GPT-4o、GPT-4o mini、o1-preview和o1-mini模型,以及这些模型的微调版本[ 触发条件: 缓存自动应用于长度超过1024...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/1%20%E4%B8%80%E5%88%87%E7%9A%86%E9%A1%B9%E7%9B%AE/qt%20Agent/API%20%E7%89%B9%E6%80%A7/Prompt%20Caching.html"}],["meta",{"property":"og:site_name","content":"转了码的刘公子"}],["meta",{"property":"og:description","content":"OpenAI的Prompt Caching功能是自动启用的,无需开发者手动开启。以下是关于OpenAI Prompt Caching的主要信息: 自动应用: Prompt Caching自动应用于最新版本的GPT-4o、GPT-4o mini、o1-preview和o1-mini模型,以及这些模型的微调版本[ 触发条件: 缓存自动应用于长度超过1024..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-11-24T16:17:22.000Z"}],["meta",{"property":"article:modified_time","content":"2024-11-24T16:17:22.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-11-24T16:17:22.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"转了码的刘公子\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[],"git":{"createdTime":1732465042000,"updatedTime":1732465042000,"contributors":[{"name":"Luis","email":"liuysh20@gmail.com","commits":1}]},"readingTime":{"minutes":1.63,"words":488},"filePathRelative":"1 一切皆项目/qt Agent/API 特性/Prompt Caching.md","localizedDate":"2024年11月25日","autoDesc":true}');export{m as comp,r as data};
