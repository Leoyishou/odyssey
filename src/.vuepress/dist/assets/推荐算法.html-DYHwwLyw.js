import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as l,f as a,o as n}from"./app-DokaGNO4.js";const p={};function t(e,i){return n(),l("div",null,i[0]||(i[0]=[a(`<figure><img src="https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F11%2F05%2F16-14-57-aeb15a19e5331b613dc9d13fb532734c-202411051614711-217908.png" alt="image.png|1000" tabindex="0" loading="lazy"><figcaption>image.png|1000</figcaption></figure><p>本人的新书 <a href="https://wiki.corp.qunar.com/download/attachments/758451995/%E8%B5%84%E6%96%99%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.pdf.zip?version=1&amp;modificationDate=1694655982000&amp;api=v2" target="_blank" rel="noopener noreferrer">资料推荐系统.pdf.zip</a> 问世一来，反响热烈，广大网友踊跃购买。作为一位拥有多年“被面试”和“面试别人”的互联网老打工人，本人总结了一份互联网大厂推荐算法岗位的经典面试问题清单，以答谢读者。</p><p>对于未购买电子书的网友，可以拿这份问题清单测试一下自己的算法水平。<strong>如果发现自己在简历上写明使用过某种技术，但是清单中关于这项技术的问题却回答不上来，那就说明你对这个技术的理解还不够深入，还不掌握技术细节，强烈建议你购买 <a href="https://wiki.corp.qunar.com/download/attachments/758451995/%E8%B5%84%E6%96%99%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.pdf.zip?version=1&amp;modificationDate=1694655982000&amp;api=v2" target="_blank" rel="noopener noreferrer">资料推荐系统.pdf.zip</a> 恶补一下</strong>。</p><p>购书方法：<em>关注微信公众号“石塔西的说书馆”，点击“发消息”，回复“购书”，或点击最下方的“购书”按钮</em>。</p><p>对于已经购买本书的读者，可以拿这份清单测试一下自己的阅读学习成果，是否将本书中的知识都掌握了。建议你将这从清单复制成Word文档，在每道问题下，用最简明扼要的语言写下你的答案。<em>一定不要抄书，一定要用自己的话来复述，一定要没废话地直击问题核心</em>，因为在面试的时候，面试官没有时间和耐心等你翻书和长篇大论。本书覆盖了此份清单中大多数问题的答案，但还是有一两道题目，<em>主观性比较强，没有标准答案，考察的是候选人对推荐系统的经验和理解程度</em>。</p><p>如果对自己的回答没那么大把握，或者对那几道主观题有自己的想法却不知道对错，<strong>可以在朋友圈、社群或自己的媒体渠道推荐、转发本文，凭借转发记录就拥有了和作者一对一讨论的机会</strong>。也欢迎网友购书入群，<strong>分享自己面试或被面试中遇到的好题目</strong>，完善本清单。</p><h1 id="特征工程" tabindex="-1"><a class="header-anchor" href="#特征工程"><span>特征工程</span></a></h1><ul><li>为什么说，用物料的后验消费数据做召回存在“幸存者偏差”？能将这些消费数据用于排序吗？</li><li>使用物料的后验消费数据做召回，会放大“马太效应”，对新物料不友好，如何缓解？</li></ul><hr><ul><li>解释什么是bias特征？你能举出哪些bias特征的例子？</li><li>bias特征怎样接入模型？能否和其他正常特征一起喂入DNN底层？为什么？</li></ul><hr><ul><li>某男性新用户对“体育”这个分类的喜好程度未知，如何填充？</li><li>某新物料的后验指标未知，如何填充？</li><li>对观看次数、观看时长这样的特征，如何做标准化？</li><li>某个物料曝光2次，被点击1次，如何计算它的CTR?</li></ul><hr><ul><li>有一个特征“某文章过去1天的点击率是10%”，如何将其构建成一个类别特征，并喂入推荐模型？</li></ul><h1 id="embedding" tabindex="-1"><a class="header-anchor" href="#embedding"><span>Embedding</span></a></h1><ul><li>为什么说Embedding提升了推荐算法的扩展性？</li><li>FFM针对FM的改进在哪里？</li><li>简述阿里Co-Action Network的基本思想？</li></ul><hr><ul><li>简述Parameter Server是如何应对推荐系统“高维稀疏”的数据环境的？</li><li>什么是异步并发（ASP）中的“梯度失效”问题？即使如此，为什么在推荐系统中仍然常用？</li></ul><h1 id="精排" tabindex="-1"><a class="header-anchor" href="#精排"><span>精排</span></a></h1><ul><li>FTRL是如何保证“在线学习”的稳定性的？</li><li>FTRL是如何保证解的稀疏性的？</li><li>FTRL是如何解决高维稀疏特征受训机会不均匀的问题的？</li></ul><hr><ul><li><p>FM相对LR的优势在哪里？</p></li><li><p>不能只回答自动交叉</p></li><li><p>是如何解决交叉特征太稀疏、受训机会少的问题的？</p></li><li><p>是如何提高扩展性的？</p></li><li><p>FM对所有特征两两交叉，岂不是n方的复杂度？</p></li><li><p>回答是的，这道题直接fail</p></li><li><p>回答不是，要追问FM的实际复杂度是什么？如何实现的？</p></li><li><p>FM的缺点有哪些？</p></li><li><p>提示：其中一个缺点是不灵活，怎么理解？</p></li><li><p>FFM相对于FM的改进有哪些？为什么要这么改进？</p></li><li><p>FFM相比于FM的缺点在哪里？（提示：效率）</p></li></ul><hr><ul><li>Wide &amp; Deep是如何做到兼顾“记忆”与“扩展”的？</li><li>什么样的特征进Deep侧？什么样的特征进Wide侧？</li><li>Wide &amp; Deep论文原文中说，训练Wide &amp; Deep侧分别使用了两种优化器，你觉得有哪些道理？</li></ul><hr><ul><li>DCN解决的是什么问题？</li><li>DCN v1和v2的差别在哪里？</li><li>DCN有哪些缺陷？（提示：输入输出的维度）</li></ul><hr><ul><li>简述基于Transformer做特征交叉的原理？</li><li>Transformer做特征交叉的缺点有哪些？（提示：输入输出维度、时间复杂度）</li></ul><hr><ul><li><p>你在建模行为序列中的每个元素时，一般会包含哪些信息？如何Embedding？</p></li><li><p>每个用户的行为序列长度不同，如何处理？Truncate很简单，关键是如何解决Padding的问题？</p></li><li><p>不解决的话，两个完全不同的序列，因为被填充的大量的0，而被模型认为相似</p></li><li><p>提示：可以看看TensorFlow Transformer的源码，看看人家是如何解决的？</p></li><li><p>Target Attention的时间复杂度</p></li><li><p>Self-Attention的时间复杂度</p></li></ul><hr><ul><li><p>DIN的建模思路是怎样的？怎么理解“千物千面”？</p></li><li><p><strong>每到“双十一”之类的促销季，用户的购买行为与他之前短期行为有较大不同，应该如何建模？</strong></p></li><li><p>其实就是长序列建模的问题，简单套用SIM不是不可以，但是仍然有效率问题</p></li><li><p>开放问题，考察候选人的经验，以及思路是否开阔</p></li><li><p>简述SIM的建模思想？你觉得它的优缺点有哪些？</p></li><li><p>如果想在召回或粗排中建模用户长序列，怎么做？</p></li></ul><h1 id="召回" tabindex="-1"><a class="header-anchor" href="#召回"><span>召回</span></a></h1><ul><li>传统的协同过滤，User CF和Item CF，哪个在工业界更常用？为什么？</li><li>大规模、分布式的CF是如何实现的？</li><li>基于矩阵分解的协同过滤，要分解的矩阵是超级稀疏的，分解这样的矩阵，需要事先将它缺失的地位都填充0吗？</li></ul><hr><ul><li><p>召回模型中如何处理用户行为序列？</p></li><li><p>回答简单的Pooling的，是常规操作</p></li><li><p>有没有可能也做target attention？拿什么当target?</p></li></ul><hr><ul><li>NCE Loss的基本思想与计算公式</li><li>NEG Loss的基本思想与计算公式？它与NCE Loss是什么关系？</li><li>Marginal Hinge Loss的基本思想与计算公式</li><li>BPR Loss的基本思想与计算公式</li></ul><hr><ul><li>Sampled Softmax的基本思想与计算公式</li><li>Sampled Softmax中温度修正的作用？调高温度有什么影响？调低温度有什么影响？</li><li>采样修正的<code>Q(t)</code>应该如何设置？</li></ul><hr><ul><li><p>从算法机理来阐述热门物料对模型的影响？</p></li><li><p>大家都知道热门物料对推荐结果的个性化造成负面影响，我需要你回答出它是如何造成这一负面影响的</p></li><li><p>为了打压热门物料，热门物料当正样本，应该降采样还是过采样？</p></li><li><p>为了打压热门物料，热门物料当负样本，应该降采样还是过采样？为什么？</p></li></ul><hr><ul><li><p>Word2Vec中，Skip-Gram和CBOW有什么区别？哪种算法对于罕见词、罕见搭配更友好？</p></li><li><p>Airbnb的I2I召回，相比于word2vec，在正样本上有哪些创新？合理性在哪里？</p></li><li><p>Airbnb的I2I召回，相比于word2vec，在负样本上有哪些创新？合理性在哪里？</p></li><li><p>Airbnb的U2I召回，是如何解决“预订样本”太稀疏这个问题的？</p></li><li><p>Airbnb的U2I召回，在负样本上有哪些创新？</p></li><li><p>直接套用Word2Vec用作召回，在正样本的选择上有哪些局限性？</p></li><li><p>Airbnb是如何突破这一局限性的？</p></li><li><p>阿里的EGES是如何突破这一局限性的？</p></li></ul><hr><ul><li><p>阐述FM用于召回的原理与作法</p></li><li><p>线下如何训练</p></li><li><p>线上如何预测</p></li><li><p>如何做到对新用户更友好</p></li></ul><hr><ul><li><p>召回随机负采样在实践中到底是怎么做的？提示：</p></li><li><p>你可以离线采样，借助Spark在更大范围内采样，怎么实现？</p></li><li><p>你可以Batch内负采样，怎么实现？</p></li><li><p>你可以混合负采样，怎么实现？</p></li><li><p>阐述以上几种方法的优劣</p></li></ul><hr><ul><li><p>GCN与DNN在迭代公式上的区别在哪里？（提示：GCN没那么玄，真的只是一个小差别）</p></li><li><p>如何在一个超大规模图上，训练GCN召回模型？希望听到：</p></li><li><p>Mini-Batch的训练细节</p></li><li><p>邻居采样，挑出重要邻居</p></li><li><p>邻居采样时有小Trick，否则会造成数据泄漏</p></li><li><p>如何在一个超大规模图上，进行GCN推理，得到各节点的Embedding？</p></li><li><p>希望候选人意识到推理与训练的不同</p></li><li><p>要避免重复计算节点的Embedding</p></li><li><p>要避免“邻居采样”那样的随机性</p></li></ul><h1 id="粗排" tabindex="-1"><a class="header-anchor" href="#粗排"><span>粗排</span></a></h1><ul><li><p>你对改进粗排有什么思路？或者说，你觉得制约粗排模型性能的有哪些因素？</p></li><li><p>粗排双塔与召回双塔的异同？提示有以下4方面的不同</p></li><li><p>物料向量的存储方式</p></li><li><p>样本，特别是负样本的选择</p></li><li><p>损失函数的设计</p></li><li><p>最终用户向量与物料向量的交互方式</p></li><li><p>你知道双塔模型有哪些改进变形？</p></li><li><p>用精排蒸馏粗排应该怎么做？</p></li><li><p>提示：共同训练？两阶段？用什么Loss？各有什么优缺点？</p></li><li><p>思考：蒸馏的假设有问题吗？</p></li><li><p>粗排环节存在哪些“样本选择偏差”？如何纠偏？</p></li></ul><h1 id="重排" tabindex="-1"><a class="header-anchor" href="#重排"><span>重排</span></a></h1><ul><li>常用的打散方式有哪些？</li><li>MMR的核心思路是什么？</li></ul><hr><ul><li>基于DPP的重排的核心思想是什么？</li><li>如何构建DPP中的核矩阵L？基于怎样的先验假设？</li><li>怎么证明如此构建的L是符合我们的先验假设的？</li></ul><hr><ul><li><p>基于上下文感知的重排模型的原理和流程</p></li><li><p>如何构建样本？</p></li><li><p>如何构建Loss？</p></li><li><p>都应该包含哪些特征？</p></li><li><p>模型训练完毕后，如何排序？（提示：顺序在训练时是已知的，但是预测时是未知的）</p></li></ul><h1 id="多任务-多场景" tabindex="-1"><a class="header-anchor" href="#多任务-多场景"><span>多任务 &amp; 多场景</span></a></h1><ul><li>为什么不为每个目标单独建模？</li><li>为什么不直接针对最终目标建模？比如直接建模购买率，没必要建模点击率？</li></ul><hr><ul><li><p>你在工作场景中遇到的多目标推荐的问题？</p></li><li><p>有哪些目标？</p></li><li><p>使用了怎样的模型结构？</p></li><li><p>排序时，我们既希望用户点击，又希望用户点开后观看尽可能长，如何建模？</p></li><li><p>提示：时长目标的单位问题，如果用均方误差，其量纲要远远大于CTR的BCE loss</p></li><li><p>你在训练模型时，是怎么将多个损失融合成一个损失的？</p></li><li><p>你在排序时，是怎么将多个目标的打分融合成一个打分的？</p></li><li><p>提示：要考虑到不同目标的打分存在天然的分布差异</p></li></ul><hr><ul><li>你遇到过多场景推荐的问题吗？你觉得难点在哪里？</li><li>你要为一个服务全球的APP设计推荐模型，用户的国籍、语言这些特征要怎么使用？</li></ul><h1 id="冷启动" tabindex="-1"><a class="header-anchor" href="#冷启动"><span>冷启动</span></a></h1><ul><li>新用户冷启，如何建模成一个多臂老虎机问题？</li><li>新物料冷启，如何建模成一个多臂老虎机问题？</li></ul><hr><ul><li>怎么用Epsilon Greedy进行新用户冷启？</li><li>怎么用UCB进行新用户冷启？</li><li>Bayesian Bandit的基本原理</li><li>Thompson Sampling用于新用户冷启的基本原理与流程</li></ul><hr><ul><li><p>LinUCB用于新闻冷启动的基本原理与流程？</p></li><li><p>这里的谁是老虎机？谁是手柄？</p></li><li><p>如何建模手柄的收益？和哪些因素有关？</p></li><li><p>如何求出每根手柄的参数？</p></li><li><p>候选新闻集合是动态变化的，如何处理？</p></li><li><p>观察到用户反馈后，如何更新每根手柄的收益分布？时间复杂度是什么？</p></li></ul><hr><ul><li>预测时遇到训练时未见过的新特征，是怎么处理的？</li><li>训练时遇到训练时未见过的新特征，是怎么处理的？</li></ul><hr><ul><li><p>简述Meta-Learning的基本原理与流程</p></li><li><p>MAML作为一种特殊的meta-learning，特殊在哪里？</p></li><li><p>简述MAML的训练流程</p></li><li><p>MAML应用于推荐系统的冷启动</p></li><li><p>每个Task的粒度是什么？</p></li><li><p>哪些参数需要从最优初值初始化？</p></li></ul><hr><ul><li><p>简述对比学习的训练流程和应用场景</p></li><li><p>你将对比学习应用于推荐系统的什么场景？</p></li><li><p>对比学习与向量化召回有什么异同？</p></li><li><p>你是怎么做对比学习的？</p></li><li><p>对比学习的样本从哪里来？</p></li><li><p>数据增强是怎么做的？</p></li><li><p>与主任务是怎么协同训练的？</p></li></ul><hr><ul><li><p><strong>让你建立一个模型，预测新入库物料的后验CTR，以找到那些潜在爆款？</strong></p></li><li><p>样本怎么选？</p></li><li><p>怎么设计特征？</p></li><li><p>标签怎么收集？（提示：注意一下时间范围）</p></li><li><p>怎么设计Loss？</p></li><li><p>怎么用这个模型？</p></li><li><p>设计对新用户友好的特征？你打算在模型中怎么用这些特征？</p></li></ul><h1 id="评估与调试" tabindex="-1"><a class="header-anchor" href="#评估与调试"><span>评估与调试</span></a></h1><ul><li>AUC的物理含义</li><li>AUC用在评价推荐性能时的缺陷</li><li>GAUC的计算方法</li><li>GAUC的缺点（提示：权重、位置）</li><li>NDCG的思路与计算方法</li></ul><hr><ul><li>AUC能不能用于评价召回模型？</li><li>你在评估召回模型时，主要使用哪些指标？</li><li>MAP的思路与计算方法</li></ul><hr><ul><li>AB Test中应该如何划分流量？</li><li>你在做AB Test时，一般会有哪些注意事项？</li><li>用通俗语言解释一下什么是p-value？</li><li>解释一下什么是Type I Error, Type II Error, Power?</li></ul><hr><ul><li>如何知道某个特征在你的模型中的特征重要性？</li><li>一个多层的DNN，你想压缩一下，如何找到&quot;滥竽充数&quot;那一层?</li></ul><hr><ul><li>你碰到过“线下AUC涨了，线上AB指标没提升”的情况吗？怎么处理解决的？</li><li>解释一下“特征穿越”现象，及如何解决？</li><li>你碰到过“老汤模型”带来的麻烦吗？如何解决？</li><li><strong>新模型小流量上线后，我收集了一批线上数据<code>D</code>做测试样本，让新老模型都在<code>D</code>上预测并计算GAUC，我的作法有什么问题？正确方式应该怎么做？</strong></li><li>解释一下“链路一致性”问题。你有没有遇到过“链路一致性”问题？如何解决的？</li></ul><h1 id="其他" tabindex="-1"><a class="header-anchor" href="#其他"><span>其他</span></a></h1><ul><li><p>以下你两个问题，只要你简历中出现GBDT相关项目，我一般都会问一下，能够答对的人不多</p></li><li><p>GBDT中的G代表梯度，那它是谁对谁的梯度？</p></li><li><p>给你一个<code>m*n</code>的数据集，<code>m</code>是样本数，<code>n</code>是特征数，问这个梯度向量G有多长？</p></li><li><p>这两个问题实际上是一体的，你只要搞清楚G的含义，自然知道它有多长</p></li><li><p>这两个问题答对了，不代表你懂GBDT，但是<strong>答不上来，你肯定不懂GBDT</strong>。连G是谁对谁的梯度都搞不清楚，就好比你对外宣称自己是德华的忠实粉丝，但是人家问德华姓什么，你却回答不上来一样尴尬。</p></li></ul><p>这是一个很好的想法！利用推荐系统来优化 Anki 卡片的出现顺序确实可以提高学习效率。结合 Hugging Face 的工具和模型，我们可以实现这个目标。以下是一些相关的想法和知识：</p><ol><li>句子相似度计算： <ul><li>使用 Hugging Face 的预训练语言模型（如 BERT、RoBERTa 或 DistilBERT）来获取句子的嵌入向量。</li><li>计算句子之间的余弦相似度来判断句子的相似程度。</li></ul></li><li>难度评估： <ul><li>使用语言模型评估句子的复杂度（如词汇难度、语法结构）。</li><li>结合用户的历史表现（如回答正确率、反应时间）来动态调整难度评估。</li></ul></li><li>主题聚类： <ul><li>使用主题建模技术（如 LDA）或基于 transformer 的聚类方法对句子进行分组。</li><li>这可以帮助将相关主题的句子组织在一起。</li></ul></li><li>间隔重复算法优化： <ul><li>结合传统的间隔重复算法（如 SM-2）和机器学习模型来预测最佳复习时间。</li></ul></li><li>个性化推荐： <ul><li>使用协同过滤或基于内容的推荐方法，考虑用户的学习历史和偏好。</li></ul></li><li>序列推荐： <ul><li>使用序列模型（如 LSTM 或 Transformer）来预测下一个最适合学习的句子。</li></ul></li><li>多模态学习： <ul><li>如果卡片包含音频，可以使用 Hugging Face 的语音识别模型来分析音频特征。</li></ul></li><li>动态难度调整： <ul><li>实时调整推荐策略，根据用户的即时表现调整下一个推荐的难度。</li></ul></li><li>词汇关联： <ul><li>使用 word2vec 或 GloVe 等词嵌入技术，找出句子中关键词的关联性。</li></ul></li><li>错误模式分析： <ul><li>使用自然语言处理技术分析用户的错误模式，推荐能够针对性改善这些错误的句子。</li></ul></li><li>迁移学习： <ul><li>利用在大规模英语语料上预训练的模型，微调到您的具体应用场景。</li></ul></li><li>进度跟踪和适应性学习路径： <ul><li>使用强化学习技术动态调整学习路径，最大化长期学习效果。</li></ul></li></ol><p>实现步骤：</p><ol><li>数据准备：将 Anki 卡片内容导出并预处理。</li><li>特征提取：使用 Hugging Face 的 Transformers 库提取句子特征。</li><li>相似度计算：实现相似度计算函数。</li><li>推荐系统设计：结合上述想法，设计适合您需求的推荐算法。</li><li>集成到 Anki：开发插件或脚本，将推荐系统集成到 Anki 中。</li><li>评估和优化：持续监控系统性能，根据学习效果进行调整。</li></ol><p>您对这些想法中的哪一个特别感兴趣？我们可以进一步讨论如何使用 Hugging Face 的工具来实现这些功能。</p><p>非常好，让我们深入探讨句子相似度计算和序列推荐这两个方面，并考虑如何使用 Hugging Face 的工具来实现它们。</p><ol><li>句子相似度计算：</li></ol><p>使用 Hugging Face 的预训练模型来计算句子相似度是一个很好的方法。这里是一个实现思路：</p><p>a) 选择模型：</p><ul><li>可以使用 BERT、RoBERTa 或 DistilBERT 等模型。对于英语句子，&quot;sentence-transformers/all-MiniLM-L6-v2&quot; 是一个不错的选择，它在计算句子相似度方面表现良好。</li></ul><p>b) 获取句子嵌入：</p><ul><li>使用选定的模型将每个句子转换为一个固定长度的向量。</li></ul><p>c) 计算相似度：</p><ul><li>使用余弦相似度来计算句子向量之间的相似程度。</li></ul><p>实现概述：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> transformers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> AutoTokenizer, AutoModel</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.nn.functional </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">as</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> F</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 加载模型和分词器</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_name </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">tokenizer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> AutoTokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> AutoModel.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> get_sentence_embedding</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">sentence</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    inputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> tokenizer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(sentence, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">return_tensors</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;pt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">padding</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">truncation</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    with</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">no_grad</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">():</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        outputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> model</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(**inputs)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> F.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">normalize</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(outputs.last_hidden_state.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">mean</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">dim</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">), </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">p</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">dim</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> compute_similarity</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">sentence1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> sentence2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    emb1 </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> get_sentence_embedding</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(sentence1)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    emb2 </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> get_sentence_embedding</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(sentence2)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.nn.functional.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">cosine_similarity</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(emb1, emb2).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">item</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用示例</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">similarity </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> compute_similarity</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;The cat is on the mat&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;A feline is resting on a rug&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;Similarity: </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">similarity</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol><li>序列推荐：</li></ol><p>对于序列推荐，我们可以使用 Transformer 模型来预测下一个最适合学习的句子。这里是一个实现思路：</p><p>a) 数据准备：</p><ul><li>将用户的学习历史转换为序列数据。</li><li>每个序列包含用户学习的最后 N 个句子的 ID。</li></ul><p>b) 模型选择：</p><ul><li>可以使用 GPT-2 或 BERT 等模型，根据需要进行微调。</li></ul><p>c) 训练过程：</p><ul><li>使用历史序列预测下一个句子。</li><li>可以加入额外特征，如难度级别、上次回答的正确性等。</li></ul><p>d) 推荐过程：</p><ul><li>根据用户最近学习的句子序列，预测下一个最适合学习的句子。</li></ul><p>实现概述：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> transformers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> GPT2LMHeadModel, GPT2Tokenizer</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 加载模型和分词器</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_name </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;gpt2&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">tokenizer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> GPT2Tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> GPT2LMHeadModel.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> prepare_sequence</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">sentence_ids</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 将句子ID转换为模型可以理解的格式</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">encode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot; &quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">join</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">map</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, sentence_ids)), </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">return_tensors</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;pt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> predict_next_sentence</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">sequence</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    inputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> prepare_sequence</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(sequence)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    with</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">no_grad</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">():</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        outputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> model</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(inputs)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        predictions </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> outputs.logits[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, :]</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">argmax</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(predictions).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">item</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用示例</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">user_history </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">101</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">102</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">103</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">104</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">105</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 假设这些是句子ID</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">next_sentence_id </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> predict_next_sentence</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(user_history)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;推荐的下一个句子ID: </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">next_sentence_id</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>将这两种方法结合起来：</p><ol><li>使用句子相似度计算来组织和聚类你的 Anki 卡片。</li><li>使用序列推荐来决定下一个应该学习的卡片或句子。</li></ol><p>实际应用中，你可能需要：</p><ul><li>对模型进行微调，使其更适合你的具体英语学习场景。</li><li>考虑句子难度、用户表现等额外因素。</li><li>实现一个评分系统来持续评估和改进推荐质量。</li></ul><p>你对这个方案有什么想法吗？或者你想更深入地了解其中的某个部分？</p>`,123)]))}const r=s(p,[["render",t],["__file","推荐算法.html.vue"]]),d=JSON.parse('{"path":"/2%20%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/1%20%E8%8A%82%E7%82%B9/CS/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95.html","title":"推荐算法","lang":"zh-CN","frontmatter":{"draw":null,"tags":[],"title":"推荐算法","date created":"2024-07-17T00:00:00.000Z","date modified":"2024-11-12T00:00:00.000Z","description":"image.png|1000image.png|1000 本人的新书 资料推荐系统.pdf.zip 问世一来，反响热烈，广大网友踊跃购买。作为一位拥有多年“被面试”和“面试别人”的互联网老打工人，本人总结了一份互联网大厂推荐算法岗位的经典面试问题清单，以答谢读者。 对于未购买电子书的网友，可以拿这份问题清单测试一下自己的算法水平。如果发现自己在简历上写...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/2%20%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/1%20%E8%8A%82%E7%82%B9/CS/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95.html"}],["meta",{"property":"og:site_name","content":"转了码的刘公子"}],["meta",{"property":"og:title","content":"推荐算法"}],["meta",{"property":"og:description","content":"image.png|1000image.png|1000 本人的新书 资料推荐系统.pdf.zip 问世一来，反响热烈，广大网友踊跃购买。作为一位拥有多年“被面试”和“面试别人”的互联网老打工人，本人总结了一份互联网大厂推荐算法岗位的经典面试问题清单，以答谢读者。 对于未购买电子书的网友，可以拿这份问题清单测试一下自己的算法水平。如果发现自己在简历上写..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F11%2F05%2F16-14-57-aeb15a19e5331b613dc9d13fb532734c-202411051614711-217908.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-12-11T14:48:27.000Z"}],["meta",{"property":"article:modified_time","content":"2024-12-11T14:48:27.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"推荐算法\\",\\"image\\":[\\"https://imagehosting4picgo.oss-cn-beijing.aliyuncs.com/imagehosting/fix-dir%2Fpicgo%2Fpicgo-clipboard-images%2F2024%2F11%2F05%2F16-14-57-aeb15a19e5331b613dc9d13fb532734c-202411051614711-217908.png\\"],\\"dateModified\\":\\"2024-12-11T14:48:27.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"转了码的刘公子\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[],"git":{"createdTime":1732465042000,"updatedTime":1733928507000,"contributors":[{"name":"Luis","email":"liuysh20@gmail.com","commits":2}]},"readingTime":{"minutes":18.01,"words":5403},"filePathRelative":"2 第二大脑/1 节点/CS/人工智能/推荐算法.md","localizedDate":"2024年11月25日","autoDesc":true}');export{r as comp,d as data};
