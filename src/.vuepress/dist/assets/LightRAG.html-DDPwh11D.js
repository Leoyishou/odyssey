import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,f as e,o}from"./app-DokaGNO4.js";const a={};function p(n,l){return o(),t("div",null,l[0]||(l[0]=[e('<h3 id="本质" tabindex="-1"><a class="header-anchor" href="#本质"><span>本质</span></a></h3><ol><li>为什么要这么设计:</li></ol><ul><li>LLM 很强大,但直接让它读长文档太浪费算力</li><li>而且 LLM 的上下文窗口有限,一次性塞不进太多内容</li><li>所以需要先找到最相关的内容再提供给 LLM</li></ul><ol start="2"><li>具体做法:</li></ol><ul><li>提前把文档切块并向量化(类似给书做索引)</li><li>用向量相似度快速找到相关内容(就像在图书馆用目录快速定位)</li><li>只把最相关的内容交给 LLM 处理(相当于让 LLM 只看重点章节)</li></ul><ol start="3"><li>好处是:</li></ol><ul><li>速度快(不用 LLM 读完全部内容)</li><li>成本低(LLM 调用次数少)</li><li>质量好(回答基于相关事实,不是想象)</li></ul><p>这就像是:</p><ul><li>先用简单但快速的方法(向量检索)找到可能相关的内容</li><li>再用&quot;专家&quot;(LLM)来仔细分析这些内容并给出答案</li></ul><p>相当于把&quot;找信息&quot;和&quot;理解信息&quot;这两件事分开做,各自用最合适的工具。这就是为什么这种方法既快速又准确。</p><ol><li>将非结构化文档转换为结构化数据</li><li>提取和保存实体与关系信息</li><li>构建知识图谱</li><li>保持数据的一致性和去重</li><li>通过向量化支持后续的检索和匹配</li></ol><h3 id="构建阶段" tabindex="-1"><a class="header-anchor" href="#构建阶段"><span>构建阶段</span></a></h3><ol><li><p>就像你在读一本书：</p><ul><li>先把书分成一页一页的（Text Chunks）</li><li>给每页做个摘要（Embedding）</li><li>把摘要存起来方便以后查找</li></ul></li><li><p>然后你会：</p><ul><li>找出书中提到的重要概念（Entities）比如人物、地点</li><li>找出这些概念之间的关系（Relations）比如谁认识谁</li><li>去掉重复的内容（Dedupe）</li><li>整理一下描述（Update Description）</li></ul></li><li><p>最后把所有信息存在三个地方：</p><ul><li>像图书馆的索引卡片（Vector DB）向量数据库</li><li>像目录一样的快速查找表（KV Storage）KV 存储</li><li>像思维导图一样的关系网（Knowledge Graph）知识图谱</li></ul></li></ol><h3 id="查找阶段" tabindex="-1"><a class="header-anchor" href="#查找阶段"><span>查找阶段</span></a></h3><ol><li><p>当你想找什么信息时：</p><ul><li>系统会去查那些&quot;索引卡片&quot;</li><li>找到相关的概念和它们之间的关系</li><li>就像在图书馆找书，先看目录，再顺着相关的主题找</li></ul></li><li><p>然后：</p><ul><li>系统会理解你具体想问什么</li><li>找到相关的信息</li><li>把这些信息组织成一个合理的回答</li></ul></li></ol><p>简单来说：</p><ul><li>第一个图是在&quot;整理笔记&quot;</li><li>第二个图是在&quot;查找笔记并回答问题&quot;</li><li>整个系统就像一个非常聪明的图书管理员，不仅会整理书籍，还能帮你找到信息并回答问题</li></ul><p>这个系统的特别之处在于它不仅记住了信息，还理解了信息之间的联系，所以可以回答比较复杂的问题。就像一个既记性好又理解力强的助手。</p><h3 id="更新阶段" tabindex="-1"><a class="header-anchor" href="#更新阶段"><span>更新阶段</span></a></h3><p>如果您选择了LightRAG并且数据源更新了，您可以利用LightRAG的增量更新功能来高效地处理新数据。LightRAG的增量更新算法具有以下优点：</p><ol><li><p>快速适应新数据：LightRAG能够在不需要重建整个索引的情况下，快速整合新的信息[2]。</p></li><li><p>高效处理：增量更新算法使用与之前相同的基于图的索引步骤来处理新文档。然后，模型将新图与现有知识图谱合并，确保信息的连贯性和一致性[3]。</p></li><li><p>最小化重新索引：与其他RAG系统相比，LightRAG在更新数据时显著减少了重新索引的需求[1]。</p></li><li><p>降低API调用成本：相较于GraphRAG，LightRAG在处理增量数据更新时需要的API调用次数更少，从而降低了成本[1]。</p></li><li><p>保持实时准确性：通过增量更新，LightRAG能够在实时应用中保持准确性和相关性，特别适用于数据不断变化的环境[2]。</p></li></ol><p>要更新您的LightRAG系统，您只需要：</p><ol><li>准备新的数据文件（例如新的PDF文档）。</li><li>使用LightRAG的API或函数来添加新文档。</li><li>系统会自动处理新数据，更新知识图谱和向量存储，而无需重建整个索引。</li></ol><p>这种方法确保了您的LightRAG系统能够始终保持最新状态，同时最大限度地减少了更新过程中的计算开销和时间消耗。</p><h2 id="亮点" tabindex="-1"><a class="header-anchor" href="#亮点"><span>亮点</span></a></h2><ul><li>LLM 用于理解和提取</li><li>Embedding 用于向量化</li><li>知识图谱用于关系存储</li><li>向量数据库用于相似性搜索</li></ul>',26)]))}const s=i(a,[["render",p],["__file","LightRAG.html.vue"]]),u=JSON.parse('{"path":"/2%20%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/1%20%E8%8A%82%E7%82%B9/CS/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/AIGC/LightRAG.html","title":"","lang":"zh-CN","frontmatter":{"description":"本质 为什么要这么设计: LLM 很强大,但直接让它读长文档太浪费算力 而且 LLM 的上下文窗口有限,一次性塞不进太多内容 所以需要先找到最相关的内容再提供给 LLM 具体做法: 提前把文档切块并向量化(类似给书做索引) 用向量相似度快速找到相关内容(就像在图书馆用目录快速定位) 只把最相关的内容交给 LLM 处理(相当于让 LLM 只看重点章节) ...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/2%20%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/1%20%E8%8A%82%E7%82%B9/CS/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/AIGC/LightRAG.html"}],["meta",{"property":"og:site_name","content":"转了码的刘公子"}],["meta",{"property":"og:description","content":"本质 为什么要这么设计: LLM 很强大,但直接让它读长文档太浪费算力 而且 LLM 的上下文窗口有限,一次性塞不进太多内容 所以需要先找到最相关的内容再提供给 LLM 具体做法: 提前把文档切块并向量化(类似给书做索引) 用向量相似度快速找到相关内容(就像在图书馆用目录快速定位) 只把最相关的内容交给 LLM 处理(相当于让 LLM 只看重点章节) ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-12-11T14:48:27.000Z"}],["meta",{"property":"article:modified_time","content":"2024-12-11T14:48:27.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-12-11T14:48:27.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"转了码的刘公子\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":3,"title":"本质","slug":"本质","link":"#本质","children":[]},{"level":3,"title":"构建阶段","slug":"构建阶段","link":"#构建阶段","children":[]},{"level":3,"title":"查找阶段","slug":"查找阶段","link":"#查找阶段","children":[]},{"level":3,"title":"更新阶段","slug":"更新阶段","link":"#更新阶段","children":[]},{"level":2,"title":"亮点","slug":"亮点","link":"#亮点","children":[]}],"git":{"createdTime":1733928507000,"updatedTime":1733928507000,"contributors":[{"name":"Luis","email":"liuysh20@gmail.com","commits":1}]},"readingTime":{"minutes":3.86,"words":1159},"filePathRelative":"2 第二大脑/1 节点/CS/人工智能/AIGC/LightRAG.md","localizedDate":"2024年12月11日","autoDesc":true}');export{s as comp,u as data};
