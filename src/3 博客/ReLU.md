---
date created: 2025-04-07
date modified: 2025-07-10
uid: 7428288c-c7c7-4dca-b4fd-0dd4a2946a8e
---

`ReLU` 是**修正线性单元 (Rectified Linear Unit)** 的缩写，它是神经网络中一种非常常用和重要的**激活函数 (Activation Function)**。

**它是干什么的？**

你可以把它想象成一个非常简单的**门卫**或者**过滤器**：

- **规则：** 对于任何输入信号（数字 `x`），
    
    - 如果信号 `x` 是**正数**（大于 0），门卫就**直接让它通过**，输出还是 `x`。
    - 如果信号 `x` 是**负数**或**零**（小于等于 0），门卫就**把它拦住，直接输出 0**。
- **数学公式：** `ReLU(x) = max(0, x)`
    

**为什么在神经网络（比如这里的 FFN）里用它？**

1. **引入非线性 (Introduce Non-linearity)**:
    
    - 像 `nn.Linear(8, 16)` 和 `nn.Linear(16, 8)` 这样的线性层，它们做的只是线性变换（乘以权重矩阵再加偏置）。如果你把一堆线性层叠在一起，没有中间的“捣乱”，那么整个叠起来的东西本质上还是一个大的线性变换，学习能力非常有限，无法拟合复杂的数据模式。
    - 激活函数（比如 ReLU）的作用就是在这些线性变换之间**加入非线性因素**。ReLU 的 `max(0, x)` 操作就是一个简单的非线性操作（因为它不是一条直线）。有了非线性，神经网络才能学习和表示更复杂的关系。
    - 在 FFN `nn.Linear -> nn.ReLU -> nn.Linear` 这个结构里，ReLU 就是那个关键的非线性“转折点”。没有它，这两层线性层就等于一层线性层了。
2. **计算简单高效 (Computationally Efficient)**:
    
    - ReLU 的计算非常快，只需要比较输入和 0 的大小，不需要像 Sigmoid 或 Tanh 函数那样计算复杂的指数。
3. **缓解梯度消失问题 (Alleviates Vanishing Gradients)**:
    
    - 在深度网络中，一些旧的激活函数（如 Sigmoid）在输入很大或很小时梯度会变得非常小，导致在反向传播时梯度逐层递减，最终消失，使得网络难以训练。
    - ReLU 在输入大于 0 时梯度恒为 1，这有助于梯度在网络中更好地传递。（虽然它也有自己的问题，比如输入小于0时梯度为0，可能导致“神经元死亡”）

**在这个 FFN 例子里：**

`nn.ReLU()` 被放在第一个线性层 `nn.Linear(8, 16)` 之后。它的作用是：接收第一个线性层输出的 16 维向量，然后**逐个元素**地应用 `max(0, x)` 规则，将所有负数替换为 0，正数保持不变。处理后的这个非线性向量再被送入第二个线性层 `nn.Linear(16, 8)`。

简单说，`nn.ReLU()` 就是给神经网络增加学习复杂模式能力的一个简单、高效的“开关”。
